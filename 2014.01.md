# [Unit Testing Your Production Code With Metrics And Logs](http://about.travis-ci.org/blog/2013-06-11-unit-testing-your-production-code/)

> We use Librato Metrics to store and visualize our data, and we're using Eric Lindvall's excellent metriks library.

Where metrics tell you all about what's going on, logs are the true soul of your application. Metrics only give you a numeric representation of what happened, logs tell you (ideally) why things happened. They're your audit trail of steps your application took to execute a specific task.

We're using Papertrail for log aggregation.

# [Performance Tuning](http://nerds.airbnb.com/performance-tuning/)

* DevTools
* PageSpeed Insights
* WebPagetest
* NewRelic

In our experience, most gains have been had from the simplest of rules: **reduce requests**. Sprite images in stylesheets, combine and minify multiple JavaScript files, and embed small images into data URIs; watch the dev tool waterfalls and make sure that you don't have assets blocking the download of other assets; and finally, put JavaScript at the bottom of the page.

We also cache non-personalized pages aggressively with Akamai.

* SPDY (HTTP pipelining)
* HTTP streaming; possible in Rails 3.1+, Node, and many other frameworks
* Cutting down bundled asset size; removing old JavaScript and CSS, and relying more heavily on our style platform
* Smarter edge caching with better global distribution

# [Redis Set Intersection - Using Sets to Filter Data](http://robots.thoughtbot.com/redis-set-intersection-using-sets-to-filter-data)

With Redis sets we can store records in groups based on time. The sunion operator then enables us to group multiple sets together based on a particular time frame.

Group flights by departure time
Group flights by city (departing & arriving)
Find flights from San Francisco to New York (`sinter`)

To find flights departing on 03/23/2013 between 1:00 pm and 3:00 pm, we'll need to group the sets of departure times, and then perform an intersection with the resulting set.

1. Create a new temp_set using sunionstore to group flights by departure time.
2. Intersect the temporary set with the departure and arrival sets.
3. Loop over the results of the intersection and generate an array of flight keys.
4. Use mget to fetch all the flight data.

# [Programmers Need To Learn Statistics Or I Will Kill Them All](http://zedshaw.com/essays/programmer_stats.html)

This article is my call for all programmers to finally learn enough about statistics to at least know they don't know shit.

I think women are better programmers because they have less ego and are typically more interested in the gear rather than the pissing contest.

A common element of process control statistics is that all processes have a period in the beginning where the process isn't stable. This "ramp-up" period is usually discarded when doing the analysis unless your run length has to include it. Most people think that 1000 is more than enough, but it totally depends on how the system functions. Many complex interacting systems can easily need 1000 iterations to get to a steady state, especially if performing 1000 transactions is very quick. Imagine a banking system that handles 10,000 transactions a second. I could take a good minute to get this system into a steady state, but your measly little 1000 transaction test is only scratching the surface.

You run 1000 test sequentially and then find out that the system blows up when you give it a parallel load. Now you're back to square one because the performance characteristics are different under parallel load.

The moral of the story is that if you give an average without standard deviations then you're totally missing the entire point of even trying to measure something. A major goal of measurement is to develop a succinct and accurate picture of what's going on, but if you don't find out the standard deviation and do at least a couple graphs then you're screwed. Just give up man. Game over. Game over.

Ah, confounding. The most difficult thing to explain to a programmer, yet the most elementary part of all scientific experimentation. It's pretty simple: **if you want to measure something, then don't measure other shit**.

Before you can measure something you really need to lay down a very concrete definition of what you're measuring. You should also try to measure the simplest thing you can and try to avoid confounding. Yet still I see software developers begging for gazillions of dollars to buy some crap tool that doesn't even mention "standard deviation".

Finally, you should check out the R Project for the programming language used in this article. It is a great language for this, with some of the best plotting abilities in the world. Learning to use R will help you also learn statistics better.

# [Securely store sensitive data in the DB?](https://news.ycombinator.com/item?id=6116243)

#### HIPAA

Put the MySQL data files on a partition which is block-level encrypted. Issue decryption keys to clients that need them, such as the application.

#### e-commerce

Restrict access to an external (read different machine/network segment), firewalled host that does the decryption. Encrypted data is passed to the service, which pulls the encryption key out of memory, decrypts the data, and sends it back to the requesting host. The encryption key is stored in (at least) two pieces, each piece is encrypted with a key encrypting key, key encrypting keys are know to very few employees, no single employee holds both key encrypting keys. The encryption keys is only assembled in its entirety while in memory.

#### Card info

You have a dedicated box that stores details and is remotely contacted through an XML-RPC/JSON-HTTP API of some sort.
The API should have two methods:

1. Add a new card to account.
2. Make payment of £xx from card NN.

The machine is locked down, runs no other services, and so cards cannot be exported/stolen from this system. You'd encrypt the filesystem and prompt for a key/passphrase at boot. Ideally you'd only login via the serial console so the only service exposed is your "add/charge" methods. Allowing the remote deletion of cards would be a security risk.

Payment gateways have it somewhat easy - the system making the "make a payment" request doesn't ever need the actual sensitive data returned. That data can be stored on a heavily secured server that only can call out to the banks.

#### General guidelines

**Separate your reads from your writes**. Using a public/private key pair, you can give one set of systems the ability to write encrypted data but not read it, and a different set of systems the ability to read the data. The systems that can decrypt / read data should be isolated as much as possible - don't expose them to public networks, limit which clients have access to them, etc. The separation also forces you to encapsulate your secure data and define an API over it; rather than arbitrary reads, hosts that don't have the decryption keys will have to ask the hosts that do to perform specific operations. If you're writing a Rails app, the [Strongbox gem](https://github.com/spikex/strongbox) enforces this pattern.

1. Keep passwords in memory (so as if you start the service it prompts for the password).
2. Asymmetrical crypto. Encrypt your CC data upon sign-up but then to run the charges you need the private key (and this is somewhere else).
3. Enable SSL communication with your DB. Postgres has this, because being defeated by network sniffing is bad.

[Translucent Databases](http://www.amazon.co.uk/dp/0967584418) with a lot of interesting use cases, where the assumption is that an adversary has gained access to the entire database.

> After all, the primary objective isn't to create an impenetrable system, but one that's exceptionally difficult to penetrate.

Re-use the principle of password hashes for API keys to simply avoid having to store hyper-sensitive data: generate a long (say 512-bit) secure random number (using OpenSSL) as the user's secret API key. Then hash the key as if it were a password and store only the hash. Now if someone steals your API key database they can't use it to authenticate as your users.

For API keys a strong hash such as bcrypt will probably be too slow and resource-intensive. However, because API keys are (long) random data, unlike passwords, you can use a faster hash function like SHA-1.

> Your best bet is to have three-factor authentication (something they are, something they have, something they know) generate a key to encrypt the data.

Use Asymmetric encryption and store the private key in an HSM. The private key never leaves the HSM device.

In Lotus/IBM Notes ID, the password is used in a KDF to generate a key, which in turn decrypts the user's private key (and certain other credentials, along with symmetric secret keys for shared encrypted doccuments). Success/failure is determined solely by the successful decryption of known bytes in the encrypted package. Other info (the user's public key, identity and certifier, all signed) are maintained in the clear and can be easily and safely exported and may be "trusted" for authentication with remote machines. There is a "password recovery" system as well (it doesn't actually recover the password, but allows a reset), requiring cooperation of two or more admins¹ (in a Shamir-type arrangement) so that previously-encrypted user data will not be lost.

# [That Wibbly Wobbly Real-Timey Wimey stuff](https://moot.it/blog/technology/real-timey-wimey-stuff.html)

We bubble up events and listen to events throughout our infrastructure and deliver events to users within a couple milliseconds. We are able to target individual tabs, sessions, groups of users, exclude specific users and so on from any arbitrary event. We even use this system to handle messaging between components of our infrastructure.

The JSON-RPC servers are NodeJS applications and we publish all the events to them using Redis pub/sub. We rolled our own communication libraries for the client and server.

We're also in the process of switching out all our internal messaging to run through ZeroMQ rather than Redis. This will allow us more complex routing to more efficiently use the network resources we have. Such as to direct notifications only to the JSON-RPC Servers that require it rather than forcing every server to process every notification.

From emitting an event until it's being serialized for a socket is more like a few hundred microseconds. We're able to emit events at near wire speed, so there's virtually no cost to our real-time event emitting or processing. We're actively sending thousands of events per second with hardly a blip in system utilization. In fact, our biggest load network wide is dealing with SSL overhead.

**Add conversations to any site with a few lines of HTML. Gorgeous discussions, yours for free.**

# [Legacy Code Refactoring](http://stevenjackson.github.io/2013/09/22/legacy-code-refactoring/)

In refactoring, we consider two things: **structure and names**. When we focus on removing duplication, structure tends to improve, we have more indirection, with smaller pieces that we understand and test more easily. This introduces useful indirection, but doesn't help identify **useful abstraction**. For that, you need to work more on naming.

Find something with a vague name and make it more precise, even if that precise name is 78 letters long. Look for conjunctions in your names (*and, if, but, or, then*) to see where to split things apart. Look for unrelated names on the same line of code to see where to split things apart. Look for patterns in names (words within long names) to see which things to gather together. Look for related names in drastically different places to see which things to gather together. Gather related things together into small piles, then give those piles a useful name. Hide the pile behind a facade interface so that you can treat the pile as a single thing. That's **abstraction**, and that will help you with the higher-level design issues you're facing.

I've found that duplication relates to **coupling risks** and names relate to **cohesion risks**. Even when we solve the coupling problems, we can become lost in a sea of incoherence/non-cohesiveness, and especially when refactoring legacy code, high coupling merely gets in the way of doing what we want, whereas low cohesion means that we can't figure out what we want!

> Spend two hours naming stuff more precisely. Jot down words that you see scattered throughout the legacy code. Imagine you've moved the relevant parts of those things closer together. What would that get you? Something useful, I'd bet.

The more precise the names, the more potential to notice similar things scattered throughout the code, and the more opportunities to gather those things together into useful abstractions.

> **Sometimes I have to let big code remain big so that I can more easily see the patterns.**

Premature abstraction (hiding details, by definition) can make it harder to notice duplication of certain details scattered throughout the code. In legacy code specifically, I put a premium on making coupling explicit over trying to reduce it early. Like naming things precisely (which tends to make bad coupling more explicit), I need more, clearer examples to lead to noticing more potential patterns, which provide more options for refactoring.

> **If I extract too much too soon, then I risk falling in love with the wrong abstractions and start protecting them.**

I made peace long ago with the notion that when I see no good place to start, that means that I can comfortably start anywhere.

When I undertake a large restructuring, I know that progress happens pseudo-exponentially: I spend a very long time making things only slightly better until one day, suddenly, the design turns a corner. I've done this enough times that I trust the phenomenon, so those early weeks don't bother me as much as they used to. Remember this: once the first two or three key abstractions lock into place, the rest will come so much more quickly.

> **Abstraction means permission to ignore details**

In the first stage, I want to make the messiness of the structure more explicit, rather than worry about improving it significantly.

> **Making names more precise helps me see where to remove duplication and what kind of duplication to remove**

First, I **want** horrible names, because those horrible names point us towards what most significantly needs improvement. Second, I don't mind starting anywhere, although I prefer to start with the code I have to touch in order to fix a mistake or add a feature. I can learn a lot about a design by simply diving in somewhere arbitrary and refactoring, but I won't necessarily improve the design in any useful direction this way, except by luck. If I have a potentially-useful place to start, such as the code we need to change to fix defect 18723, then I prefer to start there; but if not, then it doesn't much matter where I start, so long as I start.

> **Extracting pure functions (forcing everything to a narrower scope) tends to help me identify the hot spots that need the most care and attention in splitting apart.**

As long as you have an awesome safety net, you can change code more aggressively and worry less about the consequences. For me, that safety net doesn't even include tests yet, but consists of two main things: an easy way to roll production changes back and a way for the existing code and potential replacement code to coexist.

A **code load balancer** that sits in front of the area I'm changing that sends only about 5% of traffic to the replacement version, which lets me experiment more safely with changes that might or might not preserve behavior exactly. Imagine you want to replace legacy code L with shiny new code S. Add a new load balancing function B in front of L that does this: "if rand() < 0.05 then S else L". This way if S is a disaster, then the system will only suck a little until you can roll back (or change 0.05 to 0.0). I hope this describes it adequately. If S works well, then you can change 0.05 to 0.2 for a while, then to 0.5, then finally remove B and L.

Often I find that when I remove duplication, I see structure emerging, but can't judge it as "better" or "worse" than before, whereas when I improving names and new structures emerge, they tend to improve cohesion more clearly, and allow for better abstraction, which reduces my cognitive load, which I can **feel** as less stress. "Yay! I don't have to know the details of what's going on here any more." That kind of thing.