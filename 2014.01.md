# [Unit Testing Your Production Code With Metrics And Logs](http://about.travis-ci.org/blog/2013-06-11-unit-testing-your-production-code/)

> We use Librato Metrics to store and visualize our data, and we're using Eric Lindvall's excellent metriks library.

Where metrics tell you all about what's going on, logs are the true soul of your application. Metrics only give you a numeric representation of what happened, logs tell you (ideally) why things happened. They're your audit trail of steps your application took to execute a specific task.

We're using Papertrail for log aggregation.

# [Performance Tuning](http://nerds.airbnb.com/performance-tuning/)

* DevTools
* PageSpeed Insights
* WebPagetest
* NewRelic

In our experience, most gains have been had from the simplest of rules: **reduce requests**. Sprite images in stylesheets, combine and minify multiple JavaScript files, and embed small images into data URIs; watch the dev tool waterfalls and make sure that you don't have assets blocking the download of other assets; and finally, put JavaScript at the bottom of the page.

We also cache non-personalized pages aggressively with Akamai.

* SPDY (HTTP pipelining)
* HTTP streaming; possible in Rails 3.1+, Node, and many other frameworks
* Cutting down bundled asset size; removing old JavaScript and CSS, and relying more heavily on our style platform
* Smarter edge caching with better global distribution

# [Redis Set Intersection - Using Sets to Filter Data](http://robots.thoughtbot.com/redis-set-intersection-using-sets-to-filter-data)

With Redis sets we can store records in groups based on time. The sunion operator then enables us to group multiple sets together based on a particular time frame.

Group flights by departure time
Group flights by city (departing & arriving)
Find flights from San Francisco to New York (`sinter`)

To find flights departing on 03/23/2013 between 1:00 pm and 3:00 pm, we'll need to group the sets of departure times, and then perform an intersection with the resulting set.

1. Create a new temp_set using sunionstore to group flights by departure time.
2. Intersect the temporary set with the departure and arrival sets.
3. Loop over the results of the intersection and generate an array of flight keys.
4. Use mget to fetch all the flight data.

# [Programmers Need To Learn Statistics Or I Will Kill Them All](http://zedshaw.com/essays/programmer_stats.html)

This article is my call for all programmers to finally learn enough about statistics to at least know they don't know shit.

I think women are better programmers because they have less ego and are typically more interested in the gear rather than the pissing contest.

A common element of process control statistics is that all processes have a period in the beginning where the process isn't stable. This "ramp-up" period is usually discarded when doing the analysis unless your run length has to include it. Most people think that 1000 is more than enough, but it totally depends on how the system functions. Many complex interacting systems can easily need 1000 iterations to get to a steady state, especially if performing 1000 transactions is very quick. Imagine a banking system that handles 10,000 transactions a second. I could take a good minute to get this system into a steady state, but your measly little 1000 transaction test is only scratching the surface.

You run 1000 test sequentially and then find out that the system blows up when you give it a parallel load. Now you're back to square one because the performance characteristics are different under parallel load.

The moral of the story is that if you give an average without standard deviations then you're totally missing the entire point of even trying to measure something. A major goal of measurement is to develop a succinct and accurate picture of what's going on, but if you don't find out the standard deviation and do at least a couple graphs then you're screwed. Just give up man. Game over. Game over.

Ah, confounding. The most difficult thing to explain to a programmer, yet the most elementary part of all scientific experimentation. It's pretty simple: **if you want to measure something, then don't measure other shit**.

Before you can measure something you really need to lay down a very concrete definition of what you're measuring. You should also try to measure the simplest thing you can and try to avoid confounding. Yet still I see software developers begging for gazillions of dollars to buy some crap tool that doesn't even mention "standard deviation".

Finally, you should check out the R Project for the programming language used in this article. It is a great language for this, with some of the best plotting abilities in the world. Learning to use R will help you also learn statistics better.

# [Securely store sensitive data in the DB?](https://news.ycombinator.com/item?id=6116243)

#### HIPAA

Put the MySQL data files on a partition which is block-level encrypted. Issue decryption keys to clients that need them, such as the application.

#### e-commerce

Restrict access to an external (read different machine/network segment), firewalled host that does the decryption. Encrypted data is passed to the service, which pulls the encryption key out of memory, decrypts the data, and sends it back to the requesting host. The encryption key is stored in (at least) two pieces, each piece is encrypted with a key encrypting key, key encrypting keys are know to very few employees, no single employee holds both key encrypting keys. The encryption keys is only assembled in its entirety while in memory.

#### Card info

You have a dedicated box that stores details and is remotely contacted through an XML-RPC/JSON-HTTP API of some sort.
The API should have two methods:

1. Add a new card to account.
2. Make payment of £xx from card NN.

The machine is locked down, runs no other services, and so cards cannot be exported/stolen from this system. You'd encrypt the filesystem and prompt for a key/passphrase at boot. Ideally you'd only login via the serial console so the only service exposed is your "add/charge" methods. Allowing the remote deletion of cards would be a security risk.

Payment gateways have it somewhat easy - the system making the "make a payment" request doesn't ever need the actual sensitive data returned. That data can be stored on a heavily secured server that only can call out to the banks.

#### General guidelines

**Separate your reads from your writes**. Using a public/private key pair, you can give one set of systems the ability to write encrypted data but not read it, and a different set of systems the ability to read the data. The systems that can decrypt / read data should be isolated as much as possible - don't expose them to public networks, limit which clients have access to them, etc. The separation also forces you to encapsulate your secure data and define an API over it; rather than arbitrary reads, hosts that don't have the decryption keys will have to ask the hosts that do to perform specific operations. If you're writing a Rails app, the [Strongbox gem](https://github.com/spikex/strongbox) enforces this pattern.

1. Keep passwords in memory (so as if you start the service it prompts for the password).
2. Asymmetrical crypto. Encrypt your CC data upon sign-up but then to run the charges you need the private key (and this is somewhere else).
3. Enable SSL communication with your DB. Postgres has this, because being defeated by network sniffing is bad.

[Translucent Databases](http://www.amazon.co.uk/dp/0967584418) with a lot of interesting use cases, where the assumption is that an adversary has gained access to the entire database.

> After all, the primary objective isn't to create an impenetrable system, but one that's exceptionally difficult to penetrate.

Re-use the principle of password hashes for API keys to simply avoid having to store hyper-sensitive data: generate a long (say 512-bit) secure random number (using OpenSSL) as the user's secret API key. Then hash the key as if it were a password and store only the hash. Now if someone steals your API key database they can't use it to authenticate as your users.

For API keys a strong hash such as bcrypt will probably be too slow and resource-intensive. However, because API keys are (long) random data, unlike passwords, you can use a faster hash function like SHA-1.

> Your best bet is to have three-factor authentication (something they are, something they have, something they know) generate a key to encrypt the data.

Use Asymmetric encryption and store the private key in an HSM. The private key never leaves the HSM device.

In Lotus/IBM Notes ID, the password is used in a KDF to generate a key, which in turn decrypts the user's private key (and certain other credentials, along with symmetric secret keys for shared encrypted doccuments). Success/failure is determined solely by the successful decryption of known bytes in the encrypted package. Other info (the user's public key, identity and certifier, all signed) are maintained in the clear and can be easily and safely exported and may be "trusted" for authentication with remote machines. There is a "password recovery" system as well (it doesn't actually recover the password, but allows a reset), requiring cooperation of two or more admins¹ (in a Shamir-type arrangement) so that previously-encrypted user data will not be lost.

# [That Wibbly Wobbly Real-Timey Wimey stuff](https://moot.it/blog/technology/real-timey-wimey-stuff.html)

We bubble up events and listen to events throughout our infrastructure and deliver events to users within a couple milliseconds. We are able to target individual tabs, sessions, groups of users, exclude specific users and so on from any arbitrary event. We even use this system to handle messaging between components of our infrastructure.

The JSON-RPC servers are NodeJS applications and we publish all the events to them using Redis pub/sub. We rolled our own communication libraries for the client and server.

We're also in the process of switching out all our internal messaging to run through ZeroMQ rather than Redis. This will allow us more complex routing to more efficiently use the network resources we have. Such as to direct notifications only to the JSON-RPC Servers that require it rather than forcing every server to process every notification.

From emitting an event until it's being serialized for a socket is more like a few hundred microseconds. We're able to emit events at near wire speed, so there's virtually no cost to our real-time event emitting or processing. We're actively sending thousands of events per second with hardly a blip in system utilization. In fact, our biggest load network wide is dealing with SSL overhead.

**Add conversations to any site with a few lines of HTML. Gorgeous discussions, yours for free.**

# [Legacy Code Refactoring](http://stevenjackson.github.io/2013/09/22/legacy-code-refactoring/)

In refactoring, we consider two things: **structure and names**. When we focus on removing duplication, structure tends to improve, we have more indirection, with smaller pieces that we understand and test more easily. This introduces useful indirection, but doesn't help identify **useful abstraction**. For that, you need to work more on naming.

Find something with a vague name and make it more precise, even if that precise name is 78 letters long. Look for conjunctions in your names (*and, if, but, or, then*) to see where to split things apart. Look for unrelated names on the same line of code to see where to split things apart. Look for patterns in names (words within long names) to see which things to gather together. Look for related names in drastically different places to see which things to gather together. Gather related things together into small piles, then give those piles a useful name. Hide the pile behind a facade interface so that you can treat the pile as a single thing. That's **abstraction**, and that will help you with the higher-level design issues you're facing.

I've found that duplication relates to **coupling risks** and names relate to **cohesion risks**. Even when we solve the coupling problems, we can become lost in a sea of incoherence/non-cohesiveness, and especially when refactoring legacy code, high coupling merely gets in the way of doing what we want, whereas low cohesion means that we can't figure out what we want!

> Spend two hours naming stuff more precisely. Jot down words that you see scattered throughout the legacy code. Imagine you've moved the relevant parts of those things closer together. What would that get you? Something useful, I'd bet.

The more precise the names, the more potential to notice similar things scattered throughout the code, and the more opportunities to gather those things together into useful abstractions.

> **Sometimes I have to let big code remain big so that I can more easily see the patterns.**

Premature abstraction (hiding details, by definition) can make it harder to notice duplication of certain details scattered throughout the code. In legacy code specifically, I put a premium on making coupling explicit over trying to reduce it early. Like naming things precisely (which tends to make bad coupling more explicit), I need more, clearer examples to lead to noticing more potential patterns, which provide more options for refactoring.

> **If I extract too much too soon, then I risk falling in love with the wrong abstractions and start protecting them.**

I made peace long ago with the notion that when I see no good place to start, that means that I can comfortably start anywhere.

When I undertake a large restructuring, I know that progress happens pseudo-exponentially: I spend a very long time making things only slightly better until one day, suddenly, the design turns a corner. I've done this enough times that I trust the phenomenon, so those early weeks don't bother me as much as they used to. Remember this: once the first two or three key abstractions lock into place, the rest will come so much more quickly.

> **Abstraction means permission to ignore details**

In the first stage, I want to make the messiness of the structure more explicit, rather than worry about improving it significantly.

> **Making names more precise helps me see where to remove duplication and what kind of duplication to remove**

First, I **want** horrible names, because those horrible names point us towards what most significantly needs improvement. Second, I don't mind starting anywhere, although I prefer to start with the code I have to touch in order to fix a mistake or add a feature. I can learn a lot about a design by simply diving in somewhere arbitrary and refactoring, but I won't necessarily improve the design in any useful direction this way, except by luck. If I have a potentially-useful place to start, such as the code we need to change to fix defect 18723, then I prefer to start there; but if not, then it doesn't much matter where I start, so long as I start.

> **Extracting pure functions (forcing everything to a narrower scope) tends to help me identify the hot spots that need the most care and attention in splitting apart.**

As long as you have an awesome safety net, you can change code more aggressively and worry less about the consequences. For me, that safety net doesn't even include tests yet, but consists of two main things: an easy way to roll production changes back and a way for the existing code and potential replacement code to coexist.

A **code load balancer** that sits in front of the area I'm changing that sends only about 5% of traffic to the replacement version, which lets me experiment more safely with changes that might or might not preserve behavior exactly. Imagine you want to replace legacy code L with shiny new code S. Add a new load balancing function B in front of L that does this: "if rand() < 0.05 then S else L". This way if S is a disaster, then the system will only suck a little until you can roll back (or change 0.05 to 0.0). I hope this describes it adequately. If S works well, then you can change 0.05 to 0.2 for a while, then to 0.5, then finally remove B and L.

Often I find that when I remove duplication, I see structure emerging, but can't judge it as "better" or "worse" than before, whereas when I improving names and new structures emerge, they tend to improve cohesion more clearly, and allow for better abstraction, which reduces my cognitive load, which I can **feel** as less stress. "Yay! I don't have to know the details of what's going on here any more." That kind of thing.

# [The Four Elements of Simple Design](http://www.jbrains.ca/permalink/the-four-elements-of-simple-design)

A design is simple to the extent that it:

1. Passes its tests

2. Minimizes duplication

3. Maximizes clarity

4. Has fewer elements

I’m willing to copy-and-paste to get a test passing, but once the test passes, I can usually remove the duplication quickly. 

I’m willing to extract code into a method and call it `foo` in order to get rid of duplicate code, although that name `foo` rarely survives more than 15 minutes.

Finally, I will gladly introduce interfaces, classes, methods and variables to clarify the intent of a piece of code, although generally speaking once I make things more clear, I can find things to cut.

> Removing duplication tends to allow a suitable structure to emerge, whereas bad names highlight an inappropriate distribution of responsibilities. I use this observation as a key element of my demonstration, **Architecture Without Trying**.

Names tend to go through four stages: nonsense, accurate, precise, meaningful. Laziness or ignorance push us towards the left end of this spectrum, while with diligence we can move to the right. I contend that names further to the right of this spectrum provide more clarity. This, too, matches Kent Beck’s original conception of this idea, as he referred to “intention-revealing names”.

When I find fifteen lines of duplicate code, I start by extracting them to a new method, and since I probably don’t yet know what those lines of code do yet, I name the new method `foo`. After around 15 minutes of working in the same area, I begin to understand what this method does, so I give it an accurate name, such as `computeCost`. As I try to write tests for this new method, and find it more tedious than I expected, I realise that it does more than just compute the cost of an item, so I rename it more precisely: `findItemThenComputeCost`. The mere appearance of a conjunction, like **and**, **or**, **but** or **then** tells me that the method has more than one responsibility, and as a result, I soon find myself writing tests that duplicate irrelevant details.

> **Meaningful names that communicate the story of the design**

When I remove duplication, I tend to see an appropriate structure emerge, and when I fix bad names, I tend to see responsibilities slide into appropriate parts of the design.

> **If you master removing duplication and fixing bad names, then I claim you master object-oriented design.**

After practising TDD for some time, people will naturally adjust how many decisions they make *up front* over time and depending on the situation. I never expect anyone to **really** and **forever** make **literally** no design decisions before writing the first test. I notice, however, a big difference in the up-front decisions I make today compared to 15 years ago.

Regarding public methods, I don't use private/public to hide methods any more, but instead I use interface/implementation. Public methods don't break open/closed principle, but rather too many behaviors in the same place break open/closed principle. If your class has three groups of public methods, and each group works independently of the others, then it's *probable* that your class has a cohesion problem. You can split the class up, or you can use the class through one of its three interfaces (ISP helps here) without knowing that the class implements services you don't need.

> **There is no single, perfect answer, so I look for simple rules and guidelines to help me make good decisions, and to try to make the rules and guidelines smaller and simpler over time.**

We always learn something after we start designing. It is unsafe to choose (and stay with) a specific design, but it is safe to choose an approach to design that makes it easier to change decisions when we need to.

# [Putting an Age-Old Battle to Rest](http://blog.thecodewhisperer.com/2013/12/07/putting-an-age-old-battle-to-rest/)

Tests help me limit the amount of code that I write.

Removing duplication and improving names helps me reduce the liability (cost) of the code that I write.

Together, they help me reduce both the total cost and the volatility of the cost of the features I deliver.

<pre>

       structure emerges   >  structures need names

               ^                      v

     *remove duplication*       *improve names*

               ^                      v

higher-level duplication   <  abstractions emerge
</pre>

**Improve names and remove duplication in small cycles**

# [SOA and Agile: Friends? Enemies? Frenemies?](http://www.jbrains.ca/permalink/soa-and-agile-friends-enemies-frenemies)

Make your Services as small as you can, because smaller ones tend not to need to change.

Drive down the risk of changing a Service incorrectly by writing solid specifications.

Plan to migrate Services from the beginning.

Why limit the above to just Web Services and SOA?

# [Getting Unstuck: Unleashing the Expert Within!](http://www.jbrains.ca/permalink/getting-unstuck-unleashing-the-expert-within)

Looking back, building my training skill and experience helped me a lot. People buy training, in part because they want someone to tell them what to do next.

Do you think that you have enough information to keep a group of technicians (programmers, testers, business analysts, whoever you want to train) busy for 2 hours and give them the vague feeling that they’ve learned something? Great! You can start there.

Do you have a 1-hour presentation in which you describe how to do something specific (**as opposed to ranting about varied things**)? Great! You can start there, too.

> Add a 1-hour exercise to a 1-hour presentation and you have a 2-hour training mini-course.

Now ask every user group that could possibly be interested to let you present your mini-course to their audience. The first few times you do this, you will probably suck. Don’t lose hope; but rather, get those experiences out of the way, so that you can start kicking ass, or at least not sucking. You’ll get better with practice.

Is your audience falling asleep during your presentation, but engaged during the exercise? Less talking, more doing.

Is your audience asking you really tough questions? Great! More fieldstones! These questions become future “modules” in your training course. If you listen to your audience, they’ll tell you what to teach next.

After you teach a handful of these fieldstone mini-courses, you’ll almost certainly have a common theme that you can turn into a 3-day course.

Every training course I offer started out as a handful of 30-minute talks and 2-hour mini-courses.

After you’ve run your fieldstone courses at user groups and people get to know and trust you, submit them to conferences to run either as workshops during the main conference or as pre-conference tutorials. Many conferences offer tutorials for an extra fee and will split the revenue with you. This gives you some of the best feedback of all: will people pay for this? When you see that people love your mini-courses, then you know which fieldstones to flesh out into full-fledged courses. You can stitch 2 or 3 fieldstone mini-courses together, add a few more exercises, and sell it as 3 days of world class training… and people will buy it.

Remember that thing about getting people to like you? Don’t worry about that. Your enthusiasm for the topic you’ve just selected will carry you through. Just get yourself in front of an audience and they’ll love you.

# [What matters most when writing code?](http://www.jbrains.ca/permalink/what-matters-most-when-writing-code-readability-performance)

**The Contractor Effect**: whoever works on your house next will start by telling you what an idiot the last contractor was and that that person did almost everything wrong.

I have, in the past, refactored code for its own sake, and it took me a few years to stop doing that. I had to remember the advice I've read from Martin Fowler and Ron Jeffries and Kent Beck **only to refactor code that needs refactoring right now, as much as it needs refactoring**.

When I make code more readable, I save time **now**, and not only **potentially later**. In particular, the more I understand code now, the better mental model I have of it, the better I understand its structure, and the more easily I can visualise it to see how to bend and shape it as I work with it. Also, naming things well helps me spot naming patterns, and this helps me identify structures struggling to come into existence.

> Legacy code is typically full of half-born abstractions copied and scattered and mutated throughout the code base.

I don't think we can really decide what matters most at compile time, nor even at execution time, but until we have to change the code after leaving it long enough to have forgot all the key details.

> **The more easily I can read my code, the more easily I can refactor it, the more safely I can extend it, the more quickly and confidently I can change it.**

# [Refactoring Without Good Tests](http://blog.codeclimate.com/blog/2013/12/05/refactoring-without-good-tests/)

Most of the time, when you test drive code you'll write your code "outside in" - focusing on the interface that the test needs to validate before thinking about the implementation you'll need to deliver. It also makes it more likely that you'll create classes with a narrow responsibilities that are loosely coupled, as the excessive setup required for testing tightly coupled code will quickly move you towards reducing your coupling.

> What are the most important things that your key audiences need to be able to do through the app? Create a handful of concrete scenarios for each user journey and write automated acceptance tests for them.

Test drive all of new code.

Whenever you find a bug, start by writing a failing test at an appropriate level (ideally a unit test).

# [using elasticsearch and logstash to serve billions of searchable events for customers](http://www.elasticsearch.org/blog/using-elasticsearch-and-logstash-to-serve-billions-of-searchable-events-for-customers/)

A great way to use Elasticsearch’s very rich API is by setting up Kibana, a tool to “make sense of a mountain of logs”. The new incarnation, Kibana 3, is fully client side JavaScript, and will be the default interface for Logstash. Unlike previous versions, it no longer depends on a Logstash-like schema, but is now usable for any Elasticsearch index.

One way to manage retention in Elasticsearch is by setting TTLs on each document. Elasticsearch will then periodically delete expired documents in bulk. This makes it easy to do custom retention per account. However, it turns out to be a rather expensive (repetitive) operation.

**A much lighter weight operation is dropping entire indices. This is why Logstash defaults to daily indices.**

# [How To Survive a Ground-Up Rewrite Without Losing Your Sanity](http://onstartups.com/tabid/3339/bid/97052/Screw-You-Joel-Spolsky-We-re-Rewriting-It-From-Scratch.aspx)

The first, absolutely critical thing to understand about launching a major rewrite is that it's going to take insanely longer than you expect.

Migrating the data sucks beyond all belief. Because you have a lot of data, it often takes days to migrate it all.

It's brutally hard to reduce scope. People are really unhappy if you tell them: hey, we rewrote your favorite part of the product, the code is a lot cleaner now, but we took away half the functionality.

First off, before you start, you must define the business value of this rewrite. Good example:

* cut cost of storing each hit by an order of magnitude
* create new reports that weren't possible in the old system
* serve all reports faster
* serve near-real-time (instead of cached daily) reports

Over my career, I've come to place a really strong value on figuring out how to break big changes into small, safe, value-generating pieces. It's a sort of meta-design - designing the process of gradual, safe change.

**Kent Beck on Succession:**

> Design changes are usually most efficiently implemented as a series of safe steps. Succession is the art of taking a single conceptual change, breaking it into safe steps, and then finding an order for those steps that optimizes safety, feedback, and efficiency.

False incrementalism is breaking a large change up into a set of small steps, but where none of those steps generate any value on their own.

> If after each increment, an Important Person were to ask your team to drop the project right at that moment, would the business have seen some value?

An economic tradeoff where they can chose between options of what they see when.

**Always insert that dual-write layer**. It's a minor, generally somewhat fixed cost that buys you an incredible amount of insurance.

**Kent Beck, on migrating large amounts of data at Facebook:**

> Convert data fetching and mutating to a DataType, an abstraction that hides where the data is stored.
>
> Modify the DataType to begin writing the data to the new store as well as the old store.
> 
> Bulk migrate existing data.
>
> Modify the DataType to read from both stores, checking that the same data is fetched and logging any differences.
>
> When the results match closely enough, return data from the new store and eliminate the old store.

If you have any uncertainty about how long it's going to take, sequence your work to reduce that uncertainty right away, and give people some "finished" thing that will let them walk away.

**Treat your migration code as a first class citizen. It will save you a lot of time in the long run.**

A good rule of thumb for migrating and checksumming data: until you've found a half-dozen bizarre inconsistencies in the old data, you're not done.

If you learn to approach your rewrites with this kind of ferocious, incremental discipline, you can tackle incredibly hard problems without fear.

# [Infrastructure automation by example](https://practicingruby.com/articles/infrastructure-automation?u=c94a53804e)

I vastly underestimated the value of treating "infrastructure as code", especially as it relates to creating systems that are abstract, modular, testable, understandable, and utterly hackable. Narrowing the problem down to the single issue of reducing repetitive labor, I had failed to see that infrastructure automation has the potential to eliminate an entire class of problems associated with manual system configuration.

# [Rasperry Pi weather station](http://hveem.no/raspberry-pi-davis-vue-weather-station-with-custom-frontend)

# [Cooking up an Office Dashboard Pi](https://gocardless.com/blog/raspberry-pi-metric-dashboards/)

* Dashing
* redis-objects
* CircleCI
* x11
* unclutter
* midori

# [Run Docker on any OS in a Headless Hypervisor](http://continuousdelivery.uglyduckling.nl/docker/run-docker-on-any-os-in-a-headless-hypervisor/)

Imagine Docker included a Headless Hypervisor. If you wanted to execute a Docker command on your MacBook Air, Docker would start a hidden VM image with a Linux OS running Docker. All the networking and file sharing is configured by Docker. Once this VM is running, all Docker commands will be executed inside the VM through a proxy or directly with ssh.

#### [boot2docker](https://github.com/steeve/boot2docker)

boot2docker is a lightweight Linux distribution based on Tiny Core Linux made specifically to run Docker containers. It runs completely from RAM, weighs ~23MB and boots in ~5-6s (YMMV).

#### [docker-osx](https://github.com/noplay/docker-osx)

This script acts as both an installer and a Docker binary. On first run, it installs an OS X binary of the Docker client and starts a virtual machine with the Docker daemon running. It then passes through all invocations of docker to the client which controls the daemon running on the virtual machine.

The virtual machine that Docker runs on is given the hostname localdocker. For example, if you run docker run -p 8000:8000 ..., then that will be available at localdocker:8000 from OS X.