# [Unit Testing Your Production Code With Metrics And Logs](http://about.travis-ci.org/blog/2013-06-11-unit-testing-your-production-code/)

> We use Librato Metrics to store and visualize our data, and we're using Eric Lindvall's excellent metriks library.

Where metrics tell you all about what's going on, logs are the true soul of your application. Metrics only give you a numeric representation of what happened, logs tell you (ideally) why things happened. They're your audit trail of steps your application took to execute a specific task.

We're using Papertrail for log aggregation.

# [Performance Tuning](http://nerds.airbnb.com/performance-tuning/)

* DevTools
* PageSpeed Insights
* WebPagetest
* NewRelic

In our experience, most gains have been had from the simplest of rules: **reduce requests**. Sprite images in stylesheets, combine and minify multiple JavaScript files, and embed small images into data URIs; watch the dev tool waterfalls and make sure that you don't have assets blocking the download of other assets; and finally, put JavaScript at the bottom of the page.

We also cache non-personalized pages aggressively with Akamai.

* SPDY (HTTP pipelining)
* HTTP streaming; possible in Rails 3.1+, Node, and many other frameworks
* Cutting down bundled asset size; removing old JavaScript and CSS, and relying more heavily on our style platform
* Smarter edge caching with better global distribution

# [Redis Set Intersection - Using Sets to Filter Data](http://robots.thoughtbot.com/redis-set-intersection-using-sets-to-filter-data)

With Redis sets we can store records in groups based on time. The sunion operator then enables us to group multiple sets together based on a particular time frame.

Group flights by departure time
Group flights by city (departing & arriving)
Find flights from San Francisco to New York (`sinter`)

To find flights departing on 03/23/2013 between 1:00 pm and 3:00 pm, we'll need to group the sets of departure times, and then perform an intersection with the resulting set.

1. Create a new temp_set using sunionstore to group flights by departure time.
2. Intersect the temporary set with the departure and arrival sets.
3. Loop over the results of the intersection and generate an array of flight keys.
4. Use mget to fetch all the flight data.

# [Programmers Need To Learn Statistics Or I Will Kill Them All](http://zedshaw.com/essays/programmer_stats.html)

This article is my call for all programmers to finally learn enough about statistics to at least know they don't know shit.

I think women are better programmers because they have less ego and are typically more interested in the gear rather than the pissing contest.

A common element of process control statistics is that all processes have a period in the beginning where the process isn't stable. This "ramp-up" period is usually discarded when doing the analysis unless your run length has to include it. Most people think that 1000 is more than enough, but it totally depends on how the system functions. Many complex interacting systems can easily need 1000 iterations to get to a steady state, especially if performing 1000 transactions is very quick. Imagine a banking system that handles 10,000 transactions a second. I could take a good minute to get this system into a steady state, but your measly little 1000 transaction test is only scratching the surface.

You run 1000 test sequentially and then find out that the system blows up when you give it a parallel load. Now you're back to square one because the performance characteristics are different under parallel load.

The moral of the story is that if you give an average without standard deviations then you're totally missing the entire point of even trying to measure something. A major goal of measurement is to develop a succinct and accurate picture of what's going on, but if you don't find out the standard deviation and do at least a couple graphs then you're screwed. Just give up man. Game over. Game over.

Ah, confounding. The most difficult thing to explain to a programmer, yet the most elementary part of all scientific experimentation. It's pretty simple: **if you want to measure something, then don't measure other shit**.

Before you can measure something you really need to lay down a very concrete definition of what you're measuring. You should also try to measure the simplest thing you can and try to avoid confounding. Yet still I see software developers begging for gazillions of dollars to buy some crap tool that doesn't even mention "standard deviation".

Finally, you should check out the R Project for the programming language used in this article. It is a great language for this, with some of the best plotting abilities in the world. Learning to use R will help you also learn statistics better.

# [Securely store sensitive data in the DB?](https://news.ycombinator.com/item?id=6116243)

#### HIPAA

Put the MySQL data files on a partition which is block-level encrypted. Issue decryption keys to clients that need them, such as the application.

#### e-commerce

Restrict access to an external (read different machine/network segment), firewalled host that does the decryption. Encrypted data is passed to the service, which pulls the encryption key out of memory, decrypts the data, and sends it back to the requesting host. The encryption key is stored in (at least) two pieces, each piece is encrypted with a key encrypting key, key encrypting keys are know to very few employees, no single employee holds both key encrypting keys. The encryption keys is only assembled in its entirety while in memory.

#### Card info

You have a dedicated box that stores details and is remotely contacted through an XML-RPC/JSON-HTTP API of some sort.
The API should have two methods:

1. Add a new card to account.
2. Make payment of £xx from card NN.

The machine is locked down, runs no other services, and so cards cannot be exported/stolen from this system. You'd encrypt the filesystem and prompt for a key/passphrase at boot. Ideally you'd only login via the serial console so the only service exposed is your "add/charge" methods. Allowing the remote deletion of cards would be a security risk.

Payment gateways have it somewhat easy - the system making the "make a payment" request doesn't ever need the actual sensitive data returned. That data can be stored on a heavily secured server that only can call out to the banks.

#### General guidelines

**Separate your reads from your writes**. Using a public/private key pair, you can give one set of systems the ability to write encrypted data but not read it, and a different set of systems the ability to read the data. The systems that can decrypt / read data should be isolated as much as possible - don't expose them to public networks, limit which clients have access to them, etc. The separation also forces you to encapsulate your secure data and define an API over it; rather than arbitrary reads, hosts that don't have the decryption keys will have to ask the hosts that do to perform specific operations. If you're writing a Rails app, the [Strongbox gem](https://github.com/spikex/strongbox) enforces this pattern.

1. Keep passwords in memory (so as if you start the service it prompts for the password).
2. Asymmetrical crypto. Encrypt your CC data upon sign-up but then to run the charges you need the private key (and this is somewhere else).
3. Enable SSL communication with your DB. Postgres has this, because being defeated by network sniffing is bad.

[Translucent Databases](http://www.amazon.co.uk/dp/0967584418) with a lot of interesting use cases, where the assumption is that an adversary has gained access to the entire database.

> After all, the primary objective isn't to create an impenetrable system, but one that's exceptionally difficult to penetrate.

Re-use the principle of password hashes for API keys to simply avoid having to store hyper-sensitive data: generate a long (say 512-bit) secure random number (using OpenSSL) as the user's secret API key. Then hash the key as if it were a password and store only the hash. Now if someone steals your API key database they can't use it to authenticate as your users.

For API keys a strong hash such as bcrypt will probably be too slow and resource-intensive. However, because API keys are (long) random data, unlike passwords, you can use a faster hash function like SHA-1.

> Your best bet is to have three-factor authentication (something they are, something they have, something they know) generate a key to encrypt the data.

Use Asymmetric encryption and store the private key in an HSM. The private key never leaves the HSM device.

In Lotus/IBM Notes ID, the password is used in a KDF to generate a key, which in turn decrypts the user's private key (and certain other credentials, along with symmetric secret keys for shared encrypted doccuments). Success/failure is determined solely by the successful decryption of known bytes in the encrypted package. Other info (the user's public key, identity and certifier, all signed) are maintained in the clear and can be easily and safely exported and may be "trusted" for authentication with remote machines. There is a "password recovery" system as well (it doesn't actually recover the password, but allows a reset), requiring cooperation of two or more admins¹ (in a Shamir-type arrangement) so that previously-encrypted user data will not be lost.

# [That Wibbly Wobbly Real-Timey Wimey stuff](https://moot.it/blog/technology/real-timey-wimey-stuff.html)

We bubble up events and listen to events throughout our infrastructure and deliver events to users within a couple milliseconds. We are able to target individual tabs, sessions, groups of users, exclude specific users and so on from any arbitrary event. We even use this system to handle messaging between components of our infrastructure.

The JSON-RPC servers are NodeJS applications and we publish all the events to them using Redis pub/sub. We rolled our own communication libraries for the client and server.

We're also in the process of switching out all our internal messaging to run through ZeroMQ rather than Redis. This will allow us more complex routing to more efficiently use the network resources we have. Such as to direct notifications only to the JSON-RPC Servers that require it rather than forcing every server to process every notification.

From emitting an event until it's being serialized for a socket is more like a few hundred microseconds. We're able to emit events at near wire speed, so there's virtually no cost to our real-time event emitting or processing. We're actively sending thousands of events per second with hardly a blip in system utilization. In fact, our biggest load network wide is dealing with SSL overhead.

**Add conversations to any site with a few lines of HTML. Gorgeous discussions, yours for free.**

# [Legacy Code Refactoring](http://stevenjackson.github.io/2013/09/22/legacy-code-refactoring/)

Q: The problem: I’m disatisfied with my feedback loop while refactoring larger pieces of legacy code. What can I do differently?

In refactoring, we consider two things: structure and names. When we focus on removing duplication, structure tends to improve, we have more indirection, with smaller pieces that we understand and test more easily. I will guess that your refactoring has mostly focused on structural/duplication issues. This introduces useful indirection, but doesn't help identify useful *abstraction*. For that, you need to work more on names.