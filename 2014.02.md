# [Structuring Sinatra Applications](http://blog.sourcing.io/structuring-sinatra)

Our application has one main route: App. If we want to go about creating another route, we just need to mount it on App. Essentially, each route is its own separate application.

De-couple your application by splitting it up into lots of smaller applications mounted under one base. Believe me, this is the secret to keeping things simple and clean.

Database-wise I always choose Postgres. I've found, in the long run, document orientated databases aren't worth the hassle. Postgres is incredibly flexible, rock solid, and has never let me down.

For an ORM I like to use Sequel.  I also use the project sinatra-sequel, which adds a nice DSL around setting up a database.

I use CoffeeScript for JS and Stylus for CSS on the client side, and compile them using Sprockets. Even if you're not using a language that needs to be compiled, I recommend using an asset manager like Sprockets to give you concatenation and cache expiry.

# [Docker Nginx and Sentiment Engine on Steroids](http://shrikar.com/blog/2014/02/02/docker-nginx-and-sentiment-engine-on-steroids/)

Modify `/etc/security/limits.conf` to have a high number for open file descriptors.

`net.ipv4.ip_local_port_range` nginx needs to create two connection for every request, one to the client and the other one to the upstream server. Increasing the port range will prevent for port exhaustion.

`net.ipv4.tcp_fin_timeout` minimum number of seconds that must elapse before a connection in TIME_WAIT state can be recycled. Lowering this value will mean allocations will be recycled faster.

`net.ipv4.tcp_tw_recycle` enables fast recycling of TIME_WAIT sockets. Use with caution and ONLY in internal network where network connectivity speeds are “faster”.

`net.ipv4.tcp_tw_reuse` allows reusing sockets in TIME_WAIT state for new connections when it is safe from protocol viewpoint. Default value is 0 (disabled). It is generally a safer alternative to tcp_tw_recycle. Note: The tcp_tw_reuse setting is particularly useful in environments where numerous short connections are open and left in TIME_WAIT state, such as web servers. Reusing the sockets can be very effective in reducing server load.

`sudo sysctl -p` after making modifications to sysctl.conf.

[High-Performance Browser Networking](http://chimera.labs.oreilly.com/books/1230000000545/index.html)

[Optimising NginX, Node.JS and networking for heavy workloads](https://engineering.gosquared.com/optimising-nginx-node-js-and-networking-for-heavy-workloads)

# [Redis Master-Slave on Docker](http://www.hvflabs.com/posts/redis-master-slave-on-docker)

A base image which supports different versions of Redis (2.6 & 2.8). This base image will be used as the FROM for our “runtime” images. This will also be used for "cli containers" which I'll explain in a bit.

A standalone runtime image which will provide a data volume and expose the redis port and run the actual server.

A slave runtime image which will link to the standalone image and slave off it. This image will have a startup script which will inspect the environment created by the Docker linking mechanism.

# [Crash Course: Next Generation Servers With Containers](http://www.sebastien-han.fr/blog/2014/02/03/crash-course-next-generation-servers-with-containers/)

Namespace is a Kernel feature that brings isolation at different levels of the system. Each isolation level represents a specific resource.

**PID** allows us to isolate a process PID behind a parent process. It is important to note that each PID namespace has its own init-like process (PID 1).

**Network** provides an isolated network interface and a complete isolated traffic. A common technique that is used to route the traffic outside the namespace is to rely on a bridge. That is for example what Docker does, it setups a bridge and attach every namespace vnet to it.

**Filesystem** supports multiple mountpoints. As always, these mountpoints can only be seen inside the namespace.

**IPC**

cgroup does resource limiting and consumption accounting. It allows you to restrict numerous metrics of a running Linux such as:

**CPU** general cpu cycles control
**CPUSET** control which process can use which cpu
**BLKIO** block layer control, IOPS and bandwidth are managed from here
**MEM** pages allocation, cache

Depending on your Linux distribution, usually cgroup sits under the sys virtual filesystem at `/sys/fs/cgroup`.

However, Docker is not the only project that relies on namespaces and cgroup. CoreOS heavily uses systemd, so it is a good distribution to play with systemd capabilities in addition to the native Docker support.

# [Docker and Logstash: Smarter log management for your containers](http://denibertovic.com/post/docker-and-logstash-smarter-log-management-for-your-containers/)

[Dockerfile to set up logstash](https://github.com/denibertovic/logstash-dockerfile)

Now, the logstash service is kind of heavy on the resources so I didn't want to run a logstash container on every host machine but rather I wanted co collect logs from every container on every host machine and send it to a central logstash server. That's where [logstash-forwarder](https://github.com/elasticsearch/logstash-forwarder) comes in.

Logstash-forwarder can be configured to watch certain directories and files but I was more interested in the ability to listen on `stdin`.

`docker run -name logstash -p 9292:9292 -d -t logstash`

# [Docker quicktip #3 – ONBUILD](http://www.tech-d.net/2014/02/06/docker-quicktip-3-onbuild/)

ONBUILD is a new instruction for the Dockerfile. It is for use when creating a base image and you want to defer instructions to child images.

ONBUILD gets run just after the FROM and before any other instructions in a child image.

You can also have multiple ONBUILD instructions.

Use it to defer build instructions to images built from a base image. Use it to more easily build images from a common base but differ in some way, such as different git branches, or different projects entirely.

# [Docker Quicktip #2: exec it, please!](http://www.tech-d.net/2014/01/27/docker-quicktip-2-exec-it/)

As it stands right now if we try to stop this container docker will hang for a few seconds and then just kill it.

The signals to stop the process are being sent to the startup script and not postgres.

Docker allows you to proxy all signals (this is enabled by default) to the running process in the container. Need to send HUP to the running process in the container? Send it to the docker container process.

# [The exec builtin command](http://wiki.bash-hackers.org/commands/builtin/exec)

The exec builtin command is used to

* replace the shell with a given program (executing it, not as new process)
* set redirections for the program to execute or for the current shell

# [Workflows of Refactoring](http://martinfowler.com/articles/workflowsOfRefactoring/)

# [The Trello Tech Stack](http://blog.fogcreek.com/the-trello-tech-stack/)

Trello started as an HTML mockup that was put together in a week.

Well-written CoffeeScript smooths out and shortens JavaScript, while maintaining the same semantics, and does not introduce a substantial debugging indirection problem.

The Client

* Backbone.js (client-side MVC)
* HTML5 pushState
* Mustache (templating language)

The Server

* node.js
* HAProxy
* Redis
* MongoDB

# [We spent a week making Trello boards load extremely fast. Here’s how we did it.](http://blog.fogcreek.com/we-spent-a-week-making-trello-boards-load-extremely-fast-heres-how-we-did-it/)

Heavy styles like borders, shadows, and gradients can really slow down a browser.

Instead of spending all of the browser’s time generating one huge amount of DOM to insert, we could generate a small amount of DOM, insert it, generate another small amount, insert it, and so forth, so that the browser could free up the UI thread, paint something quickly, and prevent locking up. This really did the trick.

That was the whole week. If rendering on your web client is slow, look for excessive paints and layouts. I highly recommend using Chrome DevTool’s Timeline to help you find trouble areas. If you’re in a situation where you need to render a lot of things at once, look into async.queue or some other progressive rendering.

# [In praise of _boring_ technology](http://labs.spotify.com/2013/02/25/in-praise-of-boring-technology/)

More often than not, the right tool for the job is piece of software that has been around for some time, with proven success.

At Spotify we use DNS heavily. When most people think of DNS they think of A records, used to map a hostname to an IPv4 address. However, DNS can be so much more than a “phone book”. It is actually better to think of DNS as a distributed, replicated database tailored for read-heavy loads.

Service discovery is a fairly large subject but in essence it boils down to answering the question: what servers run this service.

At Spotify we have solved this problem with DNS for a long time. Specifically, we use so called SRV records. A DNS SRV record is a canonical name, typically of the form _name._protocol.site, mapped to a list of hostnames with weight, priority and port number.

<pre>
$ dig +short _spotify-client._tcp.spotify.com SRV
10 12 4070 A1.spotify.com.
10 12 4070 A2.spotify.com.
10 12 4070 A3.spotify.com.
10 12 4070 A4.spotify.com.
</pre>

The fields given above are, in order, the priority, the weight, the port number and the host. The user of a service should pick servers proportional to the weight, in the group of servers with the highest priority. Only when all servers are down should requests be sent to the servers with lower priority.

All backend services powering Spotify can be discovered by asking for the correct SRV record, and this is how the services find each other. For cases when DNS is not appropriate, such as externally visible web services, we have developed HTTP facades in front of DNS. For instance, some clients will send a HTTP request to figure out which Access Point to connect to.

In some (rare) cases, Spotify also use CNAME and PTR records in addition to SRV records. This is typically when a single server has some particular purpose. For example, we use a PTR record to locate the write master in one of our database clusters.

For administrative purposes, many tools and other infrastructural projects have been developed. One example is command line utilities that make SRV lookups easier. We also have a web service that works as a DNS cache that has some interesting features, such as reverse lookups (mapping a hostname to a service name).

As DNS is a high performance distributed database, we also use it for storing some service configuration data. In particular, we store DHT ring information in TXT records.

Our DNS servers run Bind and many of the backend services run Unbound which is also a DNS server. The use of Unbound is to improve performance and reduce load of the main DNS servers.

We have a git (previously subversion) repository where we maintain the DNS zone files. We have a small Makefile we run on the DNS master to deploy the changes from the git repository. The deploy will push the changes to the DNS slaves running in our different data centers.

This setup has been mostly unchanged since the initial beta launch in 2007. And it has scaled surprisingly well, handling thousands of servers, dozens of services and tens of millions of users.

DNS provides a static view of the world (“these are the servers that are supposed to be running”). As our server park gets larger, having a dynamic registry becomes increasingly more useful and important (“these are the servers that are actually, up and running, right now”).

PostgreSQL is a very good example of highly mature technology. Unless you overload Postgres, It “just works”, without having to mess around with, or even understand the source code. The PostgreSQL Handbook is one of the finest examples of documentation of an open source project in the world.

In the beginning of Spotify, when load was lower, PostgreSQL was definitely the right tool for the job.

Later came Postgres 9 and with it the excellent streaming replication and hot standby functionality. One of the most important database clusters at Spotify, the cluster that stores user credentials (for login), is a Postgres 9 cluster. We have one master server responsible for writes, and several hot standbys that take all the read requests.

A properly administered Cassandra cluster has better replication (especially writes).

A properly administered Cassandra cluster behaves better in the presence of networking issues and failures, such as partitions or intermittent glitches.

In general, Cassandra behaves better in certain classes of failures (server dies, network links goes down etc) from an operational perspective, than a PostgreSQL cluster.

Note that these are relatively new problem for Spotify and a lot of these issue did not exist a just a couple of years ago. Using Cassandra back then for example   would have been the wrong tool for the job.

# [Achieving fault-tolerance with intelligent daemons](http://labs.spotify.com/2013/03/06/achieving-fault-tolerance-with-intelligent-daemons/)

Spotify uses the common “circuit breaker” pattern. The outcome of each request to a server modifies caller’s notion of the server’s health status. If the health of a particular service, as understood by the client, falls below a certain predefined threshold the circuit breaker will trigger. In this case most requests with return with an error directly on the client side, without ever reaching the troubled server. Only a small fraction of the requests will trickle through to detect if the service is getting healthier.

When doing fast-failing and the health monitor has determined the service is reliable again, it is important to reintroduce new service requests slowly and in a controlled manner, instead of allowing all requests through at the same time, potentially overloading the service.

At Spotify we have put circuit breaking, health monitoring etc in a separate daemon called Arrow. Instead of service A sending a request to B (possibly to a different data center), it will send an IPC request to Arrow running on localhost, which in turn handles the communication with B.

* Arrow has several backends, such as an HTTP.
* It has a connection pool.
* Arrow has health monitoring to keep track of the health of the individual servers that make up B.
* Arrow has a circuit breaker for fast failing requests.
* Arrow collects lots of metrics about requests, such as request times, request rates, request failures, number of incoming connections, health of nodes etc.

* For services consisting of multiple worker processes, having one Arrow means a shared view of the world (such as the health of a service). Additionally, the connection pool can be shared, resulting in less resource utilization. With workers running individual clients, each worker would have their own view of the world.

* The process can be debugged and inspected (netstat, lsof, strace, gdb etc) with less noise than if inspecting the main service running the client.

* The process can be restarted, reloaded or killed with less effect or impact on the main service. This has been proven to be useful for deployments, upgrades and also shutting down traffic between services.

* Configuration is simplified as Operations has one configuration file to care about for dependencies and networking (you could of course have a configuration file for your client library, but that concept is a bit unusual).

* From a network-engineering standpoint, it is good practice to reduce the number of different programs that can cause network packets being sent between data centers.

* Aesthetically, having one program that does one thing and does it well instead of monolithic processes is very Unix, which we like at Spotify.

Spotify has a load-balancing configuration in front of certain services for rate limiting traffic and blocking any service that is generating too many requests. For example, most of our HTTP servers have an nginx in front of it, acting as a load-balancer and (sometimes) rate limiter.

This additional component has many of the same benefits as running Arrow as a separate process, and there is symmetry to it as well: the load balancer is for controlling incoming connections, Arrow is for controlling outgoing connections.

Intuitively, adding extra components or _cogs_ should be avoided as it can make your service more fragile and complex. However, while each individual server evolves in complexity, it reduces the complexity of a cluster of services. On the service level, having Arrow as a process instead of a library reduces the code complexity of the service program.

# [Backend infrastructure at Spotify](http://labs.spotify.com/2013/03/15/backend-infrastructure-at-spotify/)

One key concept at Spotify is that each development team should be autonomous.

All code in the Spotify client, Spotify backend and Spotify infrastructure is available to all the developers at Spotify to read or change. If a squad is blocking on some other squad to make a change in some code, they always have the option to go ahead and make the change themselves.

All infrastructure that is needed should be available as a self service entity. That way, there is no need to wait for another team to get hardware, setup a storage cluster or do configuration changes.

A slightly oversimplified description is that all the physical screen area of all the pages and views in our clients is owned by some squad. All of the features in the Spotify clients belong to a specific squad. The squad is responsible for that feature across all platforms – all the way from how it appears on an iOS device or a browser via the real time requests handled by the Spotify backend to the batch oriented data crunching that takes place in our Hadoop cluster to power features like recommendations, radio and search.

If one feature fails, the other features of our clients are independent and will continue to work. If there is a weak dependency between features, failure of one feature may sometimes lead to degradation of service of another feature, but not to the entire Spotify service failing.

Since all the users are not using all the features at the same time, the number of users that has to be handled by the backend of a particular feature is typically much smaller than the number of users of the entire Spotify service.

Since all the knowledge around one particular feature is concentrated to one squad it is very easy to A/B test features, look at the data collected and take an informed decision with all the relevant people involved.

Feature partitioning gives scalability, reliability and an efficient way of focusing team efforts.

The Spotify infrastructure strives to minimize the difference between running in our own data centers and on a public cloud. In short you get better latency, and a more stable environment in our own data centers. On a public cloud you get much faster provisioning of hardware and much more dynamic scaling possibilities.

For each feature the squad will have to create a storage solution that fits the needs of that particular service. The Spotify infrastructure offers a few different options for storage: Cassandra, PostgreSQL and memcached.

If the feature’s data needs to be partitioned, then the squad has to implement the sharding themselves in their services, however many services rely on Cassandra doing full replicas of data between sites. Setting up a full storage cluster with replication and failover between sites is complicated so we are building infrastructure to setup and maintain multi site Cassandra or postgreSQL clusters as one unit.

We have built our own low latency, low overhead messaging layer and are planning to extend it with high delivery guarantees, failover routing and more sophisticated load-balancing.

Each squad has to make sure that their features always scale to the current load. The squad can choose to keep track of this manually by monitoring traffic to their services and identify and fix bottlenecks and scale out as needed. We are also building an infrastructure that allows squads to scale their services automatically with load. Automatic scaling typically only works for bottlenecks that you are aware of, so there is always a certain level of human monitoring that the squad need to handle. Our infrastructure allows the easy creation of graphs and alerts to support this.

# [Agile à la Spotify](http://labs.spotify.com/2013/03/20/agile-a-la-spotify/)

#### Part of my work is to look for ways to continuously improve, both personally, and in the wider organisation.

* Taking part in regular squad and project retrospectives
* Being willing to experiment and trying new things
* Promoting a culture of no blame and no fear
* Seeing each change as an opportunity for improvement

#### We believe in short learning cycles, so that we can validate our assumptions as quickly as possible.

* Identify the smallest, simplest step that will help us learn
* Demo and release often
* Use A/B testing and other ways of gaining data-driven insight to verify our assumptions
* Hypothesize, measure, analyze, learn, adjust

#### Scaling what we do is key to success. Simplicity should be your guidance during scaling. This is as true for our technical solutions, as for our methods of working and organising the organisation.

* Simplicity allows for transparency, reuse and easy knowledge transfer
* Removing complexity takes time and needs to be done iteratively
* It’s important to have the hard discussions on how to do things simply
* We favor direct communication and avoid unnecessary layers in communication
* We don’t over-engineer and we don’t cut corners
* We strive to avoid single points of failure

#### We trust our people and teams to make informed decisions about the way they work and what they work on.

* Support and make decisions to make us a success
* Politely question each other to improve the group
* Figure out the best way to work with each other (which process to use)
* Find their own solutions to complex problems
* Identify and attack problems themselves, rather than seeing them as someone else’s responsibility

#### Managers are focused on coaching, mentorship, and solving impediments rather than telling people what to do.

* Decisions are transparent
* Managers encourage collaboration to solve problems rather than dictating a solution
* Managers help to address impediments that the squad/chapter cannot solve themselves
* You have regular one-on-one coaching and mentorship time with your manager
* People development happens alongside the group's success.

# [Analytics at Spotify](http://labs.spotify.com/2013/05/13/analytics-at-spotify/)

Spotify strives to be entirely data driven. We are a company full of ambitious, highly intelligent, and highly opinionated people and yet as often as possible decisions are made using data. Decisions that cannot be made by data alone are meticulously tracked and fed back into the system so future decisions can be based off of it.

So the conclusion is to rely on data whenever possible.  Don’t have enough data?  Get more.  Make data the most important asset you have because it is the only reliable decision maker that can scale your company.

# [Ruby 2.1: Out-of-Band GC](http://tmm1.net/ruby21-oobgc/)

Ruby 2.1's GC is better than ever, but ruby still uses a stop-the-world GC implementation. This means collections triggered during request processing will add latency to your response time. One way to mitigate this is by running GC in-between requests, i.e. "Out-of-Band".

OOBGC is a popular technique, first introduced by Unicorn and later integrated into Passenger. Traditionally, these out-of-band collectors force a GC every N requests. While this works well, it requires careful tuning and can add CPU pressure if unnecessary collections occur too often.

# [AWS Tips I Wish I'd Known Before I Started](http://wblinks.com/notes/aws-tips-i-wish-id-known-before-i-started/)

#### Store no application state on your servers

The reason for this is so that if you server gets killed, you won't lose any application state.

* sessions in the db (or session store)
* centralized log store
* upload directly to S3 (pre-signed URLs)
* worker queue & workers

#### Store extra information in your logs

* instance-id, region, zone, environment
* instance metadata (cache to a local file `/env/az`)

#### Servers are ephemeral, you don't care about them, you only care about the service as a whole

#### Don't give servers static/elastic IPs (load balance)

#### Get your alerts to become notifications

There's usually no action to take when getting a CloudWatch alert, as everything should be automated.

#### Set up granular billing alerts

So the first week's alert for say $1,000, the second for $2,000, third for $3,000, etc. If the week-2 alarm goes off before the 14th/15th of the month, then I know something is probably going wrong.

#### Use EC2 roles, do not give applications an IAM account

If your application has AWS credentials baked into it, you're "doing it wrong". One of the reasons it's important to use the AWS SDK for your language is that you can really easily use EC2 IAM roles. The idea of a role is that you specify the permissions a certain role should get, then assign that role to an EC2 instance. Whenever you use the AWS SDK on that instance, you don't specify any credentials. Instead, the SDK will retrieve temporary credentials which have the permissions of the role you set up.

#### Set up automated security auditing (CloudTrail)

#### Use random strings at the start of your keys.

This seems like a strange idea, but one of the implementation details of S3 is that Amazon use the object key to determine where a file is physically placed in S3. So files with the same prefix might end up on the same hard disk for example. By randomizing your key prefixes, you end up with a better distribution of your object files.

#### Use a VPC

First, you can control traffic at the network level using ACLs, you can modify instance size, security groups, etc. without needing to terminate an instance. You can specify egress firewall rules (you cannot control outbound traffic from normal EC2). But the biggest thing is that you have your own private subnet where your instances are completely cut off from everyone else, so it adds an extra layer of protection.

#### Use reserved instances for big savings

Reserved instances are a purely logical concept in AWS, you don't assign a specific instance to be reserved, but rather just specify the type and size, and any instances that match the criteria will get the lower price.

#### Lock down your security groups

For example, if your instances are behind an ELB, you should set your security groups to only allow traffic from the ELBs, rather than from 0.0.0.0/0. You can do that by entering "amazon-elb/amazon-elb-sg" as the CIDR (it should auto-complete for you). If you need to allow some of your other instances access to certain ports, don't use their IP, but specify their security group identifier instead (just start typing "sg-" and it should auto-complete for you).

#### Terminate SSL on the load balancer

#### Use CloudWatch detailed monitoring

~$3.50 / instance / month

#### Scale down on INSUFFICIENT_DATA as well as ALARM

#### Use ELB health check instead of EC2 health checks

# [The value of Rails worst practices](http://dmcca.be/2014/02/02/the-value-of-rails-worst-practices.html)