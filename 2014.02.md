# [Structuring Sinatra Applications](http://blog.sourcing.io/structuring-sinatra)

Our application has one main route: App. If we want to go about creating another route, we just need to mount it on App. Essentially, each route is its own separate application.

De-couple your application by splitting it up into lots of smaller applications mounted under one base. Believe me, this is the secret to keeping things simple and clean.

Database-wise I always choose Postgres. I've found, in the long run, document orientated databases aren't worth the hassle. Postgres is incredibly flexible, rock solid, and has never let me down.

For an ORM I like to use Sequel.  I also use the project sinatra-sequel, which adds a nice DSL around setting up a database.

I use CoffeeScript for JS and Stylus for CSS on the client side, and compile them using Sprockets. Even if you're not using a language that needs to be compiled, I recommend using an asset manager like Sprockets to give you concatenation and cache expiry.

# [Docker Nginx and Sentiment Engine on Steroids](http://shrikar.com/blog/2014/02/02/docker-nginx-and-sentiment-engine-on-steroids/)

Modify `/etc/security/limits.conf` to have a high number for open file descriptors.

`net.ipv4.ip_local_port_range` nginx needs to create two connection for every request, one to the client and the other one to the upstream server. Increasing the port range will prevent for port exhaustion.

`net.ipv4.tcp_fin_timeout` minimum number of seconds that must elapse before a connection in TIME_WAIT state can be recycled. Lowering this value will mean allocations will be recycled faster.

`net.ipv4.tcp_tw_recycle` enables fast recycling of TIME_WAIT sockets. Use with caution and ONLY in internal network where network connectivity speeds are “faster”.

`net.ipv4.tcp_tw_reuse` allows reusing sockets in TIME_WAIT state for new connections when it is safe from protocol viewpoint. Default value is 0 (disabled). It is generally a safer alternative to tcp_tw_recycle. Note: The tcp_tw_reuse setting is particularly useful in environments where numerous short connections are open and left in TIME_WAIT state, such as web servers. Reusing the sockets can be very effective in reducing server load.

`sudo sysctl -p` after making modifications to sysctl.conf.

[High-Performance Browser Networking](http://chimera.labs.oreilly.com/books/1230000000545/index.html)

[Optimising NginX, Node.JS and networking for heavy workloads](https://engineering.gosquared.com/optimising-nginx-node-js-and-networking-for-heavy-workloads)

# [Redis Master-Slave on Docker](http://www.hvflabs.com/posts/redis-master-slave-on-docker)

A base image which supports different versions of Redis (2.6 & 2.8). This base image will be used as the FROM for our “runtime” images. This will also be used for "cli containers" which I'll explain in a bit.

A standalone runtime image which will provide a data volume and expose the redis port and run the actual server.

A slave runtime image which will link to the standalone image and slave off it. This image will have a startup script which will inspect the environment created by the Docker linking mechanism.

# [Crash Course: Next Generation Servers With Containers](http://www.sebastien-han.fr/blog/2014/02/03/crash-course-next-generation-servers-with-containers/)

Namespace is a Kernel feature that brings isolation at different levels of the system. Each isolation level represents a specific resource.

**PID** allows us to isolate a process PID behind a parent process. It is important to note that each PID namespace has its own init-like process (PID 1).

**Network** provides an isolated network interface and a complete isolated traffic. A common technique that is used to route the traffic outside the namespace is to rely on a bridge. That is for example what Docker does, it setups a bridge and attach every namespace vnet to it.

**Filesystem** supports multiple mountpoints. As always, these mountpoints can only be seen inside the namespace.

**IPC**

cgroup does resource limiting and consumption accounting. It allows you to restrict numerous metrics of a running Linux such as:

**CPU** general cpu cycles control
**CPUSET** control which process can use which cpu
**BLKIO** block layer control, IOPS and bandwidth are managed from here
**MEM** pages allocation, cache

Depending on your Linux distribution, usually cgroup sits under the sys virtual filesystem at `/sys/fs/cgroup`.

However, Docker is not the only project that relies on namespaces and cgroup. CoreOS heavily uses systemd, so it is a good distribution to play with systemd capabilities in addition to the native Docker support.

# [Docker and Logstash: Smarter log management for your containers](http://denibertovic.com/post/docker-and-logstash-smarter-log-management-for-your-containers/)

[Dockerfile to set up logstash](https://github.com/denibertovic/logstash-dockerfile)

Now, the logstash service is kind of heavy on the resources so I didn't want to run a logstash container on every host machine but rather I wanted co collect logs from every container on every host machine and send it to a central logstash server. That's where [logstash-forwarder](https://github.com/elasticsearch/logstash-forwarder) comes in.

Logstash-forwarder can be configured to watch certain directories and files but I was more interested in the ability to listen on `stdin`.

`docker run -name logstash -p 9292:9292 -d -t logstash`

# [Docker quicktip #3 – ONBUILD](http://www.tech-d.net/2014/02/06/docker-quicktip-3-onbuild/)

ONBUILD is a new instruction for the Dockerfile. It is for use when creating a base image and you want to defer instructions to child images.

ONBUILD gets run just after the FROM and before any other instructions in a child image.

You can also have multiple ONBUILD instructions.

Use it to defer build instructions to images built from a base image. Use it to more easily build images from a common base but differ in some way, such as different git branches, or different projects entirely.

# [Docker Quicktip #2: exec it, please!](http://www.tech-d.net/2014/01/27/docker-quicktip-2-exec-it/)

As it stands right now if we try to stop this container docker will hang for a few seconds and then just kill it.

The signals to stop the process are being sent to the startup script and not postgres.

Docker allows you to proxy all signals (this is enabled by default) to the running process in the container. Need to send HUP to the running process in the container? Send it to the docker container process.

# [The exec builtin command](http://wiki.bash-hackers.org/commands/builtin/exec)

The exec builtin command is used to

* replace the shell with a given program (executing it, not as new process)
* set redirections for the program to execute or for the current shell

# [Workflows of Refactoring](http://martinfowler.com/articles/workflowsOfRefactoring/)

# [The Trello Tech Stack](http://blog.fogcreek.com/the-trello-tech-stack/)

Trello started as an HTML mockup that was put together in a week.

Well-written CoffeeScript smooths out and shortens JavaScript, while maintaining the same semantics, and does not introduce a substantial debugging indirection problem.

The Client

* Backbone.js (client-side MVC)
* HTML5 pushState
* Mustache (templating language)

The Server

* node.js
* HAProxy
* Redis
* MongoDB

# [We spent a week making Trello boards load extremely fast. Here’s how we did it.](http://blog.fogcreek.com/we-spent-a-week-making-trello-boards-load-extremely-fast-heres-how-we-did-it/)

Heavy styles like borders, shadows, and gradients can really slow down a browser.

Instead of spending all of the browser’s time generating one huge amount of DOM to insert, we could generate a small amount of DOM, insert it, generate another small amount, insert it, and so forth, so that the browser could free up the UI thread, paint something quickly, and prevent locking up. This really did the trick.

That was the whole week. If rendering on your web client is slow, look for excessive paints and layouts. I highly recommend using Chrome DevTool’s Timeline to help you find trouble areas. If you’re in a situation where you need to render a lot of things at once, look into async.queue or some other progressive rendering.

# [In praise of _boring_ technology](http://labs.spotify.com/2013/02/25/in-praise-of-boring-technology/)

More often than not, the right tool for the job is piece of software that has been around for some time, with proven success.

At Spotify we use DNS heavily. When most people think of DNS they think of A records, used to map a hostname to an IPv4 address. However, DNS can be so much more than a “phone book”. It is actually better to think of DNS as a distributed, replicated database tailored for read-heavy loads.

Service discovery is a fairly large subject but in essence it boils down to answering the question: what servers run this service.

At Spotify we have solved this problem with DNS for a long time. Specifically, we use so called SRV records. A DNS SRV record is a canonical name, typically of the form _name._protocol.site, mapped to a list of hostnames with weight, priority and port number.

```
$ dig +short _spotify-client._tcp.spotify.com SRV
10 12 4070 A1.spotify.com.
10 12 4070 A2.spotify.com.
10 12 4070 A3.spotify.com.
10 12 4070 A4.spotify.com.
```

The fields given above are, in order, the priority, the weight, the port number and the host. The user of a service should pick servers proportional to the weight, in the group of servers with the highest priority. Only when all servers are down should requests be sent to the servers with lower priority.

All backend services powering Spotify can be discovered by asking for the correct SRV record, and this is how the services find each other. For cases when DNS is not appropriate, such as externally visible web services, we have developed HTTP facades in front of DNS. For instance, some clients will send a HTTP request to figure out which Access Point to connect to.

In some (rare) cases, Spotify also use CNAME and PTR records in addition to SRV records. This is typically when a single server has some particular purpose. For example, we use a PTR record to locate the write master in one of our database clusters.

For administrative purposes, many tools and other infrastructural projects have been developed. One example is command line utilities that make SRV lookups easier. We also have a web service that works as a DNS cache that has some interesting features, such as reverse lookups (mapping a hostname to a service name).

As DNS is a high performance distributed database, we also use it for storing some service configuration data. In particular, we store DHT ring information in TXT records.

Our DNS servers run Bind and many of the backend services run Unbound which is also a DNS server. The use of Unbound is to improve performance and reduce load of the main DNS servers.

We have a git (previously subversion) repository where we maintain the DNS zone files. We have a small Makefile we run on the DNS master to deploy the changes from the git repository. The deploy will push the changes to the DNS slaves running in our different data centers.

This setup has been mostly unchanged since the initial beta launch in 2007. And it has scaled surprisingly well, handling thousands of servers, dozens of services and tens of millions of users.

DNS provides a static view of the world (“these are the servers that are supposed to be running”). As our server park gets larger, having a dynamic registry becomes increasingly more useful and important (“these are the servers that are actually, up and running, right now”).

PostgreSQL is a very good example of highly mature technology. Unless you overload Postgres, It “just works”, without having to mess around with, or even understand the source code. The PostgreSQL Handbook is one of the finest examples of documentation of an open source project in the world.

In the beginning of Spotify, when load was lower, PostgreSQL was definitely the right tool for the job.

Later came Postgres 9 and with it the excellent streaming replication and hot standby functionality. One of the most important database clusters at Spotify, the cluster that stores user credentials (for login), is a Postgres 9 cluster. We have one master server responsible for writes, and several hot standbys that take all the read requests.

A properly administered Cassandra cluster has better replication (especially writes).

A properly administered Cassandra cluster behaves better in the presence of networking issues and failures, such as partitions or intermittent glitches.

In general, Cassandra behaves better in certain classes of failures (server dies, network links goes down etc) from an operational perspective, than a PostgreSQL cluster.

Note that these are relatively new problem for Spotify and a lot of these issue did not exist a just a couple of years ago. Using Cassandra back then for example   would have been the wrong tool for the job.

# [Achieving fault-tolerance with intelligent daemons](http://labs.spotify.com/2013/03/06/achieving-fault-tolerance-with-intelligent-daemons/)

Spotify uses the common “circuit breaker” pattern. The outcome of each request to a server modifies caller’s notion of the server’s health status. If the health of a particular service, as understood by the client, falls below a certain predefined threshold the circuit breaker will trigger. In this case most requests with return with an error directly on the client side, without ever reaching the troubled server. Only a small fraction of the requests will trickle through to detect if the service is getting healthier.

When doing fast-failing and the health monitor has determined the service is reliable again, it is important to reintroduce new service requests slowly and in a controlled manner, instead of allowing all requests through at the same time, potentially overloading the service.

At Spotify we have put circuit breaking, health monitoring etc in a separate daemon called Arrow. Instead of service A sending a request to B (possibly to a different data center), it will send an IPC request to Arrow running on localhost, which in turn handles the communication with B.

* Arrow has several backends, such as an HTTP.
* It has a connection pool.
* Arrow has health monitoring to keep track of the health of the individual servers that make up B.
* Arrow has a circuit breaker for fast failing requests.
* Arrow collects lots of metrics about requests, such as request times, request rates, request failures, number of incoming connections, health of nodes etc.

* For services consisting of multiple worker processes, having one Arrow means a shared view of the world (such as the health of a service). Additionally, the connection pool can be shared, resulting in less resource utilization. With workers running individual clients, each worker would have their own view of the world.

* The process can be debugged and inspected (netstat, lsof, strace, gdb etc) with less noise than if inspecting the main service running the client.

* The process can be restarted, reloaded or killed with less effect or impact on the main service. This has been proven to be useful for deployments, upgrades and also shutting down traffic between services.

* Configuration is simplified as Operations has one configuration file to care about for dependencies and networking (you could of course have a configuration file for your client library, but that concept is a bit unusual).

* From a network-engineering standpoint, it is good practice to reduce the number of different programs that can cause network packets being sent between data centers.

* Aesthetically, having one program that does one thing and does it well instead of monolithic processes is very Unix, which we like at Spotify.

Spotify has a load-balancing configuration in front of certain services for rate limiting traffic and blocking any service that is generating too many requests. For example, most of our HTTP servers have an nginx in front of it, acting as a load-balancer and (sometimes) rate limiter.

This additional component has many of the same benefits as running Arrow as a separate process, and there is symmetry to it as well: the load balancer is for controlling incoming connections, Arrow is for controlling outgoing connections.

Intuitively, adding extra components or _cogs_ should be avoided as it can make your service more fragile and complex. However, while each individual server evolves in complexity, it reduces the complexity of a cluster of services. On the service level, having Arrow as a process instead of a library reduces the code complexity of the service program.

# [Backend infrastructure at Spotify](http://labs.spotify.com/2013/03/15/backend-infrastructure-at-spotify/)

One key concept at Spotify is that each development team should be autonomous.

All code in the Spotify client, Spotify backend and Spotify infrastructure is available to all the developers at Spotify to read or change. If a squad is blocking on some other squad to make a change in some code, they always have the option to go ahead and make the change themselves.

All infrastructure that is needed should be available as a self service entity. That way, there is no need to wait for another team to get hardware, setup a storage cluster or do configuration changes.

A slightly oversimplified description is that all the physical screen area of all the pages and views in our clients is owned by some squad. All of the features in the Spotify clients belong to a specific squad. The squad is responsible for that feature across all platforms – all the way from how it appears on an iOS device or a browser via the real time requests handled by the Spotify backend to the batch oriented data crunching that takes place in our Hadoop cluster to power features like recommendations, radio and search.

If one feature fails, the other features of our clients are independent and will continue to work. If there is a weak dependency between features, failure of one feature may sometimes lead to degradation of service of another feature, but not to the entire Spotify service failing.

Since all the users are not using all the features at the same time, the number of users that has to be handled by the backend of a particular feature is typically much smaller than the number of users of the entire Spotify service.

Since all the knowledge around one particular feature is concentrated to one squad it is very easy to A/B test features, look at the data collected and take an informed decision with all the relevant people involved.

Feature partitioning gives scalability, reliability and an efficient way of focusing team efforts.

The Spotify infrastructure strives to minimize the difference between running in our own data centers and on a public cloud. In short you get better latency, and a more stable environment in our own data centers. On a public cloud you get much faster provisioning of hardware and much more dynamic scaling possibilities.

For each feature the squad will have to create a storage solution that fits the needs of that particular service. The Spotify infrastructure offers a few different options for storage: Cassandra, PostgreSQL and memcached.

If the feature’s data needs to be partitioned, then the squad has to implement the sharding themselves in their services, however many services rely on Cassandra doing full replicas of data between sites. Setting up a full storage cluster with replication and failover between sites is complicated so we are building infrastructure to setup and maintain multi site Cassandra or postgreSQL clusters as one unit.

We have built our own low latency, low overhead messaging layer and are planning to extend it with high delivery guarantees, failover routing and more sophisticated load-balancing.

Each squad has to make sure that their features always scale to the current load. The squad can choose to keep track of this manually by monitoring traffic to their services and identify and fix bottlenecks and scale out as needed. We are also building an infrastructure that allows squads to scale their services automatically with load. Automatic scaling typically only works for bottlenecks that you are aware of, so there is always a certain level of human monitoring that the squad need to handle. Our infrastructure allows the easy creation of graphs and alerts to support this.

# [Agile à la Spotify](http://labs.spotify.com/2013/03/20/agile-a-la-spotify/)

#### Part of my work is to look for ways to continuously improve, both personally, and in the wider organisation.

* Taking part in regular squad and project retrospectives
* Being willing to experiment and trying new things
* Promoting a culture of no blame and no fear
* Seeing each change as an opportunity for improvement

#### We believe in short learning cycles, so that we can validate our assumptions as quickly as possible.

* Identify the smallest, simplest step that will help us learn
* Demo and release often
* Use A/B testing and other ways of gaining data-driven insight to verify our assumptions
* Hypothesize, measure, analyze, learn, adjust

#### Scaling what we do is key to success. Simplicity should be your guidance during scaling. This is as true for our technical solutions, as for our methods of working and organising the organisation.

* Simplicity allows for transparency, reuse and easy knowledge transfer
* Removing complexity takes time and needs to be done iteratively
* It’s important to have the hard discussions on how to do things simply
* We favor direct communication and avoid unnecessary layers in communication
* We don’t over-engineer and we don’t cut corners
* We strive to avoid single points of failure

#### We trust our people and teams to make informed decisions about the way they work and what they work on.

* Support and make decisions to make us a success
* Politely question each other to improve the group
* Figure out the best way to work with each other (which process to use)
* Find their own solutions to complex problems
* Identify and attack problems themselves, rather than seeing them as someone else’s responsibility

#### Managers are focused on coaching, mentorship, and solving impediments rather than telling people what to do.

* Decisions are transparent
* Managers encourage collaboration to solve problems rather than dictating a solution
* Managers help to address impediments that the squad/chapter cannot solve themselves
* You have regular one-on-one coaching and mentorship time with your manager
* People development happens alongside the group's success.

# [Analytics at Spotify](http://labs.spotify.com/2013/05/13/analytics-at-spotify/)

Spotify strives to be entirely data driven. We are a company full of ambitious, highly intelligent, and highly opinionated people and yet as often as possible decisions are made using data. Decisions that cannot be made by data alone are meticulously tracked and fed back into the system so future decisions can be based off of it.

So the conclusion is to rely on data whenever possible.  Don’t have enough data?  Get more.  Make data the most important asset you have because it is the only reliable decision maker that can scale your company.

# [Ruby 2.1: Out-of-Band GC](http://tmm1.net/ruby21-oobgc/)

Ruby 2.1's GC is better than ever, but ruby still uses a stop-the-world GC implementation. This means collections triggered during request processing will add latency to your response time. One way to mitigate this is by running GC in-between requests, i.e. "Out-of-Band".

OOBGC is a popular technique, first introduced by Unicorn and later integrated into Passenger. Traditionally, these out-of-band collectors force a GC every N requests. While this works well, it requires careful tuning and can add CPU pressure if unnecessary collections occur too often.

# [AWS Tips I Wish I'd Known Before I Started](http://wblinks.com/notes/aws-tips-i-wish-id-known-before-i-started/)

#### Store no application state on your servers

The reason for this is so that if you server gets killed, you won't lose any application state.

* sessions in the db (or session store)
* centralized log store
* upload directly to S3 (pre-signed URLs)
* worker queue & workers

#### Store extra information in your logs

* instance-id, region, zone, environment
* instance metadata (cache to a local file `/env/az`)

#### Servers are ephemeral, you don't care about them, you only care about the service as a whole

#### Don't give servers static/elastic IPs (load balance)

#### Get your alerts to become notifications

There's usually no action to take when getting a CloudWatch alert, as everything should be automated.

#### Set up granular billing alerts

So the first week's alert for say $1,000, the second for $2,000, third for $3,000, etc. If the week-2 alarm goes off before the 14th/15th of the month, then I know something is probably going wrong.

#### Use EC2 roles, do not give applications an IAM account

If your application has AWS credentials baked into it, you're "doing it wrong". One of the reasons it's important to use the AWS SDK for your language is that you can really easily use EC2 IAM roles. The idea of a role is that you specify the permissions a certain role should get, then assign that role to an EC2 instance. Whenever you use the AWS SDK on that instance, you don't specify any credentials. Instead, the SDK will retrieve temporary credentials which have the permissions of the role you set up.

#### Set up automated security auditing (CloudTrail)

#### Use random strings at the start of your keys.

This seems like a strange idea, but one of the implementation details of S3 is that Amazon use the object key to determine where a file is physically placed in S3. So files with the same prefix might end up on the same hard disk for example. By randomizing your key prefixes, you end up with a better distribution of your object files.

#### Use a VPC

First, you can control traffic at the network level using ACLs, you can modify instance size, security groups, etc. without needing to terminate an instance. You can specify egress firewall rules (you cannot control outbound traffic from normal EC2). But the biggest thing is that you have your own private subnet where your instances are completely cut off from everyone else, so it adds an extra layer of protection.

#### Use reserved instances for big savings

Reserved instances are a purely logical concept in AWS, you don't assign a specific instance to be reserved, but rather just specify the type and size, and any instances that match the criteria will get the lower price.

#### Lock down your security groups

For example, if your instances are behind an ELB, you should set your security groups to only allow traffic from the ELBs, rather than from 0.0.0.0/0. You can do that by entering "amazon-elb/amazon-elb-sg" as the CIDR (it should auto-complete for you). If you need to allow some of your other instances access to certain ports, don't use their IP, but specify their security group identifier instead (just start typing "sg-" and it should auto-complete for you).

#### Terminate SSL on the load balancer

#### Use CloudWatch detailed monitoring

~$3.50 / instance / month

#### Scale down on INSUFFICIENT_DATA as well as ALARM

#### Use ELB health check instead of EC2 health checks

# [The value of Rails worst practices](http://dmcca.be/2014/02/02/the-value-of-rails-worst-practices.html)

Any beginning Rails developer can synthesize Rubydocs, Railscasts, and Stack Overflow posts into a functioning application, but as that application grows in scope and complexity, worst practices will start to creep in. A more experienced developer, however, can draw on years of legacy code maintenance and messy refactoring projects to keep a codebase in check.

When interviewing potential Rails developers, I've found that the quickest way to gauge the experience of a potential hire is to show them some shockingly bad Rails code and ask them what they see. This process takes as long as whiteboarding FizzBuzz while being vastly more revealing. Plus, it can be easily done over the phone.

# [Distributed Semaphores with RabbitMQ](http://www.rabbitmq.com/blog/2014/02/19/distributed-semaphores-with-rabbitmq/)

We publish one message to the resource.semaphore queue. Then we start the processes that will seek access to that message. Each process will consume from the resource.semaphore queue; the first process to arrive will get the message and all the others will sit idle waiting for it. The trick will be that these processes will never acknowledge the message, but they will consume from the resource.semaphore queue with ack_mode=on. So, RabbitMQ will keep track of the message and if the processes crashes or exists, the message will go back to the queue, and it will be delivered to the next process listening from our semaphore queue.

With this simple technique we will have only one process at the time having access to the resource, and we are sure the process won't hold the resource if it crashes. Of course we assume that all the processes accessing the semaphore are well behaved, i.e.: they will never acknowledge the message. If they do, RabbitMQ will delete the message and all the other processes in the group will starve.

We make sure our process picks only one message from the queue by setting the prefetch-count equal to 1 in our basicQos call. Once a message arrives, the process will start using the resource. When the condition shouldStopProcessing() is met, the process will basicReject the message, telling RabbitMQ to requeue it. Keep in mind that the consumer was started in ack-mode and that it will never ack the message received from the semaphore queue. If it does, then it's considered buggy.

Since version 3.2.0 RabbitMQ supports Consumer Priorities. By using consumer priorities we can tell RabbitMQ which processes to favor when passing around the token message from the semaphore.

To have counting semaphore, publish 1 message for every concurrent process that you wish to run. All the above rules still apply.

# [The Four Stages of Disruption](http://recode.net/2014/01/06/the-four-stages-of-disruption-2/)

#### 1. Introduce product with new point of view

Introduces new product with a distinct point of view, knowing it does not solve all the needs of the entire existing market, but advances the state of the art in technology and/or business.

#### 2. Innovate rapidly along this new trajectory

Proceeds to rapidly add features and capabilities, filling out the value proposition after initial traction with select early adopters.

#### 3. Complete value proposition relative to legacy

Sees opportunity to acquire broader customer base by appealing to slow movers. Also sees limitations of own new product and learns from what was done in the past, reflected in a new way. Potential risk is being leapfrogged by even newer technologies and business models as focus turns to “installed base” of incumbent.

#### 4. Re-think the entire category

Approaches a decision point as new entrants to the market can benefit from all your product has demonstrated, without embracing the legacy customers as done previously. Embrace legacy market more, or keep pushing forward?

Apple famously talked about the 10-year project that was the iPhone, with many gaps, and while the iPad appears a quick successor, it, too, was part of that odyssey. Sometimes a new product appears to be a response to a new entry, but in reality it was under development for perhaps the same amount of time as another entry.

As a social science, business does not lend itself to provable operational rules. As appealing as disruption theory might be, the context and actions of many parties create unique circumstances each and every time. There is no guarantee that new technologies and products will disrupt incumbents, just as there is no certainty that existing companies must be disrupted. Instead, product leaders look to patterns, and model their choices in an effort to create a new path.

# [Complaint-Driven Development](http://www.codinghorror.com/blog/2014/02/complaint-driven-development.html)

1. Do a ton of detailed research on everything out there in your space.
2. Based on this research, build the minimum viable product that does something useful.
3. If you aren't living in the software you're building, each day, every day, all day, you've already lost.
4. Launch a brief closed beta and get feedback from your Special Internet Friends™.
5. Rapidly get to a public launch. It will suck, but you will ship it anyway.
6. Turns out once you put all your ideas in front of actual honest-to-god real world users they were all … completely … wrong.

> * Get your software in front of as many real users as you can.
* Listen to all the things they complain about. It will be… a lot.
* Identify and fix the top 3 things people keep repeatedly complaining about.
* Do it again.

Actually listening to your customers should matter to your business.

You'll always find more problems than you have the resources to fix, so it's very important that you focus on fixing the most serious ones first. And three users are very likely to encounter many of the most significant problems related to the tasks that you're testing.

It took us a full year of complaint driven development to get to software worth using.

The only thing I've ever seen work is getting down deep and dirty in the trenches with your users, communicating with them and cultivating relationships. That's how you suss out the rare 10% of community feedback that is amazing and transformative. That's how you build a community that gives a damn about what you're doing – by caring enough to truly listen to them and making changes they care about.

# [Playing with private Docker](http://starkandwayne.com/articles/2014/02/10/private-docker/)

[Docker private registry Dockerfile](https://github.com/drnic/docker-registry-dockerfile/blob/master/Dockerfile)

```
docker build -t registry .
docker run -p 5000:5000 -v $(pwd)/cache:/registry registry
docker tag ubuntu:13.04 localhost:5000/ubuntu-13.04
docker push localhost:5000/ubuntu-13.04

# another Docker instance
docker run localhost:5000/ubuntu-13.04 echo hello world
```

# [Experimenting with test driven development for docker](http://blog.wercker.com/2013/12/23/Test-driven-development-for-docker.html)

TDD PostgreSQL container

```
require 'docker'

describe "Postgres image" do
    before(:all) {
        @image = Docker::Image.all().detect{|i| i.info['Repository'] == 'pjvds/postgres'}
    }

    it "should be availble" do
        expect(@image).to_not be_nil
    end
    
    it "should expose the default tcp port" do
        expect(image.json["container_config"]["ExposedPorts"]).to include("5432/tcp")
    end
end

describe "running it as a container" do
    before(:all) do
        id = `docker run -d -p 5432:5432 #{@image.id}`.chomp
        @container = Docker::Container.get(id)
    end

    after(:all) do
        @container.kill
    end
    
    it "should accept connection to the default port" do
        expect{ PG.connect('host=127.0.0.1') }.to_not raise_error(PG::ConnectionBad, /Connection refused/)
    end
    
    it "should accept connections from superuser" do
        expect{ PG.connect(:host => '127.0.0.1', :user => 'root', :password => 'h^oAYozk&rC&', :dbname => 'postgres')}.to_not raise_error()
    end
end
```

# [Drone and Docker, Open Source CI](http://blog.drone.io/2014/2/5/open-source-ci-docker.html)

Drone provides a set of pre-built Docker images for 12+ languages and nearly every major database. This means you don’t have to spend time installing software and configuring your build environment. Of course, if you need a highly customized environment Drone gives you the flexibility to use a custom Docker image.

* Drone is open source
* Drone is built on Docker
* Drone is built in Go
* Drone is easily hosted on your own infrastructure
* Drone provides a CLI to run builds locally inside Docker containers
* Drone integrates with GitHub out of the box

# [How we cut down redis memory usage by 82%](http://labs.octivi.com/how-we-cut-down-memory-usage-by-82)

In one of our Redis instance we store mid-large json objects. Thanks to PHP gzcompress function which internally uses ZLIB library we were able to reduce memory usage by 82% – from about 340 GB to 60 GB.

Gzcompress proved to be about 6 times faster than bzcompress when compressing and decompressing data.

Compression ratio was similar for both algorithms.

The CPU overhead in comparison to memory saving is small. Also compression operations are running on application server, so they aren’t affecting in any way Redis performance.

Values stored in hashes don’t come with additional metadata. Internally Redis uses simple dictionary structure for storing them.

# [Storing hundreds of millions of simple key-value pairs in Redis](http://instagram-engineering.tumblr.com/post/12202313862/storing-hundreds-of-millions-of-simple-key-value-pairs)

All of our Redis deployments run in master-slave, with the slave set to save to disk about every minute.

```
SET media:1155315 939
GET media:1155315
> 939
```

While prototyping this solution, however, we found that Redis needed about 70 MB to store 1,000,000 keys this way. Extrapolating to the 300,000,000 we would eventually need, it was looking to be around 21GB worth of data—already bigger than the 17GB instance type on Amazon EC2.

Hashes in Redis are dictionaries that are can be encoded in memory very efficiently; the Redis setting ‘hash-zipmap-max-entries’ configures the maximum number of entries a hash can have while still being encoded efficiently. We found this setting was best around 1000; any higher and the HSET commands would cause noticeable CPU activity.

To take advantage of the hash type, we bucket all our Media IDs into buckets of 1000 (we just take the ID, divide by 1000 and discard the remainder). That determines which key we fall into; next, within the hash that lives at that key, the Media ID is the lookup key *within* the hash, and the user ID is the value.

```
HSET "mediabucket:1155" "1155315" "939"
HGET "mediabucket:1155" "1155315"
> "939"
```

The size difference was pretty striking; with our 1,000,000 key prototype (encoded into 1,000 hashes of 1,000 sub-keys each), Redis only needs 16MB to store the information. Expanding to 300 million keys, the total is just under 5GB.

# [Introducing Chassis](http://hawkins.io/2014/02/introducing_chassis/)

Chassis contains everything you need to build a loosely coupled application. It's built using specifically chosen gems. All gems have been evaluated on their code and extendability. There are no gems with C-extensions. All the code is implementation agnostic. Each gem has minimal or no dependencies, if a gem has dependencies, they are undergo the same scrunity.

Chassis builds an ideal stack for building web applications. You shouldn't really need any other gems to construct an application.

* Sinatra w/extra middleware & helpers for building web applications
* Manifold - For CORS. enable :cors in Chassis::WebService
* Prox - A completely transparent object proxy. Perfect for decorating objects
* Harness - Portable and exendable application performance library
* harness-rack - All requests to Chassis::WebService are tracked for performance
* Virtus - for building form objects
* Faraday - w/extra middleware for outgoing HTTP
* logger-better - make the stdlib logger more useful and provide a null implementation.

# [Make Cucumber features more readable with this one weird trick](http://chrismdp.com/2014/02/make-cucumber-features-more-readable-with-this-one-weird-trick/)

The writer will explain the feature clearly and concisely using completely different words to what is actually written in the feature. They use great contextual information that’s missing from the written preamble, and work to ensure I really understand what’s going on.

# [Process is Poison](http://blog.gosquadron.com/process-is-poison)

After every outage, you should hold a postmortem meeting to discuss what went wrong and how to fix it.

First, identify the root cause of the problem via a method such as the Five Whys. An easy mistake to make is to place blame on people in the Five Why's, don't do this, you're a team, you're all accountable for each other.

After you've identified the problem, it's time for solutions. Sometimes root causes are aberrations and aren't worth addressing, but almost all the time, the root cause is a human.

# [Pairing vs. Code Review: Comparing Developer Cultures](http://phinze.github.io/2013/12/08/pairing-vs-code-review.html)

Two people pairing can balance the natural daily ebbs and flows of energy for each other.

The constant presence of a peer generates motivation for self-improvement; everybody wants to perform well when being watched by someone they respect. Tactical decisions are made more easily and with better results: two people are much less likely to take shortcuts lightly, and a conversation naturally happens about any trade-off that comes up.

The concept of collective code ownership is easy to come by, since it’s rare that a single person is responsible for a given line of code. This makes for a healthier view of failures as those of the team and not of individuals.

Knowledge imbalance and talent imbalance can compound themselves and lead to a team that’s underwater and unable to rotate effectively, further exacerbating the issues.

Pair programming is a rather intensive practice that does not lend itself to every personality type. This means that by choosing this practice, a team is limiting the hiring pool to other people willing to spend their days in constant collaboration. For teams that pair, this is a conscious trade off made to gain the benefits of pairing.

Pairing is great for consistent forward progress and the sharing of certain kinds of knowledge, but it can be detrimental to the process of making decisions that require more deep thought or creativity. These sorts of decisions often come up when tackling the larger problems of system design and architecture.

With code reviews, nobody sees your code until you’re ready for them to see it. Since every developer knows there will inevitably be peer eyeballs on their code, the positive motivating pressure to perform well is still there.

Without the pairing urgency, developers have access to a larger toolbox of problem solving techniques that can benefit the overall process of code construction.

On a team that does code reviews, you are defined by your code, since the primary point of professional contact with your peers is now over the code you’ve written. This lends itself to a wider pool of personality types that can be effective in an environment like this. You can easily imagine a person you’d rather not hang out with, but whose code is rock solid and you’re glad to have on your team.

Code reviews are asynchronous, which provides a host of benefits. First of all it means it’s much easier for a team to adopt a flexible schedule for its members. If a certain person is more productive from 5am to noon, that’s perfectly fine. If there’s another developer that is going to night school and prefers to work late, no problem. You can also strategically distribute code reviews, ensuring that more experienced developers always touch certain kinds of code reviews, which provides an extra measure of quality control and protection against bugs landing in production.

I’ve also found that performing code reviews forces you to think about your values more-so than pairing does.

While there’s still motivation to perform well for your peers, on an hour-by-hour basis you are the only one who knows what you’re doing.

This is almost precisely the opposite of pairing’s bias towards forward progress; solo development provides absolutely no requirement to make progress in a given minute, hour, or day. It’s all up to you. And that can be terrifying.

The activation energy required for a comment followed by a revision and re-push and re-review is quite high, so this makes reviewers likely to be more lenient on code quality issues.

# [Nginx High Availability Via Heartbeat](http://people.adams.edu/~cdmiller/posts/nginx-heartbeat-ha/)

Tell Heartbeat what to do when a host goes away. On each server use the same configuration. We define HOST1 as the primary node, IP 192.156.134.105 as our takeover address, and nginx as a service to fail over.

On each host place the same authkeys file, used to identify the heartbeat.

From a fresh system restart heartbeat should start the IP alias address and nginx on the primary node. Test failover either by shutting down HOST1 or move the IP and nginx service back and forth using heartbeat commands.

# [HAProxy advanced Redis health check](http://blog.exceliance.fr/2014/01/02/haproxy-advanced-redis-health-check/)

There is even a built-in health check for redis in HAProxy.
Unfortunately, there was no easy way for HAProxy to detect the status of a redis server: master or slave node.

* 4 web application servers need to store and retrieve data to/from a Redis database
* one (better using 2) HAProxy servers which load-balance redis connections
* 2 (at least) redis servers in an active/standby mode with replication

The HAProxy health check sequence above allows to consider the Redis master server as UP in the farm and redirect connections to it.

When the Redis master server fails, the remaining nodes elect a new one. HAProxy will detect it thanks to its health check sequence.

# [Validating the MVP](http://www.sergioschuler.com/startup-lessons-learned-from-my-failed-startup/)

**Try it free** and free trials in general are good if you know your product rocks and you will convert the customer. However, when you are testing demand, it is not the best, because it invites people that are only curious, not really thinking of buying.

Put a price page, see who would click that and after capturing the email telling that the product is not ready and they won’t be charged anything just yet. I would probably get fewer prospects, but the people who did subscribe would be serious about the product.

> I had all those people who said they were interested and the obvious next step was NOT building the product, but TALKING to the people who were interested.

After discovering their needs, going through with them on what is your idea, to get feedback if you are on the right track.

> Since we were 3 business people, we spent all this time into idiot plans, budget forecasts, BUSINESS CARDS, fancy website… all useless things which in the end did not contribute to anything.

If a company stops invoicing, they will immediately feel the pain. If a company stops managing well their teams, it can take as much as 1 year for people to realize there is something wrong – and they won’t know what is wrong. Maybe some people will leave, but it is easy to put the blame on the employee or the market.

> It is possible to sell soda in the desert (nice to have), but it is much easier to sell water (must have).

Multiply $30 times 1.000 clients times 24 months. WOW, we will be rich!

Oh, silly you, you have no idea how hard it is to get 1.000 clients paying anything monthly for 24 months. Here is my advice: get your first client. Then get your first 10. Then get more and more.

> Until you have your first 10 clients, you have proved nothing, only that you can multiply numbers.

Forget business cards and all the crap about registering company, etc etc etc. While you have no sales (product validation), you should not invest in such gimmicks that only distract from the main goal, which is to find a repeatable business model.

> Just speak to prospects and extract their pain, then sell the painkiller (before building the product). If they are willing to buy, do take their money and invest that money into building the product.

# [Super successful companies](http://blog.samaltman.com/super-successful-companies)

They are obsessed with the quality of the product/experience. 

They are obsessed with talent.

They hire slowly.

They can explain the vision for the company in a few clear words.

They generate revenue very early on in their lives.

They are tough and calm.

They keep expenses low.

They make something a small number of users really love.

They grow organically.

They are focused on growth.

They balance a focus on growth with strategic thinking about the future.

They do things that don't scale.

They have a whatever-it-takes attitude.

They prioritize well.

They don't get excited about pretending to run a startup.

They get stuff done.

They move fast.

# [How Google sets goals: OKRs](http://www.gv.com/lib/how-google-sets-goals-objectives-and-key-results-okrs)

> OKRs are about the company’s goals and how each employee contributes to those goals.

Objectives are ambitious, and should feel somewhat uncomfortable

Key Results are measurable; they should be easy to grade with a number (at Google we use a 0 – 1.0 scale to grade each key result at the end of a quarter)

OKRs are public; everyone in the company should be able to see what everyone else is working on (and how they did in the past)

The “sweet spot” for an OKR grade is .6 – .7; if someone consistently gets 1.0, their OKRs aren’t ambitious enough. Low grades shouldn’t be punished; see them as data to help refine the next quarter’s OKRs.

# [Shell programming with bash: by example, by counter-example](http://matt.might.net/articles/bash-by-example/)

In shell programming, true is a program that always "succeeds," and false is a program that always "fails".

The process id of the current shell is available as `$$`.

`$?` is the exit status of the last command.

The process id of the most recently backgrounded process is available as `$!`.

In a feature unique among many languages, bash can operate on the value of a variable while dereferencing that variable.

```
foo="I'm a cat."
echo ${foo/cat/dog}  # prints "I'm a dog."
 
foo="I'm a cat, and she's cat."
echo ${foo/cat/dog}   # prints "I'm a dog, and she's a cat."
echo ${foo//cat/dog}  # prints "I'm a dog, and she's a dog."

foo="hello" 
echo ${foo/hello/goodbye}  # prints "goodbye"
echo $foo                  # still prints "hello" 

foo="I like meatballs."
echo ${foo/balls}       # prints I like meat.

minipath="/usr/bin:/bin:/sbin"
echo ${minipath#/usr}           # prints /bin:/bin:/sbin
echo ${minipath#*/bin}          # prints :/bin:/sbin
echo ${minipath##*/bin}         # prints :/sbin

minipath="/usr/bin:/bin:/sbin"
echo ${minipath%/usr*}           # prints nothing
echo ${minipath%/bin*}           # prints /usr/bin:
echo ${minipath%%/bin*}          # prints /usr
```

The prefix operator `#` counts the number of characters in a string or the number of elements in an array.

```
ARRAY=(one two three)
echo ${#ARRAY}          # prints 3 -- the length of the array?

ARRAY=(a b c)
echo ${#ARRAY}          # prints 1,
${#ARRAY} == ${#ARRAY[0]}

ARRAY=(a b c)
echo ${#ARRAY[@]}      # prints 3
 
string="I'm a fan of dogs."
echo ${string:6:3}           # prints fan

array=(a b c d e f g h i j)
echo ${array[@]:3:2}         # prints d e 
```

For operations that test whether a variable is set, they can be forced to check whether the variable is set and not empty by adding a ":".

```
unset foo
unset bar

echo ${foo-abc}   # prints abc
echo ${bar:-xyz}  # prints xyz

foo=""
bar=""

echo ${foo-123}   # prints nothing
echo ${bar:-456}  # prints 456
 
unset cache
echo ${cache:=1024}   # prints 1024
echo $cache           # prints 1024

echo ${cache:=2048}   # prints 1024
echo $cache           # prints 1024

unset foo
unset bar

foo=30

echo ${foo+42}    # prints 42
echo ${bar+1701}  # prints nothing
```

Indirect look-up

```
foo=bar
bar=42
 
echo ${!foo}  # prints $bar, which is 42

alpha=(a b c d e f g h i j k l m n o p q r s t u v w x y z)
char=alpha[12]

echo ${!char} # prints ${alpha[12]}, which is m
```

`$*` combines all arguments into a single string, while `$@` requotes the individual arguments.

There is another subtle difference between the two: if the variable `IFS` (internal field separator, must contain a single character) is set, then the contents of this variable are spliced between elements in `$*`.

In bash, variable scope is at the level of processes: each process has its own copy of all variables. Variables must be marked for export to child processes.

```
(( x = 3 + 12 )); echo $x    # prints 15
(( x = 3 * 12 )); echo $x    # prints 36

echo $(( 3 + 12 ))   # prints 15
echo $(( 3 * 12 ))   # prints 36

declare -i number
number=2+4*10
echo $number        # prints 42

another=2+4*10
echo $another       # prints 2+4*10

number="foobar"
echo $number        # prints 0
```

STDOUT, STDERR & STDIN

```
grep foo < myfile # prints lines in myfile containing "foo"
cat file1 file2 > combined
cat file1 file2 1> combined # same as the previous one

cat <<UNTILHERE
All of this will be printed out.

Since all of this is going into cat on STDIN.

UNTILHERE

grep foo nofile 2>&1 # errors will appear on STDOUT

echo $(date) $(whoami) >> log

echo user: `<config/USER` # prints the contents of config/USER

cat /etc/passwd | grep root
```

Redirecting with exec

```
exec < file # STDIN has become file
exec > file # STDOUT has become file
```

Some program accept a filename from which to read instead of reading from STDIN.

<pre>
cat <(uptime) <(date) <(tail -1 event.log) >> main.log
</pre>

Processes

```
time-consuming-command &
pid=$!
wait $pid
echo Process $pid finished.

for f in *.jpg
do 
  convert $f ${f%.jpg}.png &
done 
wait
echo All images have been converted.
```

Glob patterns have several special forms:

* `*` matches any string
* `?` matches a single character
* `[chars]` matches any character in chars
* `[a-b]` matches any character between a and b

Once declared, a function acts almost like a separate script: arguments to the function come as $n for the nth argument. One major different is that functions can see and modify variables defined in the outer script:

# [How To: Hosting on Amazon S3 with CloudFront](http://paulstamatiou.com/hosting-on-amazon-s3-with-cloudfront/)

# [TDD and ActiveRecord in Rails](http://solnic.eu/2014/01/06/tdd-and-activerecord-in-rails.html)

If you have a user model, don’t write tests checking when it’s valid or not. This doesn’t help much and the value of such tests is surprisingly small.

Forget about `#save`, `.create`, `.where` etc. Those are details, don’t expose them on the higher layers in your system. What you can do instead is to introduce your own interface to interract with the underlaying ActiveRecord objects.

Every interaction with the database is a use case. Capture that use case and write a test for it.

Your code is coupled to X methods coming from ActiveRecord and your tests are probably checking 50% of what you are actually doing. That’s really bad!

The less you’re coupled to Rails interfaces, the better. Adding simple methods that communicate your intentions is way better than relying on “raw” interfaces exposed by Rails.

# [Beginning a journey to chatops with Hubot](http://tech.toptable.co.uk/blog/2013/11/22/beginning-a-journey-to-chatops-with-hubot/)

By building tools and executing commands in a chat room that can be automated by a bot, communication doesn’t become an afterthought to operational processes but is core to how you operate. If I want to deploy code, I type a command into chat. If I want to take a server offline, I type a command into chat. If I want to merge a git pull request into master, I type in a command into chat, and so on. Communication is baked in.

# [etcd @ GoSF 2013](https://speakerdeck.com/philips/etcd-at-gosf)

Service discovery & locking

# [Server monitoring tools](https://news.ycombinator.com/item?id=7124720)

# [Static Web Rising](http://www.intridea.com/blog/2014/1/27/static-web-rising)

Rapid Development. Using modern frameworks such as Bootstrap and AngularJS, and tools such as Divshot, you can quickly "sketch out" an interface and turn it into a rough static prototype. That prototype can then become your application by hooking it to back-end data services.

Static web apps are just files stored on a web server and sent to the end user as-is. They can be scaled to millions of users with off-the-shelf technology. You'll still need efficient, scalable back-end services, but such services are increasingly available from third party vendors. Static apps let you focus on your application, not your infrastructure.

All web applications at scale separate into some form of modular architecture. This allows developers to build pieces of the overall application using specialized technologies that are well-suited to specific problems. Static web applications encourage this modularity from the beginning by forcing all back-end data to be provided by services separated from the front-end interface.

It's easy to build alternative interfaces when front and back ends are separated. Building a mobile application, exposing a public data API, making a command-line client, creating an administrative control panel, and more become much simpler when the back-end and the front-end are built independently.

# [Good Tech Lead, Bad Tech Lead](http://engineering.foursquare.com/2014/01/30/good-tech-lead-bad-tech-lead/)

Good tech leads act as a member of the team, and consider themselves successful when the team is successful.

Bad tech leads take the high-profile tasks for themselves and are motivated by being able to take credit for doing the work.

Good tech leads have an overall vision for the technical direction of the product and make sure the team understands it.

Bad tech leads resist explaining or clarifying the technical direction and dictate decisions instead.

Good tech leads listen and encourage debate.

Bad tech leads allow debates to go on for too long without resolution, hampering the productivity of the team.

Good tech leads are proactive.

Bad tech leads are reactive.

Good tech leads are pragmatic and find a balance between doing it right and getting it done.

Bad tech leads take shortcuts that save time in the short term but cost more in the long term, and let technical debt pile up.

Good tech leads know that their role is much more than writing code, that effective communication is a vital part of their job.

Bad tech leads believe that they are most productive when they are writing code, and think communication is a distraction.

Good tech leads are in a conversation with product managers and designers about how the product should work.

Bad tech leads throw product decisions “over the wall” and do not take ownership of the product.

Good tech leads are resilient to changes to the product specification and react calmly to surprises.

Bad tech leads are upset when the specification changes, or prematurely generalize their design in areas where changes are unlikely to occur.

Good tech leads are easy-going but assertive.

Bad tech leads get defensive when given feedback.

# [Using Enumerations to Make a Faster Activity Feed in Rails](http://blog.givegab.com/post/75043413459/using-enumerations-to-make-a-faster-activity-feed-in)

Using the `find_each` method on each model and allowing Rails to batch the database loads for us behind the scene. We can even go one step further and convert the result to an enumerator using `enum_for`.

# [Design Decisions For Scaling Your High Traffic Feeds](http://highscalability.com/blog/2013/10/28/design-decisions-for-scaling-your-high-traffic-feeds.html)

Feeds are a core component of many large startups such as Pinterest, Instagram, Wanelo and Fashiolista. At Fashiolista the feed system powers the flat feed, aggregated feed and the notification system.

The Feed system at Fashiolista went through three major redesigns. The first version worked on a PostgreSQL database, the second used Redis and the third and current version runs on Cassandra.

The most surprising thing was how robust this system was. We passed 1M loves and it kept on working, soon after we hit 5M loves and it still kept on working. Our bet was that it would break after 10M loves, but it just kept on running smoothly. It took some database tweaking but this simple system held up well into ~100M loves and 1M users. Around that time the performance of this solution started to fluctuate. In general it kept on working, but for some users the latency spiked to multiple seconds.

Redis was a good solution, but several reasons made us look for an alternative. Firstly we wanted to support multiple content types, which made falling back to the database harder and increased our storage requirements. In addition the database fallback we were still relying on became slower as our community grew. Both these problems could be addressed by storing more data in Redis, but doing so was prohibitively expensive.

Fashiolista currently does a full push flow for the flat feed and a combination between push and pull for the aggregated feed. We store a maximum of 3600 activities in your feed, which currently takes up 2.12TB of storage. The fluctuations caused by high profile users are mitigated using priority queues, overcapacity and auto scaling.

For notification feeds and feeds built on Cassandra we recommend denormalizing your data. For feeds built on Redis you want to minimize your memory usage and keep your data normalized.

Reduce the load caused by these high profile users by selectively disabling fanouts. Twitter has apparently seen great performance improvements by disabling fanout for high profile users and instead loading their tweets during reads (pull).

Another possibility of selective fanouts is to only fan-out to your active users. (Say users who logged in during the last week). At Fashiolista we used a modified version of this idea, by storing the last 3600 activities for active users, but only 180 activities for inactive ones. After those 180 items we would fallback to the database. This setup slows down the experience for inactive users returning to your site, but can really reduce your memory usage and costs.

Redis however has a few limitations. All of your data needs to be stored in RAM which eventually becomes expensive. In addition there is no support for sharding built into Redis. This means that you have to roll your own system for sharding across nodes. (Twemproxy is a great option for this). Sharding across nodes is quite easy, but moving data when you add or remove nodes is a pain. You can work around these limitations by using Redis as a cache and falling back to your database. As soon as it becomes hard to fallback to the database I would consider moving from Redis to Cassandra.

# [PostgreSQL awesomeness for Rails developers](http://www.amberbit.com/blog/2014/2/4/postgresql-awesomeness-for-rails-developers/)

```
listen_addresses = '*' # To which interface we should bind. '*'
                       # makes your PostgreSQL visible to the Internet

max_connections = 200  # How many connections we should allow from
                       # our app, workers, delayed_jobs etc. combined
shared_buffers = 16GB  # How much memory our PostgreSQL can use for
                       # buffers. Default value is insanely small.
                       # If PostgreSQL is the only thing we run on
                       # the machine, set it to 1/4 of available RAM
work_mem = 10MB        # Increase the small value so the
                       # sorts perform better.
maintenance_work_mem = 128MB 

synchronous_commit = off # Speed up writes in exchange for possible
                         # loss of data (be careful here!)

wal_buffers = 16MB # Basically how much data we can loose. But
                   # increasing makes things faster. Choose wisely.
                   # Also applies to settings below.
wal_writer_delay = 400ms
checkpoint_segments = 32
checkpoint_timeout = 900s 
checkpoint_completion_target = 0.9

random_page_cost = 2.0 # Make planner use indices a bit more often
                       # vs. sequential table scans.

effective_cache_size = 32GB  # How much memory in total our
                             # PostgreSQL can use. Twice of
                             # shared_buffers seems good.
```

Streaming replication is my choice, which means that updates to write-ahead-log (WAL) are streamed from master to slaves. Setting it up this way, gives you unexpected benefit: you are getting possibility to do point-in-time recovery, which is great because you can restore your system to exact state just before major disaster happened.

[Zero to PostgreSQL streaming replication in 10 mins](http://www.rassoc.com/gregr/weblog/2013/02/16/zero-to-postgresql-streaming-replication-in-10-mins/)

[Database sharding for ActiveRecord](https://github.com/tchandy/octopus/wiki/replication)

Another scaling option is partitioning. It is good choice if you want to create applications that you will need to perform joins/queries on whole tables, but you can speed things up by physically distributing records between partitions (which are normal PG tables).

# [A List of Deprecated Stuff in Ruby 2.1](http://batsov.com/articles/2014/02/05/a-list-of-deprecated-stuff-in-ruby/)

To see the deprecation messages from `rb_warn` & `rb_warning`  functions you’ll have to run Ruby with `-w`,

`Dir.exists?` is a deprecated name, use `Dir.exist?` instead
`Enumerator.new` without a block is deprecated; use `Object#to_enum`
`StringIO#bytes` is deprecated; use `StringIO#each_byte` instead
`StringIO#chars` is deprecated; use `StringIO#each_char` instead
`StringIO#codepoints` is deprecated; use `StringIO#each_codepoint` instead
`StringIO#lines` is deprecated; use `StringIO#each_line` instead
`File.exists?` is a deprecated name, use `File.exist?` instead
`Hash#index` is deprecated; use `Hash#key`
`ENV.index` is deprecated; use `ENV.key`
`IO#lines` is deprecated; use `IO#each_line` instead
`IO#bytes` is deprecated; use `IO#each_byte` instead
`IO#chars` is deprecated; use `IO#each_char` instead
`IO#codepoints` is deprecated; use `IO#each_codepoint` instead
`ARGF#lines` is deprecated; use `ARGF#each_line` instead
`ARGF#bytes` is deprecated; use `ARGF#each_byte` instead
`ARGF#chars` is deprecated; use `ARGF#each_char` instead
`ARGF#codepoints` is deprecated; use `ARGF#each_codepoint` instead
`Object#untrusted?` is deprecated and its behavior is same as `Object#tainted?`
`Object#untrust` is deprecated and its behavior is same as `Object#taint`
`Object#trust` is deprecated and its behavior is same as `Object#untaint`
passing a block to `String#lines` is deprecated
passing a block to `String#bytes` is deprecated
passing a block to `String#chars` is deprecated
passing a block to `String#codepoints` is deprecated
