# [5 things I've learned in 5 years of running a SaaS](http://mir.aculo.us/2013/11/27/5-things-ive-learned-in-5-years-of-running-a-saas/)

#### 1. You're not a tech company - you're a make customers awesome company

People don't pay you because you have amazing programming skills and can write nginx configurations blindfolded. People pay you money because the product you sell to them saves them time, money, effort and nerves. It's your job to make your customer more awesome. Every decision you make for your product and business should revolve around that.

#### 2. Never promise dates for a feature launch

#### 3. Spend money on things that help you stay productive

#### 4. Do not work too much

Overworking yourself is the first step to failure in business. You can't do your best if you're permanently stressed out. Don't check email in the evenings.

#### 5. Don't believe the hype

People are good at getting excited. And people are good at believing the hype about new technologies, frameworks, programming languages and ways to deploy.

The fact is that you need to be pragmatic - **your goal is to run a business**. Use technology that is proven - **to you** -, and that you know how to work with.

You need to optimize for shipping. That includes writing less code, having broad test coverage, and concentrate on getting things out in order of long-term profitability for your business.

## [HN Comments](https://news.ycombinator.com/item?id=6818679)

My PHP application without any fancy technology makes me close to 100,000 â‚¬ in revenue per year. It delivers, it keeps delivering, there are no problems. It's consumer-oriented, it costs 2 euro a month, it has several thousand users.

I concentrated on important things, I do customer support myself, I listen to people, I'm friendly, I help people solve their problems. I care about them, about their privacy and do everything to protect it. I write emails that take me 30 minutes if it's necessary. Why? Because I only get a handful of emails per day. It's not scalable but then, it doesn't have to be. I do some SEO and rank well for my niche keywords. About 50-70% is word of mouth.

I don't have a blog, I didn't integrate any social network crap, I don't use Google Analytics or any other fancy third-party application that must be included via JS. I use Piwik that I check occasionally (sometimes every week, sometimes only every few months). I don't have big metrics. My most imporant metrics are: Money earned per week/month. Amount of active (paid) users. Signups per week.

My product has nothing to do with PHP and doesn't target IT people. Just in case this is misinterpreted. I mention PHP only, because people on HN hate PHP that much and sometimes forget that it doesn't matter if it's PHP or anything else, as long as it pays the bills. PHP is fast, PHP is reliable and I get shit done in PHP. That's why I use PHP.

After a long thought process and endless discussions with an older friend, I came to the conclusion that literally change my life: **I had to start charging people for my product. All of them.** No freemium nonsense and all that other crap that was in my list.

Being the altruistic student, I anticipated that people would be disappointed. I could almost feel their soon-to-come disappointment and asking them for money was something that didn't fit my world view for a long time. It was hard to accept that I'd lose people. Sounds stupid in hindsight, but that's how I felt.

> **If people don't value what you have to offer, they're not worth your time.**

So, even if people were disappointed, it wouldn't be my fault. After all, I offer a truly great service and it should be worth a couple bucks a month.

I got this braindead idea to rewrite everything from scratch in a newer technology to have it maintainable for years to come. I thought I'd have to offer more content, more articles, more whatever. It was madness. And nonsense.

Anyways, your point is not totally invalid. I did a mistake when I changed from free to paid. I started with 99&cent; per month. The goal was to convert as many users as possible. This worked well. Only about 10-20% stopped using my service. However, it made future price increases harder.

There is not much to improve. In the space I'm operating in, the software, as it is, can be considered more or less complete. I mean, I can always add features, but the core mechanics, the way it works, the way it delivers value and the value people expect from the software is more or less complete. There's not much to add that improves the perceived value for people in such a way that they'd be willing to pay more for it.

But the fundamental idea is right: There must be some additional thing that's good for people, that people want. There's also related products, added benefit, whatnot. I won't do Gold membership, but I might be able to sell different things that complete the offering for some people.

Compare this to Microsoft Office. Word is for writing stuff. It's more or less feature-complete. Sure, you could add some stuff, but most people are happy with how it is in its basic edition. So Microsoft offers also Excel and PowerPoint. They are for very similar people, they complete the offering, but they're totally different. A "Word Gold" wouldn't make any sense.

> **For the owners of a small bootstrapped company, pretty much anything that doesn't increase revenue is a distraction and should be delegated to someone else, or some other company.**

* You could set up and configure an email server to send email to your users, or just use a service with an API like Mailgun or Mailchimp.

* You could write your own Wordpress theme and plugins from scratch, or just pay \$10 for a slick theme and \$50 for a few plugins that meet all your needs.

* You could spend time each week backing up your system with a set of external drives, or just pay a monthly fee to Crashplan or Dropbox.

* You could buy a discount VPS account and spend all your time keeping it patched and running, or just pay for a fully managed server until you outgrow it.

# [Pinboard Creator Maciej Ceglowski Talks About Why Boring Architecture is Good](http://readwrite.com/2011/02/10/pinboard-creator-maciej-ceglow)

There is absolutely nothing interesting about the Pinboard architecture or implementation,
**I consider that a feature.**

> **I believe that relying on very basic and well-understood technologies at the architectural level forces you to save all your cleverness and new ideas for the actual app, where it can make a difference to users.**

I think many developers (myself included) are easily seduced by new technology and are willing to burn a lot of time rigging it together just for the joy of tinkering. So nowadays we see a lot of fairly uninteresting web apps with very technically sweet implementations. In designing Pinboard, **I tried to steer clear of this temptation by picking very familiar, vanilla tools wherever possible so I would have no excuse for architectural wank.**

The other reason I like the approach is that the tried-and-true stuff is extensively debugged and documented. The chances of you finding a bug in MySQL or PHP as the author of a mid-sized website are microscopic. That's not the case for newer infrastructure like NoSQL or the various web frameworks.

* Use a RDBMS and take the time to learn it very thoroughly. High-Performance MySQL (the O'Reilly Book) and the Percona blog are indispensible for MySQL; I'm sure similar resources exist for Postgres.

* Use dedicated (not virtualized) hardware. I/O can be awful on virtualized servers and debugging I/O slowness there is next to impossible.

* Use caching as a last resort, not as a fundamental design strategy. It's 2011 - unless you have millions of users, your app should be able to run fast with all caches disabled.

* Use frameworks for prototyping, but build from the ground up once you know what you're building.

* Resist excessive abstraction

* Set performance targets for yourself. For example, one goal for Pinboard is a median page load time of under 300ms. This will force you to instrument well and optimize appropriately.

One interesting quirk of Pinboard is a complete absence of unit tests. I used to be a die-hard believer in testing, but in Pinboard tried a different approach, as an experiment. Instead of writing tests I try to be extremely careful in coding, and keep the code size small so I continue to understand it.

I've found my defect rate to be pretty comparable to earlier projects that included extensive test suites and fixtures, but I am much more productive on Pinboard.

# [Startup Playbook - Mike Subelsky](https://github.com/subelsky/startup_playbook)

* most interesting/urgent problems are B2B
 
* fun beats sexy
 
* hard/tedious == opportunity

* buyer == user

* main goal is to get feedback (worth the risk? worth the cost?)

* [The Lean Startup](http://theleanstartup.com/principles)

* [customer development](http://www.startuplessonslearned.com/2008/11/what-is-customer-development.html)

* [manual first](http://viniciusvacanti.com/2013/05/07/the-manual-first-startup/)

* [MVP](http://steveblank.com/2013/07/22/an-mvp-is-not-a-cheaper-product-its-about-smart-learning/) - the 80% solution

* don't launch, avoid press releases

* don't make people sign NDAs

* modular from the beginning (Rails engines, private gems - gemfury, open source as much as posibble, lay the groundwork for SOA)

* Rails Admin

* 0 defects, fix all bugs, root-cause analysis, BDD/TDD

* deploy continuously (avoid staging &amp; QA)

* document continuously (readme-driven-development)

* treat the db as a fortress

* don't build a JS front-end (?!?)

* don't run servers (Heroku, Papertrail, Mailgun etc.)

# [Idea Gardening: A Primer](http://davetroy.com/posts/idea-gardening-a-primer)

In Greek mythology, Sisyphus was sentenced to push a boulder up a hill, only to watch it roll back down so he could push it again for all of eternity. This is a decidedly bad gig. Some businesses are like this too, and too many well-meaning, intelligent entrepreneurs spend time on these kind of Sisyphean enterprises - consuming a lot of precious resources and never getting traction.

In business, there are rocks to push sometimes: hard work is always part of doing something worthwhile. But successful businesses always reach the summit and get to watch the rock roll down the other side.

So the advice to all entrepreneurs thus becomes essentially the same: **pick ideas that will work in the marketplace and expend resources only on those ideas**.

> **As a general rule of thumb, assume you know a lot less than you think you do about what ideas will work and what ideas won't, because you're likely wrong.**

Start thinking instead about what you want your idea garden to look
like:

* What ideas motivate you, and fill you with a sense of childlike wonder?

* What ideas give you inner peace and create a sense of aesthetic fulfillment?
 
* What higher causes do you aspire to?

* What causes do you think you can motivate others to rally around?

Today, we have tools to test the resonance of ideas with fairly wide audiences - for free. Twitter, Facebook, the web, and other mechanisms allow us to expand our networks to find people to bounce our ideas off of. Start bouncing your ideas - **the ideas you're most passionate about** - off of a wider audience. Put out feelers. See what sticks.

You may worry that sharing an idea with people will _let it out of the bag_ and someone else will _steal it_. You're not so smart to have come up with an idea that no one else has thought of before - believe that.

> **What you must have that is unique and irreplaceable is the vision, passion, and relationships required to bring your idea to fruition.**

But the idea itself - the raw two or three sentences that define your concept - has very little potential by itself. By sharing your idea with others, you can strengthen it. Others can contribute to it, pointing out the places where it's weak, and repurposing it in ways you never imagined. Don't be afraid to share your ideas, in whole or in part, so that others can help you bring them to fruition.

Gardening takes time - time for sunlight, for seed, for rain to converge in fecundity. The same is true of Idea Gardening. **Patience is required for ideas, people, and resources to converge in a way that releases stored energy.** If you're having to use too much pesticide (lawyering) or fertilizer (cash) to make your idea work, you're likely going against the forces of nature, and not taking advantage of the energy of the marketplace.

Don't overextend yourself by sinking resources into the first idea you have that looks to be viable. As a committed gardener, you will have many sprouts and leads that are viable. **Put your attention to the ideas that seem to be the strongest, and use all of your available resources to drive multiple ideas forward in parallel.** Otherwise you'll have the kind of fragile and brittle all-beet monoculture that will have a hard time surviving market conditions.

People are suckers for sunk costs; this is the instinct that makes folks want to double down in Vegas to recover their losses. But losses are losses, whether measured in time or in money, and **chasing after a failed idea to recover yourself to some perceived baseline is a mistake.**

Clearly the only rational thing to do is to stop burning money and move on to something that will work. It's not your fault it didn't work out - the market didn't want what you were selling. So, without pride or prejudice, stop the bleeding and move on. It may be your idea is still viable, but it might be viable for someone else, someplace else, at some different point in time. Put it on ice and return to it then.

The key to avoiding the trap of sunk costs in the Idea Gardening model is to minimize costs until an idea looks to be viable. If you have 10 ideas you're experimenting with, and 4 show promise, what can you do to put a minimal amount of investment in only those four that will advance them to a stage where you can learn more about their prospects?

For the software developers out there, this is the agile approach to business. **Start small, iterate, and follow the market need. This also means that failures, when they occur, happen quickly.** And that is the best thing any entrepreneur could hope for.

An entrepreneur who is also a software developer is uniquely positioned to try out dozens of ideas and let the market decide which ones will work.

Edison put himself into the Idea Gardening business. His labs in Menlo Park, New Jersey were a virtual playground for engineers. They generated more than 1,500 patents and went on to form General Electric. He was famously quoted as saying that **Genius is 1% inspiration and 99% perspiration**, but this is often misunderstood.

Edison wasn't saying that entrepreneurship was driven by **hard work** of the Sisyphean kind - no one can sustain that kind of load and be that prolific. Rather, he was suggesting that the hard work of invention lay in hoeing the rows and planting countless seeds of innovation so that, in time, the best ideas could bear fruit and thereby transform the world.

So entrepreneurs, plant your gardens. Give them sun, water, and time. The rest will follow, and you, too, will go on to transform the world.

# [The Code is your Enemy](http://blog.asmartbear.com/the-code-is-your-enemy.html)

Most startups fail because not enough people show up on the home page, or people show up but they don't try the product, or they don't pay for the product, or it's too expensive to get them to show up, or they don't tell their friends to come along, or because it's not solving a pain that people have, or it's not solving a pain that people know they have, or it's too hard to explain the pain, or a bunch of other things that are not whether the code works or whether it looks good.

> **Customers don't open their wallets based on your unit test coverage or whether you used Bodoni instead of Times New Roman on the home page.**
Those things are actually not the most important things.

1. Have you talked to 50 potential customers? By that I mean fifty, not
   a dozen. Watch out for bias in the first 10.

2. **Are people coming to your website every day?** If not, solving that is much harder and much more outside your control than building software.

Consider: Would you rather get hired as the CTO of a company with 1k daily new, unique, qualified visitors with no product, or the CTO of a company with a stable product and 10 uniques visits to the home page? You know you can solve for the first case; who knows about the second? But if you stay nose-down in the code instead of working on getting attention, that's exactly the company you're building.

So force yourself out of your comfort zone. You'll also do coding and design and that's fine of course, but force yourself to mostly do those other things that create a valuable business.

# [Staying fast and good when making enterprise software](http://www.subelsky.com/2013/11/staying-fast-and-good-when-making.html)

Code you wrote today doesn't end up getting in front of a user until next week, and you've already forgotten the context to explain why you wrote that code the way you did. When there's a bug, you have to start from scratch when figuring out the problem. Feedback gets delayed, if you get it at all.

#### Work in small batches

Smaller changes are less likely to cause a disruption, and are easier to pinpoint when they do cause problems.

#### Don't take shortcuts

Skipping tests and hacking things together ultimately add unacceptable risk and technical debt.

#### Practice five whys/root cause analysis

Don't make the same mistake twice; gradually develop a cluster immune system

#### Expect 100% unit test code coverage

Backed by appropriate integration and feature tests: catch problems as early as possible

#### Setup a continuous integration server

We have too many modules for any one developer to test all of them at once, so this server will become a backstop, alerting us when there is a problem in one of our modules.

#### Use Feature flags

While we are testing new features, hide them from users until they are fully baked.

# [Case Study: Continuous deployment makes releases non-events](http://www.startuplessonslearned.com/2010/01/case-study-continuous-deployment-makes.html)

Continuous Deployment is Continuous Flow applied to software. The goal of both is to eliminate waste. The biggest waste in manufacturing is created from having to transport products from one place to another.

> **The biggest waste in software is created from waiting for software as it moves from one state to another: waiting to code, waiting to test, waiting to deploy.**
Reducing or eliminating these waits leads to faster iterations which is the key to success.

Prior to adopting continuous deployment, I used to release software on a weekly schedule (come rain or shine) which I viewed as pretty agile, disciplined, and aggressive. I identified the must-have code updates on Monday, official code cutoff was on Thursday, and Friday was slated for the big release event. The release process took at least half a day and sometimes the whole day. Dedicating up to 20% of the week on releasing software is incredibly wasteful for a small team.

> **The fundamental challenge with Continuous Deployment is getting comfortable with releasing all the time.**

I took things easy at first - made small changes and audited the release process maniacally. I started relying heavily on functional tests (over unit tests) which allowed me to test changes as a user would. I also identified a set of events that would indicate something terribly going wrong (e.g. no users on the system) and built real-time alerting around them (using nagios/ganglia). As we built confidence, we started committing bigger and multi-part changes, each time building up our suite of testing and monitoring scripts. After a few iterations, our fear level was actually lower than how we used to feel after a staged release. Because we were committing less code per release, we could correlate issues to a release with certainty.

Smaller releases lead to faster build/measure/learn loops. I've used these faster build/measure/learn loops to optimize my User Activation flow, delight customers with "near-instant" fixes to issues, and even eliminate features that no one was using.

**DON'T keep adding features until you've validated the MVP**, or more specifically the unique value proposition of the MVP. Unneeded features are waste and not only create more work but can needlessly complicate the product and prolong the "customer validation" phase.

> **Build in response to a signal from the "customer", otherwise rest or improve.**

As a technologist, I too love to measure progress based on how much stuff I build. But instead of channeling all my energy towards building new features, I channel roughly **80% of it towards measuring and optimizing existing features**. I am not advocating adding no features at all. Users will naturally ask for more stuff and your MVP by definition is minimal and needs more love. Just don't push it.

I've previously described my 2 hour blocks of maker time for maximizing my "work flow". Prior to starting any maker activity, I clearly identify what needs to get done (the goal) and sketch out how it needs to get done (the design).

> **Prefer functional tests over unit tests whenever possible**

I deem excessive unit testing a form of waste. Whenever possible, I rely on functional tests that verify user actions.

# [What is customer development?](http://www.startuplessonslearned.com/2008/11/what-is-customer-development.html)

> **Get out of the building**

Very few startups fail for lack of technology. They almost always fail for lack of customers. I've been guilty of this many times in my career - it's just so easy to focus on product and technology instead.

#### In a startup no facts exist inside the building, only opinions

Most likely, your business plan is loaded with opinions and guesses, sprinkled with a dash of vision and hope.

Surprisingly early, you can start to get a sense for who the customer of your product might be, how you'll reach them, and what they will ultimately need. Customer development is an attempt to minimize the risk of total failure by checking your theories against reality.

#### Theory of market types

There are three fundamental situations that change what your company needs to do:

* creating a new market

* bringing a new product to an existing market

* resegmenting an existing market

#### Finding a market for the product as specified

Our goal in product development is to find the minimum feature set required to get early customers.

#### Phases of product &amp; company growth

* Customer Discovery (when you're just trying to figure out if there are any customers who might want your product)

* Customer Validation (when you make your first revenue by selling your early product)

* Customer Creation (akin to a traditional startup launch, only with strategy involved)

* Company Building (where you gear up to Cross the Chasm)

#### Learning and iterating vs. linear execution

In the beginning, startups are focused on figuring out which way is up. They really don't have a clue what they should be doing, and everything is guesses.

Startups need time spent in a mindset of learning and iterating, before they try to launch. During that time, they can collect facts and change direction in private, without dramatic and public embarrassment for their founders and investors.

# [The Lean Startup](http://theleanstartup.com/principles)

Too many startups begin with an idea for a product that they think people want. They then spend months, sometimes years, perfecting that product without ever showing the product, even in a very rudimentary form, to the prospective customer. When they fail to reach broad uptake from customers, it is often because they never spoke to prospective customers and determined whether or not the product was interesting. When customers ultimately communicate, through their indifference, that they don't care about the idea, the startup fails.

The lack of a tailored management process has led many a start-up to _a human institution designed to create a new product or service under conditions of extreme uncertainty_, to abandon all process. They take a _just do it_ approach that avoids all forms of management. But this is not the only option. Using the Lean Startup approach, companies can create order not chaos by providing tools to test a vision continuously.
Lean isn't simply about spending less money. Lean isn't just about failing fast, failing cheap. It is about putting a process, a methodology around the development of a product.

The Lean Startup methodology has as a premise that every startup is a grand experiment that attempts to answer a question. The question is not _Can this product be built?_ Instead, the questions are **Should this product be built?** and **Can we build a sustainable business around this set of products and services?** This experiment is more than just theoretical inquiry; it is a first product.

A core component of Lean Startup methodology is the build-measure-learn feedback loop. The first step is figuring out the problem that needs to be solved and then developing a minimum viable product (MVP) to begin the process of learning as quickly as possible. Once the MVP is established, a startup can work on tuning the engine. This will involve
measurement and learning and must include actionable metrics that can demonstrate cause and effect question.

Progress in manufacturing is measured by the production of high quality goods. The unit of progress for Lean Startups is validated learning - a rigorous method for demonstrating progress when one is embedded in the soil of extreme uncertainty.


# [How To Make It as a First-Time Entrepreneur](http://viniciusvacanti.com/2013/05/07/the-manual-first-startup/)

So, we took a shortcut. Instead of building a crawler, my co-founder and I would crawl out of bed at 3 am and manually enter the deals into our database. Plus, when youâ€™re doing it yourself, classification was easy. **We did it all manually.**

The funny thing is that within a few months of our launch, several competitors emerged and they all had crawlers. But, from our usersâ€™s perspective, we were more advanced since we had categorization which was definitely no easily automated task.

We didnâ€™t actually build a real crawler for the first 9 months and just kept scaling manually by hiring more data entry professionals. Instead, we were able to focus our resources on improving the product and user acquisition.

It's now clear to me that not building that crawling technology early on was one of the reasons our startup succeeded.

Taking this "manual-first" approach was our secret sauce.

ZeroCater, a Y Combinator company, started with just a big spreadsheet trying to connect companies with restaurants that would cater.

Groupon started with just a WordPress blog and manually sending PDFs with the first vouchers.

Fastest way to get to the **moment of truth**. Having your potential customer evaluate your product and see if it addresses their need is the moment every founder is trying to get to and doing things manually allow you to quickly get there.

Easy to change your solution if it doesn't work. There's no code to re-write, there's no sunk cost. You just have to change how you're manually doing something.

Will really understand what to automate with tech when you've been manually doing it. When you've been manually providing the solution, you'll know exactly where the pain points are that you should be automating.

Can really wow your potential customers. When you do things manually, you can try different things that really wow the customer and see which ones are worth trying to scale.

> Customer doesn't know how your product works behind the scenes. They won't judge you for your manual approach because they don't know that's how you're doing it. All they will care about it is that your product works.

Your product will **just work**. Because you're manually providing the solution, the product will just work. When trying to implement a solution with technology, it can be very hard to make sure that it just works.

Helps you focus your time on the problem, not the solution. It's very tempting to fall in love with the technology behind your solution only to painfully realize that the problem you set out to solve isn't a real problem.

Reid Hoffman, founder of LinkedIn, once said: **If you're not embarrassed by your first release, you probably spent too much time on it.**

> **If people don't laugh at how you first implemented your product, you probably spent too much time on it.**

# [An MVP is not a Cheaper Product, it's about Smart Learning](http://steveblank.com/2013/07/22/an-mvp-is-not-a-cheaper-product-its-about-smart-learning/)

They concluded that the only way to get a delighted early customer was to build an MVP. They believed that the MVP needed to:

1. demonstrate a drone flight
2. make sure their software could stitch together all the images of a field
3. present the data to the farmer in a way he could use it

The team confused the goal of the MVP (seeing if they could find a delighted farmer who would pay for the data), with the process of getting to the goal. They had the right goal but the wrong MVP to test it.

Since the startup defined itself as a data services company, at the end of the day, the farmer couldn't care less whether the data came from satellites, airplanes, drones, or magic as long as they had timely information.

What they needed to spend their time is first testing is whether farmers cared about the data.

Would it be cheaper to rent a camera and plane or helicopter, and fly over the farmers field, hand process the data and see if that's the information farmers would pay for?  Couldn't you do that in a day or two, for a tenth of the money you're looking for?

A minimum viable product is not always a smaller/cheaper version of your final product. Think about cheap hacks to test the goal. Great founders keep their eye on the prize.

# [Nine Webcasts to Learn From](http://www.startuplessonslearned.com/2013/11/nine-webcasts-to-learn-from.html)

#### Kent Beck

When I was younger I was convinced that programming was the most fun thing I would ever do and I'd be very happy to program increasingly large systems myself. And I think basically what happened was I kept doing that, and not having the impact I wanted to have. Because in my fantasy I could produce a massive program that's used by billions of
people and has enormous complexity and is incredibly innovative, by myself. Just, you know, with my bare hands.

> **The truth of any program is, it requires teams, and customers, and it's this complicated ecosystem...**

So the person who's considered the _founder_ or the person who created the complicated system, it doesn't matter if it's Linux or Facebook or anything, **somebody had to plant that initial seed**, and that's very satisfying.

> **In order for us to remember it and to care about the fact that they are the founder of that thing, they had to do an incredible amount of management of people to get them to grow that seed into something that is significant**.

And what's frustrating to me - it was then and it still is - is that as soon as I became a manager and a team leader and an architect and really thinking out how to do that stuff, **I was doing human systems engineering and I was no longer making things with my bare hands**. And so I've also had that frustration. Now, that's frustrating but also very satisfying, in that I'm very proud of the things that teams that I've worked with have built. But for me anyway, that transition from being a team leader to whatever it is that I do now, to try to cultivate this community and try to share these ideas on a wider scale - that was actually a much easier transition than going from an individual contributor to a team leader. Because to me, it's like, as soon as I was not making things myself, with my bare hands, it's all about, ok, then what activities will give me the greatest influence to have the impact I want to see in the world?

> **It's a fundamental waste of human energy and talent to have people working on things that nobody wants and that have no impact. That's actually morally wrong to have a system that does that.**

# [How to Build a Subscription Service on Rails: A Noob's Guide](http://www.joelhooks.com/blog/2013/10/30/how-to-build-a-subscription-service-on-rails-a-noobs-guide)

> It is all about finding pain and providing value to people in exchange for money.

It is also astonishingly easy to deploy Rails apps to Heroku. For production, Heroku quickly becomes not free as you add in background workers, SSL, and other essential pieces, but you can go a long way with free.

Stripe through [Koudoku gem](https://github.com/andrewculver/koudoku)

# [How to Stub External Services in Tests](http://robots.thoughtbot.com/how-to-stub-external-services-in-tests)

When integrating with external services we want to make sure our test suite isn't hitting any 3rd party services. Our tests should run in isolation.

```
require 'webmock/rspec'
WebMock.disable_net_connect!(allow_localhost: true)
# ...
stub_request(:get, /api.github.com/).with(headers: {}).to_return(status: 200, body: "", headers: {})
```

Use WebMock to route all requests to our Sinatra app:

```
stub_request(:any, /api.github.com/).to_rack(FakeGitHub)

require 'sinatra/base'

class FakeGitHub < Sinatra::Base
  get '/repos/:organization/:project/contributors' do
    json_response 200, 'contributors.json'
  end

  private

  def json_response(response_code, file_name)
  end
end
```

# [Constant Resolution in Ruby](http://valve.github.io/blog/2013/10/26/constant-resolution-in-ruby)

Ruby constant is anything that starts with a capital.

`Module.nesting` returns an array of searcheable lexical scopes, starting from current.

When Ruby has finished searching the constants up the nesting and ancestors chain and didn't find it, it gives the calling code the last chance by calling the `const_missing` method.

When Ruby finds a constant with a given name, it stops looking further.

# [LEGOs, Play-Doh, and Programming](http://weblog.jamisbuck.org/2008/11/9/legos-play-doh-and-programming)

In order to master LEGO brick building, you have to know all of the pieces available to you, and have a good intuitive feel for how and when they should be used. That's a lot of information to keep tabs on.

Where LEGO models require significant work to alter or extend, Play-Doh models are dead-simple. If you want to add something to the base of your model, just graft more Play-Doh onto it. Want to change the shape of the keystone of your arch? Just pinch and mold in place.

Ruby's philosophy is like that of Play-Doh's: provide a basic set of tools and make it relatively simple to build something complex with them. The very Ruby language itself is designed for this: closures, super-simple introspection of objects, runtime modification of existing objects, and the use of modules for extending classes and objects all tend to result in an environment that is simple, malleable, and extensible.

> **Always, always, always build just what you need, and only when you need it.**
You're in Ruby, the Play-Doh of programming languages, and the cost of adding features later is really, really low.

Expose only what you need. The rest can be there, available, but not formally exposed. Only when (and if) you discover a need to expose more, should you expose more.

* A smaller API is easier to describe, document, and support.
* A smaller API is easier for people to learn.
* A smaller API is easier for you to test.
* Extending a small API is much less onerous on your users than changing or restricting a larger API.

Now, separation of concerns and modularity are good things, when used in moderation. But like any design pattern, it becomes evil when taken to extremes. Too much modularity and you wind up with component soup (and I hope you're hungry, because you're going to have lots of it).

> With lots of tiny components, the interactions between those components can become
difficult to test.

> **When I added dependency injection to the mix, it became very, very difficult to follow the flow of the program, and to understand the dependencies.**

No component was too small! No object too insignificant! I was on the dependency injection horse, and riding it for all it was worth.

**Loose coupling** and **high cohesion** are terms you'll hear bandied about in defense of dependency injection, and those traits are certainly desirable. **But strike a balance with pragmatism.** There will be some who call me heretic for saying this, but don't be afraid to introduce tighter coupling when it makes sense. Loose coupling everywhere is what I had with `Net::SSH 1.x`, and the result was nearly unmaintainable.

> **Be wise. You're competent. Trust your instincts.**

**Just in time. Not just in case.** Don't play _what if_ games when you're coding. Practice discipline, and implement only what you need, when you need it. You'll wind up with tighter, more testable code that is easier to maintain in the long run.

# [Break Apart Rails Monoliths Using This 1 Weird Trick](https://www.bluebox.net/insight/blog-article/break-apart-rails-monoliths-using-this-1-weird-trick)

No business logic in models, views or controllers, put them in POROs.

Giant classes - difficult to understand, feels like luck that anything works.

Let the business object orchestrate the message passing between model objects.

# [Introducing Mutations: Putting SOA on Rails for security and maintainability](https://developer.uservoice.com/blog/2013/02/27/introducing-mutations-putting-soa-on-rails-for-security-and-maintainability/) {#mutations}

[mutations](https://github.com/cypriss/mutations) - compose your business logic into commands that sanitize and validate input.

Problems:

* different user interfaces with different controllers serving them

* invalid input polluting exception logs

* extremely fat models with callbacks that feed into each other

#### Service Layer Patter

Each service object is essentially a module of code that has one primary method and one responsibility, eg `TwitterPoster.post(tweet, user)`

We've called these service objects **Mutations**, as quite often they represent a change of state in our database from state A to state B.

Removed model callbacks and put them into service objects. Each mutation operates in a single context, there's little conditional logic.

Mutations let you validate and specify inputs on your service objects.

By building an internal API (service layer) that your controllers can use, you'll have less code duplication if you have multiple controllers that operate on the same types of data.

If you specify that a parameter must be a string, then inside the execute method, it is guaranteed to be a string. If an ignorant or malicious user passes in an array, then an error will be raised. If the parameter can be safely coerced to a string (eg an integer or boolean is passed; `3` becomes "3" and `false` becomes "false"), then the coercion is done and you have a string. This makes it much easier to write correct code, and correct code is much more likely to be secure.

With a Mutation, you have a single business rule that you're coding against. You're likely to have a short, linear method.

# [7 Patterns to Refactor Fat ActiveRecord Models](http://blog.codeclimate.com/blog/2012/10/17/7-ways-to-decompose-fat-activerecord-models/)

ActiveRecord classes handle persistence, associations and not much else.

Refactor to break them down and spread out the logic evenly. Repeat this process and you'll end up with a set of simple objects with well defined interfaces working together in a veritable symphony.

> **Any application with an `app/concerns` directory is concerning.**

**Prefer composition to inheritance.** Using mixins like this is akin to _cleaning_ a messy room by dumping the clutter into six separate junk drawers and slamming them shut. Sure, it looks cleaner at the surface, but the junk drawers actually make it harder to identify and implement the decompositions and extractions necessary to clarify the domain model.

**Value Objects** (think Domain-Driven Design) are simple objects whose equality is dependent on their value rather than an identity. They are usually immutable. Date, URI, and Pathname are examples from Ruby's standard library.

Some actions in a system warrant a **Service Object** to encapsulate their operation. I reach for Service Objects when an action meets one or more of these criteria:

* it is complex (eg closing the books at the end of an accounting period)

* it reaches across multiple models (eg an e-commerce purchase using Order, CreditCard and Customer objects)

* it interacts with an external service (eg posting to social networks)

* it is not a core concern of the underlying model (eg sweeping up outdated data after a certain time period)

* there are multiple ways of performing the action (eg authenticating with an access token or password). This is the Gang of Four Strategy pattern.

When multiple ActiveRecord models might be updated by a single form submission, a Form Object can encapsulate the aggregation. Virtus to the rescue, Form Objects quacking like an ActiveRecord.

Since validation logic is often contextual, it can be defined in the place exactly where it matters instead of needing to guard validations in the ActiveRecord itself.

For complex SQL queries littering the definition of your ActiveRecord subclass (either as scopes or class methods), consider extracting query objects.

If logic is needed purely for display purposes, it does not belong in the model.

Sometimes complex read operations might deserve their own objects. In these cases I reach for a Policy Object. This allows you to keep tangential logic, like which users are considered active for analytics purposes, out of your core domain objects.

I use the term **Service Object** for write operations and **Policy Object** for reads. They are also similar to Query Objects, but **Query Objects** focus on executing SQL to return a result set, whereas Policy Objects operate on domain models already loaded into memory.

Decorators let you layer on functionality to existing operations, and therefore serve a similar purpose to callbacks.

One sign you've added too many responsibilities in callbacks is slow and brittle tests or an urge to stub out side effects in wholly unrelated test cases.

Even in a Rails application, there are many tools to manage complexity in the model layer. None of them require you to throw out Rails. ActiveRecord is a fantastic library, but any pattern breaks down if you depend on it exclusively.

# [Crazy, Heretical, and Awesome: The Way I Write Rails Apps](http://jamesgolick.com/2010/3/14/crazy-heretical-and-awesome-the-way-i-write-rails-apps.html)

As applications grew, test suites would get slow - like minutes slow. When you're depending on your persistence objects to do all of the work, your unit tests absolutely must hit the database, and hitting the database is slow. It's a given in the rails world: `big_app == slow_tests`.

But slow tests are bad. Developers are less likely to run them. And when they do, it takes forever, which often turns in to checking twitter, reading reddit, or a coffee break, harming productivity.

Also, coupling all of your business logic to your persistence objects can have weird side-effects. In our application, when something is created, an after_create callback generates an entry in the logs, which are used to produce the activity feed. What if I want to create an object without logging - say, in the console? I can't. Saving and logging are married forever and for all eternity.

When we deploy new features to production, we roll them out selectively. To achieve this, both versions of the code have to co-exist in the application. At some level, there's a conditional that sends the user down one code path or the other. Since both versions of the code typically use the same tables in the database, the persistence objects have to be flexible enough to work in either situation.

To decouple the logging from the creation of the database record, we're going to use something called a service object. A service object is typically used to coordinate two or more objects; usually, the service object doesn't have any logic of its own (simplified definition). We're also going to use Dependency Injection so that we can mock everything out and make our tests awesomely fast (seconds not minutes).

Have you ever been in a situation where a model wouldn't save because a callback was mistakenly returning nil? Debugging (necessarily) opaque callback mechanisms is hard.

You might even write a few percent more lines of actual code. But you'll wind up with more maintainability for it (not to mention faster tests, code that's easier to understand, etc).

The truth is that in a simple application, obese persistence objects might never hurt. It's when things get a little more complicated than CRUD operations that these things start to pile up and become pain points. That's why so many rails plugins seem to get you 80% of the way there, like immediately, but then wind up taking forever to get that extra 20%.

Ever wondered why it seems impossible to write a really good state machine plugin - or why file uploads always seem to hurt eventually, even with something like paperclip? It's because these things don't belong coupled to persistence. The kinds of functionality that are typically jammed in to active record callbacks simply do not belong there.

# [Where's Your Business Logic?](http://collectiveidea.com/blog/archives/2012/06/28/wheres-your-business-logic/)

The fundamental problem with almost every Rails project (and I'm sure in other frameworks as well), is that there is no direct codifying of the business rules and use cases of the application. There is no single location you can point to and say _here, these objects implement our use cases_.

First, we rarely spend enough time up front thinking about the problem space and designing a solution, and second, we haven't been listening and reacting to test pain.

#### How do you mock behaviour provided by other "Interactor" classes?

I'm taking the stance of never mocking objects I own. I know this goes against the "isolate and test" idea, but I've been bit pretty hard by code with too many mocks, and mocking done wrong, so I don't mock them. As this project progresses I'll see if this idea is manageable or if interface mocking is really needed for keeping tests clean.

#### Aren't you simply describing the Command pattern?

This pattern is focused on being an implementation of a single application use-case, which is IMO higher level than a set of Commands that act on your domain models. The other distinction I see is that where Commands are supposed to be given everything they need to act, Interactors are supposed to be the top level access to your application.

There is a very decent implementation of the [Repository pattern](https://github.com/playlouder/persistence). Tim Cowlishaw and Tom Stuart (among others) have been working on it in 2011 (part of PlayLouder it seems).

Robert Martin has called Interactions **Transactions** in his books, where they're indeed an implementation of Command pattern. What distinguish your Interactors from Commands (or Transactions) is lack of consistent interface: method `run` takes different number of arguments. In comparison, Martin provides all required information for Transactions in constructors and setter methods. I see no reason not to provide consistent interface.

It looks to me like this is already something Eric Evans has described in Domain Driven Design with Domain Service objects (which have to be distinguished from Application Service objects.)

> **Services in DDD conflict with Martin's Transactions**
>
> **DDD tells**: Services should not have their state
> **Martin tells**: Transactions are just Command objects (thus they have state).

_Feels like low-ceremony DCI_

_Nice ideas, though your interactor class is really just a function forced into an object oriented perspective_

#### [guru_watch](https://github.com/qertoip/guru_watch)

App using Rails for the front-end and ActiveRecord for the back-end that demonstrates the Use Case Driven Approach aka Entity-Controller-Boundary.

> **Personal observations**
>
> The app is too simple, it has a single model.
>
> Almost all "use-cases" are actually persistence-related and do nothing more than spread the model and the controller concerns into command objects.
>
> The ActiveRecord back-end feels like its exposing the internals of the library which are abstracted for a reason.
>
> ActiveMemory doesn't look like a real thing.
>
> Associations? Callbacks? Guards/filters?

[See the mutations gem](#mutations) (it combines the command object with sanitizing &amp; validations).


# [The Principles of OOD](http://butunclebob.com/ArticleS.UncleBob.PrinciplesOfOod)

Whenever we bring up on our screens a nasty batch of tangled legacy code, we are experiencing the results of poor dependency management. Poor dependency managment leads to code that is hard to change, fragile, and non-reusable.

Abbreviation | Name | Description
- | -| -
SRP | The Single Responsibility Principle | A class should have one, and only one, reason to change
OCP | The Open Closed Principle | You should be able to extend a classes behavior, without modifying it
LSP | The Liskov Substitution Principle | Derived classes must be substitutable for their base classes
ISP | The Interface Segregation Principle | Make fine grained interfaces that are client specific
DIP | The Dependency Inversion Principle | Depend on abstractions, not on concretions


#### Package principles

Abbreviation | Name | Description
- | -| -
REP | The Release Reuse Equivalency Principle | The granule of reuse is the granule of release
CCP | The Common Closure Principle | Classes that change together are packaged together
CRP | The Common Reuse Principle | Classes that are used together are packaged together
ADP | The Acyclic Dependencies Principle | The dependency graph of packages must have no cycles
SDP | The Stable Dependencies Principle | Depend in the direction of stability
SAP | The Stable Abstractions Principle | Abstractness increases with stability

# [Why I wrote gocraft/web](https://developer.uservoice.com/blog/2013/12/12/why-i-wrote-gocraft-web/)

The `UserRequired` middleware would validate your session and find the appropriate user. If there was no session, it would redirect you to a login secreen. Each time I wanted to have middleware interact with my structs in this way, I'd have to write more reflection.

Traffic is a lightweight framework (akin to Sinatra), but it also requires type assertions.

Both Revel and Traffic are good web frameworks, and I'd recommend them to anyone looking for a framework. But as I get more experience, I'm finding I don't want a framework.

Don't tell me how to do app config. I don't want to use your INI config file format. I want to use TOML, or I want to use environment variables ala 12-factor apps, or I want to use etcd.

Another fallacy of many web frameworks is they assume you only want one application per executable. They assume this by using global state. This prevents you from binding different servers to different interfaces/ports to give you a public web experience and a protected health/monitoring API. Global state sucks.

**They're not controllers; they're contexts**. They don't control anything. They're simply a struct, a piece of data, that is created for you on each request, that lets you set request-specific fields on as it gets passed down your middleware.


# [Vundle, the plug-in manager for Vim](https://github.com/gmarik/vundle)

# [Anti-Pattern: Iteratively Building a Collection](http://robots.thoughtbot.com/iteration-as-an-anti-pattern)

```
def mutually_signed?
  signers.map(&:signed_by?).inject(:&)
end

def mutually_signed_concise?
  signers.all?(&:signed_by?)
end
```

# [Visualizing Garbage Collection in Ruby and Python](http://patshaughnessy.net/2013/10/24/visualizing-garbage-collection-in-ruby-and-python)

The standard version of Ruby, also known as "Matz's Ruby Interpreter" (MRI), uses a GC algorithm similar to the one used by McCarthyâ€™s implementation of Lisp in 1960. For better or worse, Ruby uses a 53 year old algorithm for garbage collection. Just as Lisp did, Ruby creates objects ahead of time and hands them to your code when you allocate new objects or values.

Ruby doesn't immediately clean up old objects my code is no longer using! Working as a Ruby developer is like living in a messy house, with clothes lying on the floor or dirty dishes in the kitchen sink. As a Ruby developer you have to work with unused, garbage objects surrounding you. Ruby uses Mark and Sweep algorithm invented by John McCarthy (worked on the original implementation of Lisp) to clean up this messy house every now and then.

At first glance, Python's GC algorithm seems far superior to Rubyâ€™s: why live in a messy house when you can live in a tidy one? Why does Ruby force your application to stop running periodically each time it cleans up, instead of using Python's algorithm?

Reference counting is difficult to implement. Python has to leave room inside of each object to hold the reference count. There's a minor space penalty for this. But worse, a simple operation such a changing a variable or reference becomes a more complex operation since Python needs to increment one counter, decrement another, and possibly free the object.

It can be slower. Although Python performs GC work smoothly as your application runs (cleaning dirty dishes as soon as you put them in the sink), this isn't necessarily faster. Python is constantly updating the reference count values. And when you stop using a large data structure, such as a list containing many elements, Python might have to free many objects all at once. Decrementing reference counts can be a complex, recursive process.

Reference counting can't handle cyclic data structures â€“ data structures that contain circular references.

# [How does Bundler do its magic!?](http://kerdany.wordpress.com/2013/11/23/how-does-bundler-do-its-magic)

In Ruby1.9+ RubyGems is included in RubyCore, and is enabled by default, and there's no need to `require 'rubygems'`, it works out of the box.

<pre>
ri Kernel#require

When you call require 'x', this is what happens:
* If the file can be loaded from the existing Ruby loadpath, it is.
* Otherwise, installed gems are searched for a file that matches. If it's
  found in gem 'y', that gem is activated (added to the loadpath).

The normal require functionality of returning false if that file has already
been loaded is preserved.
</pre>

After Bundler resolves the exact gem versions that we want to be activated/enabled, `Bundler.setup` (or `require 'bundler/setup'`) simply collects the paths to these gem versions' lib/ directories, and prepends them all to the `$LOAD_PATH` global array, where any subsequent call to a `require '<file>'` would first start looking into the lib/ directories enabled by Bundler.

Any subsequent call to `require 'file'`, will thus simply find the needed file in the `$LOAD_PATH`, since it's now ready with the correct gem versions. There will be no need to search for new directories to add to the load path, and if the &lt;file&gt; wasnâ€™t found in the `$LOAD_PATH` prepared by Bundler, a LoadError will be raised.

In order to prevent `Kernel#require` from looking further in the other installed gems, Bundler also disables the patched `Kernel#require` by reverting to the original implementation.

`Bundler.require` on the other hand, is just a convenience method that simply auto-requires all the code in your bundler, without you having to make lots of calls to `require`. `Bundler.require` also implicitly calls `Bundler.setup`, so you don't have to call `Bundler.setup` before calling `Bundler.require`.

`bundle exec` adds `-rbundler/setup` ruby command-line option (equivalent to `require 'bundler/setup'` and `Bundler.setup`) to the `$RUBYOPT` variable, so that when it executes any ruby &lt;command&gt; in a sub shell/process, Bundler will do its magic (as explained above). It invokes command through `Kernel#exec`.

# [How to Optimize Unicorn Workers in a Ruby on Rails App](https://www.digitalocean.com/community/articles/how-to-optimize-unicorn-workers-in-a-ruby-on-rails-app)

Unicorn uses forked processes to achieve concurrency. Since forked processes are essentially copies of each other, this means that the Rails application need not be thread safe.

Unicorn gives our Rails apps concurrency even when they are not thread safe. However, this comes at a cost. Rails apps running on Unicorn tend to consume much more memory.

When a child process is forked, it is the exact same copy as the parent process. However, the actual physical memory copied need not be made. Since they are exact copies, both child and parent processes can share the same physical memory. Only when a write is made - then we copy the child process into physical memory.

When the garbage collector of Ruby 1.9 kicks in, a write would have been made, thus rendering CoW useless.

`preload_app` should be set to true so that the start up time of the Unicorn worker processes is reduced. This uses CoW to preload the application before forking other worker processes. However, there is a big gotcha. We must take special care that any sockets (such as database connections) are properly reopened:

```
after_fork do |server, worker|
  if defined?(ActiveRecord::Base)
    ActiveRecord::Base.establish_connection
  end

  if defined?(Resque)
    Resque.redis.client.reconnect
  end
end
```

# [Screen Scraping With A Saw: A Nokogiri Tutorial With Examples](http://ruby.elevatedintel.com/blog/screen-scraping-with-a-saw-a-nokogiri-tutorial-with-examples)

# [Powering Actions with ElasticSearch Percolate](http://blog.feedbin.me/2013/11/10/powering-actions-with-elasticsearch-percolate)

Percolate allows you to register queries ahead of time, then whenever you send a new document to be indexed, ElasticSearch can tell you if this document matches any saved queries.

A model called Entry is used for storing RSS articles. Whenever a new entry is added it also gets sent to elasticsearch for indexing.

# [Announcing Suro: Backbone of Netflix's Data Pipeline](http://techblog.netflix.com/2013/12/announcing-suro-backbone-of-netflixs.html)

1. Applications emits events to Suro. The events include log lines.

2. Suro dispatches all the events to S3 by default. Hadoop jobs will process these events.

3. Based on a dynamically configurable routing rule, Suro also dispatches these log events to a designated Kafka cluster under a mapped topic.

4. Druid cluster indexes the log lines on the fly, making them immediately available for querying. For example, our service automatically detects error surges for each application within a 10 minute window and sends out alerts to application owners

5. A customized ElasticSearch cluster also ingests the same sets of log lines, de-duplicates them, and makes them immediately available for querying. Users are able to jump from an aggregated view on the Druid UI to individual records on ElasticSearch's Kibana UI to see exactly what went wrong.

Suro evolved over the past few years alongside many other powerful data pipeline solutions such as Apache Flume and Facebook Scribe.

# [What is Druid?](http://druid.io/druid.html)

Druid is open source infrastructure for real-time exploratory analytics that supports fast ad-hoc queries on large-scale data sets.

Druid's real-time nodes employ lock-free ingestion of append-only data sets to allow for simultaneous ingestion and querying of 10,000+ events per second. Simply put, the latency between when an event happens and when it is visible is limited only by how quickly the event can be delivered to Druid.

Druid is purpose built infrastructure that provides for exploration of very large quantities of data as it is ingested into the system. It is currently used for dashboarding of ad impression streams and operational monitoring of systems. If you have a dataset that is too large for your current infrastructure, your data has a timestamp associated with every event and you want to arbitrarily filter into the data with your
queries, then Druid can probably provide value for whatever your use case is as well.

# [jq](http://stedolan.github.io/jq/)

**jq** is like sed for JSON data - you can use it to slice and filter and map and transform structured data with the same ease that **sed**, **awk**, **grep** and friends let you play with text.

# [The Log: What every software engineer should know about real-time data's unifying abstraction](http://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying)

We were just beginning to run up against the limits of our monolithic, centralized database and needed to start the transition to a portfolio of specialized distributed systems. This has been an interesting experience: we built, deployed, and run to this day a distributed graph database, a distributed search backend, a Hadoop installation, and a first and second generation key-value store.