# [5 things I've learned in 5 years of running a SaaS](http://mir.aculo.us/2013/11/27/5-things-ive-learned-in-5-years-of-running-a-saas/)

#### 1. You're not a tech company - you're a make customers awesome company

People don't pay you because you have amazing programming skills and can write nginx configurations blindfolded. People pay you money because the product you sell to them saves them time, money, effort and nerves. It's your job to make your customer more awesome. Every decision you make for your product and business should revolve around that.

#### 2. Never promise dates for a feature launch

#### 3. Spend money on things that help you stay productive

#### 4. Do not work too much

Overworking yourself is the first step to failure in business. You can't do your best if you're permanently stressed out. Don't check email in the evenings.

#### 5. Don't believe the hype

People are good at getting excited. And people are good at believing the hype about new technologies, frameworks, programming languages and ways to deploy.

The fact is that you need to be pragmatic - **your goal is to run a business**. Use technology that is proven - **to you** -, and that you know how to work with.

You need to optimize for shipping. That includes writing less code, having broad test coverage, and concentrate on getting things out in order of long-term profitability for your business.

## [HN Comments](https://news.ycombinator.com/item?id=6818679)

My PHP application without any fancy technology makes me close to 100,000 € in revenue per year. It delivers, it keeps delivering, there are no problems. It's consumer-oriented, it costs 2 euro a month, it has several thousand users.

I concentrated on important things, I do customer support myself, I listen to people, I'm friendly, I help people solve their problems. I care about them, about their privacy and do everything to protect it. I write emails that take me 30 minutes if it's necessary. Why? Because I only get a handful of emails per day. It's not scalable but then, it doesn't have to be. I do some SEO and rank well for my niche keywords. About 50-70% is word of mouth.

I don't have a blog, I didn't integrate any social network crap, I don't use Google Analytics or any other fancy third-party application that must be included via JS. I use Piwik that I check occasionally (sometimes every week, sometimes only every few months). I don't have big metrics. My most imporant metrics are: Money earned per week/month. Amount of active (paid) users. Signups per week.

My product has nothing to do with PHP and doesn't target IT people. Just in case this is misinterpreted. I mention PHP only, because people on HN hate PHP that much and sometimes forget that it doesn't matter if it's PHP or anything else, as long as it pays the bills. PHP is fast, PHP is reliable and I get shit done in PHP. That's why I use PHP.

After a long thought process and endless discussions with an older friend, I came to the conclusion that literally change my life: **I had to start charging people for my product. All of them.** No freemium nonsense and all that other crap that was in my list.

Being the altruistic student, I anticipated that people would be disappointed. I could almost feel their soon-to-come disappointment and asking them for money was something that didn't fit my world view for a long time. It was hard to accept that I'd lose people. Sounds stupid in hindsight, but that's how I felt.

> **If people don't value what you have to offer, they're not worth your time.**

So, even if people were disappointed, it wouldn't be my fault. After all, I offer a truly great service and it should be worth a couple bucks a month.

I got this braindead idea to rewrite everything from scratch in a newer technology to have it maintainable for years to come. I thought I'd have to offer more content, more articles, more whatever. It was madness. And nonsense.

Anyways, your point is not totally invalid. I did a mistake when I changed from free to paid. I started with 99&cent; per month. The goal was to convert as many users as possible. This worked well. Only about 10-20% stopped using my service. However, it made future price increases harder.

There is not much to improve. In the space I'm operating in, the software, as it is, can be considered more or less complete. I mean, I can always add features, but the core mechanics, the way it works, the way it delivers value and the value people expect from the software is more or less complete. There's not much to add that improves the perceived value for people in such a way that they'd be willing to pay more for it.

But the fundamental idea is right: There must be some additional thing that's good for people, that people want. There's also related products, added benefit, whatnot. I won't do Gold membership, but I might be able to sell different things that complete the offering for some people.

Compare this to Microsoft Office. Word is for writing stuff. It's more or less feature-complete. Sure, you could add some stuff, but most people are happy with how it is in its basic edition. So Microsoft offers also Excel and PowerPoint. They are for very similar people, they complete the offering, but they're totally different. A "Word Gold" wouldn't make any sense.

> **For the owners of a small bootstrapped company, pretty much anything that doesn't increase revenue is a distraction and should be delegated to someone else, or some other company.**

* You could set up and configure an email server to send email to your users, or just use a service with an API like Mailgun or Mailchimp.

* You could write your own Wordpress theme and plugins from scratch, or just pay \$10 for a slick theme and \$50 for a few plugins that meet all your needs.

* You could spend time each week backing up your system with a set of external drives, or just pay a monthly fee to Crashplan or Dropbox.

* You could buy a discount VPS account and spend all your time keeping it patched and running, or just pay for a fully managed server until you outgrow it.

# [Pinboard Creator Maciej Ceglowski Talks About Why Boring Architecture is Good](http://readwrite.com/2011/02/10/pinboard-creator-maciej-ceglow)

There is absolutely nothing interesting about the Pinboard architecture or implementation,
**I consider that a feature.**

> **I believe that relying on very basic and well-understood technologies at the architectural level forces you to save all your cleverness and new ideas for the actual app, where it can make a difference to users.**

I think many developers (myself included) are easily seduced by new technology and are willing to burn a lot of time rigging it together just for the joy of tinkering. So nowadays we see a lot of fairly uninteresting web apps with very technically sweet implementations. In designing Pinboard, **I tried to steer clear of this temptation by picking very familiar, vanilla tools wherever possible so I would have no excuse for architectural wank.**

The other reason I like the approach is that the tried-and-true stuff is extensively debugged and documented. The chances of you finding a bug in MySQL or PHP as the author of a mid-sized website are microscopic. That's not the case for newer infrastructure like NoSQL or the various web frameworks.

* Use a RDBMS and take the time to learn it very thoroughly. High-Performance MySQL (the O'Reilly Book) and the Percona blog are indispensible for MySQL; I'm sure similar resources exist for Postgres.

* Use dedicated (not virtualized) hardware. I/O can be awful on virtualized servers and debugging I/O slowness there is next to impossible.

* Use caching as a last resort, not as a fundamental design strategy. It's 2011 - unless you have millions of users, your app should be able to run fast with all caches disabled.

* Use frameworks for prototyping, but build from the ground up once you know what you're building.

* Resist excessive abstraction

* Set performance targets for yourself. For example, one goal for Pinboard is a median page load time of under 300ms. This will force you to instrument well and optimize appropriately.

One interesting quirk of Pinboard is a complete absence of unit tests. I used to be a die-hard believer in testing, but in Pinboard tried a different approach, as an experiment. Instead of writing tests I try to be extremely careful in coding, and keep the code size small so I continue to understand it.

I've found my defect rate to be pretty comparable to earlier projects that included extensive test suites and fixtures, but I am much more productive on Pinboard.

# [Startup Playbook - Mike Subelsky](https://github.com/subelsky/startup_playbook)

* most interesting/urgent problems are B2B
 
* fun beats sexy
 
* hard/tedious == opportunity

* buyer == user

* main goal is to get feedback (worth the risk? worth the cost?)

* [The Lean Startup](http://theleanstartup.com/principles)

* [customer development](http://www.startuplessonslearned.com/2008/11/what-is-customer-development.html)

* [manual first](http://viniciusvacanti.com/2013/05/07/the-manual-first-startup/)

* [MVP](http://steveblank.com/2013/07/22/an-mvp-is-not-a-cheaper-product-its-about-smart-learning/) - the 80% solution

* don't launch, avoid press releases

* don't make people sign NDAs

* modular from the beginning (Rails engines, private gems - gemfury, open source as much as posibble, lay the groundwork for SOA)

* Rails Admin

* 0 defects, fix all bugs, root-cause analysis, BDD/TDD

* deploy continuously (avoid staging &amp; QA)

* document continuously (readme-driven-development)

* treat the db as a fortress

* don't build a JS front-end (?!?)

* don't run servers (Heroku, Papertrail, Mailgun etc.)

# [57 startup lessons](http://www.defmacro.org/2013/07/23/startup-lessons.html)

Slava Akhmechet, RethinkDB co-founder.

# [Idea Gardening: A Primer](http://davetroy.com/posts/idea-gardening-a-primer)

In Greek mythology, Sisyphus was sentenced to push a boulder up a hill, only to watch it roll back down so he could push it again for all of eternity. This is a decidedly bad gig. Some businesses are like this too, and too many well-meaning, intelligent entrepreneurs spend time on these kind of Sisyphean enterprises - consuming a lot of precious resources and never getting traction.

In business, there are rocks to push sometimes: hard work is always part of doing something worthwhile. But successful businesses always reach the summit and get to watch the rock roll down the other side.

So the advice to all entrepreneurs thus becomes essentially the same: **pick ideas that will work in the marketplace and expend resources only on those ideas**.

> **As a general rule of thumb, assume you know a lot less than you think you do about what ideas will work and what ideas won't, because you're likely wrong.**

Start thinking instead about what you want your idea garden to look
like:

* What ideas motivate you, and fill you with a sense of childlike wonder?

* What ideas give you inner peace and create a sense of aesthetic fulfillment?
 
* What higher causes do you aspire to?

* What causes do you think you can motivate others to rally around?

Today, we have tools to test the resonance of ideas with fairly wide audiences - for free. Twitter, Facebook, the web, and other mechanisms allow us to expand our networks to find people to bounce our ideas off of. Start bouncing your ideas - **the ideas you're most passionate about** - off of a wider audience. Put out feelers. See what sticks.

You may worry that sharing an idea with people will _let it out of the bag_ and someone else will _steal it_. You're not so smart to have come up with an idea that no one else has thought of before - believe that.

> **What you must have that is unique and irreplaceable is the vision, passion, and relationships required to bring your idea to fruition.**

But the idea itself - the raw two or three sentences that define your concept - has very little potential by itself. By sharing your idea with others, you can strengthen it. Others can contribute to it, pointing out the places where it's weak, and repurposing it in ways you never imagined. Don't be afraid to share your ideas, in whole or in part, so that others can help you bring them to fruition.

Gardening takes time - time for sunlight, for seed, for rain to converge in fecundity. The same is true of Idea Gardening. **Patience is required for ideas, people, and resources to converge in a way that releases stored energy.** If you're having to use too much pesticide (lawyering) or fertilizer (cash) to make your idea work, you're likely going against the forces of nature, and not taking advantage of the energy of the marketplace.

Don't overextend yourself by sinking resources into the first idea you have that looks to be viable. As a committed gardener, you will have many sprouts and leads that are viable. **Put your attention to the ideas that seem to be the strongest, and use all of your available resources to drive multiple ideas forward in parallel.** Otherwise you'll have the kind of fragile and brittle all-beet monoculture that will have a hard time surviving market conditions.

People are suckers for sunk costs; this is the instinct that makes folks want to double down in Vegas to recover their losses. But losses are losses, whether measured in time or in money, and **chasing after a failed idea to recover yourself to some perceived baseline is a mistake.**

Clearly the only rational thing to do is to stop burning money and move on to something that will work. It's not your fault it didn't work out - the market didn't want what you were selling. So, without pride or prejudice, stop the bleeding and move on. It may be your idea is still viable, but it might be viable for someone else, someplace else, at some different point in time. Put it on ice and return to it then.

The key to avoiding the trap of sunk costs in the Idea Gardening model is to minimize costs until an idea looks to be viable. If you have 10 ideas you're experimenting with, and 4 show promise, what can you do to put a minimal amount of investment in only those four that will advance them to a stage where you can learn more about their prospects?

For the software developers out there, this is the agile approach to business. **Start small, iterate, and follow the market need. This also means that failures, when they occur, happen quickly.** And that is the best thing any entrepreneur could hope for.

An entrepreneur who is also a software developer is uniquely positioned to try out dozens of ideas and let the market decide which ones will work.

Edison put himself into the Idea Gardening business. His labs in Menlo Park, New Jersey were a virtual playground for engineers. They generated more than 1,500 patents and went on to form General Electric. He was famously quoted as saying that **Genius is 1% inspiration and 99% perspiration**, but this is often misunderstood.

Edison wasn't saying that entrepreneurship was driven by **hard work** of the Sisyphean kind - no one can sustain that kind of load and be that prolific. Rather, he was suggesting that the hard work of invention lay in hoeing the rows and planting countless seeds of innovation so that, in time, the best ideas could bear fruit and thereby transform the world.

So entrepreneurs, plant your gardens. Give them sun, water, and time. The rest will follow, and you, too, will go on to transform the world.

# [The Code is your Enemy](http://blog.asmartbear.com/the-code-is-your-enemy.html)

Most startups fail because not enough people show up on the home page, or people show up but they don't try the product, or they don't pay for the product, or it's too expensive to get them to show up, or they don't tell their friends to come along, or because it's not solving a pain that people have, or it's not solving a pain that people know they have, or it's too hard to explain the pain, or a bunch of other things that are not whether the code works or whether it looks good.

> **Customers don't open their wallets based on your unit test coverage or whether you used Bodoni instead of Times New Roman on the home page.**
Those things are actually not the most important things.

1. Have you talked to 50 potential customers? By that I mean fifty, not
   a dozen. Watch out for bias in the first 10.

2. **Are people coming to your website every day?** If not, solving that is much harder and much more outside your control than building software.

Consider: Would you rather get hired as the CTO of a company with 1k daily new, unique, qualified visitors with no product, or the CTO of a company with a stable product and 10 uniques visits to the home page? You know you can solve for the first case; who knows about the second? But if you stay nose-down in the code instead of working on getting attention, that's exactly the company you're building.

So force yourself out of your comfort zone. You'll also do coding and design and that's fine of course, but force yourself to mostly do those other things that create a valuable business.

# [Staying fast and good when making enterprise software](http://www.subelsky.com/2013/11/staying-fast-and-good-when-making.html)

Code you wrote today doesn't end up getting in front of a user until next week, and you've already forgotten the context to explain why you wrote that code the way you did. When there's a bug, you have to start from scratch when figuring out the problem. Feedback gets delayed, if you get it at all.

#### Work in small batches

Smaller changes are less likely to cause a disruption, and are easier to pinpoint when they do cause problems.

#### Don't take shortcuts

Skipping tests and hacking things together ultimately add unacceptable risk and technical debt.

#### Practice five whys/root cause analysis

Don't make the same mistake twice; gradually develop a cluster immune system

#### Expect 100% unit test code coverage

Backed by appropriate integration and feature tests: catch problems as early as possible

#### Setup a continuous integration server

We have too many modules for any one developer to test all of them at once, so this server will become a backstop, alerting us when there is a problem in one of our modules.

#### Use Feature flags

While we are testing new features, hide them from users until they are fully baked.

# [Case Study: Continuous deployment makes releases non-events](http://www.startuplessonslearned.com/2010/01/case-study-continuous-deployment-makes.html)

Continuous Deployment is Continuous Flow applied to software. The goal of both is to eliminate waste. The biggest waste in manufacturing is created from having to transport products from one place to another.

> **The biggest waste in software is created from waiting for software as it moves from one state to another: waiting to code, waiting to test, waiting to deploy.**
Reducing or eliminating these waits leads to faster iterations which is the key to success.

Prior to adopting continuous deployment, I used to release software on a weekly schedule (come rain or shine) which I viewed as pretty agile, disciplined, and aggressive. I identified the must-have code updates on Monday, official code cutoff was on Thursday, and Friday was slated for the big release event. The release process took at least half a day and sometimes the whole day. Dedicating up to 20% of the week on releasing software is incredibly wasteful for a small team.

> **The fundamental challenge with Continuous Deployment is getting comfortable with releasing all the time.**

I took things easy at first - made small changes and audited the release process maniacally. I started relying heavily on functional tests (over unit tests) which allowed me to test changes as a user would. I also identified a set of events that would indicate something terribly going wrong (e.g. no users on the system) and built real-time alerting around them (using nagios/ganglia). As we built confidence, we started committing bigger and multi-part changes, each time building up our suite of testing and monitoring scripts. After a few iterations, our fear level was actually lower than how we used to feel after a staged release. Because we were committing less code per release, we could correlate issues to a release with certainty.

Smaller releases lead to faster build/measure/learn loops. I've used these faster build/measure/learn loops to optimize my User Activation flow, delight customers with "near-instant" fixes to issues, and even eliminate features that no one was using.

**DON'T keep adding features until you've validated the MVP**, or more specifically the unique value proposition of the MVP. Unneeded features are waste and not only create more work but can needlessly complicate the product and prolong the "customer validation" phase.

> **Build in response to a signal from the "customer", otherwise rest or improve.**

As a technologist, I too love to measure progress based on how much stuff I build. But instead of channeling all my energy towards building new features, I channel roughly **80% of it towards measuring and optimizing existing features**. I am not advocating adding no features at all. Users will naturally ask for more stuff and your MVP by definition is minimal and needs more love. Just don't push it.

I've previously described my 2 hour blocks of maker time for maximizing my "work flow". Prior to starting any maker activity, I clearly identify what needs to get done (the goal) and sketch out how it needs to get done (the design).

> **Prefer functional tests over unit tests whenever possible**

I deem excessive unit testing a form of waste. Whenever possible, I rely on functional tests that verify user actions.

# [What is customer development?](http://www.startuplessonslearned.com/2008/11/what-is-customer-development.html)

> **Get out of the building**

Very few startups fail for lack of technology. They almost always fail for lack of customers. I've been guilty of this many times in my career - it's just so easy to focus on product and technology instead.

#### In a startup no facts exist inside the building, only opinions

Most likely, your business plan is loaded with opinions and guesses, sprinkled with a dash of vision and hope.

Surprisingly early, you can start to get a sense for who the customer of your product might be, how you'll reach them, and what they will ultimately need. Customer development is an attempt to minimize the risk of total failure by checking your theories against reality.

#### Theory of market types

There are three fundamental situations that change what your company needs to do:

* creating a new market

* bringing a new product to an existing market

* resegmenting an existing market

#### Finding a market for the product as specified

Our goal in product development is to find the minimum feature set required to get early customers.

#### Phases of product &amp; company growth

* Customer Discovery (when you're just trying to figure out if there are any customers who might want your product)

* Customer Validation (when you make your first revenue by selling your early product)

* Customer Creation (akin to a traditional startup launch, only with strategy involved)

* Company Building (where you gear up to Cross the Chasm)

#### Learning and iterating vs. linear execution

In the beginning, startups are focused on figuring out which way is up. They really don't have a clue what they should be doing, and everything is guesses.

Startups need time spent in a mindset of learning and iterating, before they try to launch. During that time, they can collect facts and change direction in private, without dramatic and public embarrassment for their founders and investors.

# [The Lean Startup](http://theleanstartup.com/principles)

Too many startups begin with an idea for a product that they think people want. They then spend months, sometimes years, perfecting that product without ever showing the product, even in a very rudimentary form, to the prospective customer. When they fail to reach broad uptake from customers, it is often because they never spoke to prospective customers and determined whether or not the product was interesting. When customers ultimately communicate, through their indifference, that they don't care about the idea, the startup fails.

The lack of a tailored management process has led many a start-up to _a human institution designed to create a new product or service under conditions of extreme uncertainty_, to abandon all process. They take a _just do it_ approach that avoids all forms of management. But this is not the only option. Using the Lean Startup approach, companies can create order not chaos by providing tools to test a vision continuously.
Lean isn't simply about spending less money. Lean isn't just about failing fast, failing cheap. It is about putting a process, a methodology around the development of a product.

The Lean Startup methodology has as a premise that every startup is a grand experiment that attempts to answer a question. The question is not _Can this product be built?_ Instead, the questions are **Should this product be built?** and **Can we build a sustainable business around this set of products and services?** This experiment is more than just theoretical inquiry; it is a first product.

A core component of Lean Startup methodology is the build-measure-learn feedback loop. The first step is figuring out the problem that needs to be solved and then developing a minimum viable product (MVP) to begin the process of learning as quickly as possible. Once the MVP is established, a startup can work on tuning the engine. This will involve
measurement and learning and must include actionable metrics that can demonstrate cause and effect question.

Progress in manufacturing is measured by the production of high quality goods. The unit of progress for Lean Startups is validated learning - a rigorous method for demonstrating progress when one is embedded in the soil of extreme uncertainty.


# [How To Make It as a First-Time Entrepreneur](http://viniciusvacanti.com/2013/05/07/the-manual-first-startup/)

So, we took a shortcut. Instead of building a crawler, my co-founder and I would crawl out of bed at 3 am and manually enter the deals into our database. Plus, when you’re doing it yourself, classification was easy. **We did it all manually.**

The funny thing is that within a few months of our launch, several competitors emerged and they all had crawlers. But, from our users’s perspective, we were more advanced since we had categorization which was definitely no easily automated task.

We didn’t actually build a real crawler for the first 9 months and just kept scaling manually by hiring more data entry professionals. Instead, we were able to focus our resources on improving the product and user acquisition.

It's now clear to me that not building that crawling technology early on was one of the reasons our startup succeeded.

Taking this "manual-first" approach was our secret sauce.

ZeroCater, a Y Combinator company, started with just a big spreadsheet trying to connect companies with restaurants that would cater.

Groupon started with just a WordPress blog and manually sending PDFs with the first vouchers.

Fastest way to get to the **moment of truth**. Having your potential customer evaluate your product and see if it addresses their need is the moment every founder is trying to get to and doing things manually allow you to quickly get there.

Easy to change your solution if it doesn't work. There's no code to re-write, there's no sunk cost. You just have to change how you're manually doing something.

Will really understand what to automate with tech when you've been manually doing it. When you've been manually providing the solution, you'll know exactly where the pain points are that you should be automating.

Can really wow your potential customers. When you do things manually, you can try different things that really wow the customer and see which ones are worth trying to scale.

> Customer doesn't know how your product works behind the scenes. They won't judge you for your manual approach because they don't know that's how you're doing it. All they will care about it is that your product works.

Your product will **just work**. Because you're manually providing the solution, the product will just work. When trying to implement a solution with technology, it can be very hard to make sure that it just works.

Helps you focus your time on the problem, not the solution. It's very tempting to fall in love with the technology behind your solution only to painfully realize that the problem you set out to solve isn't a real problem.

Reid Hoffman, founder of LinkedIn, once said: **If you're not embarrassed by your first release, you probably spent too much time on it.**

> **If people don't laugh at how you first implemented your product, you probably spent too much time on it.**

# [An MVP is not a Cheaper Product, it's about Smart Learning](http://steveblank.com/2013/07/22/an-mvp-is-not-a-cheaper-product-its-about-smart-learning/)

They concluded that the only way to get a delighted early customer was to build an MVP. They believed that the MVP needed to:

1. demonstrate a drone flight
2. make sure their software could stitch together all the images of a field
3. present the data to the farmer in a way he could use it

The team confused the goal of the MVP (seeing if they could find a delighted farmer who would pay for the data), with the process of getting to the goal. They had the right goal but the wrong MVP to test it.

Since the startup defined itself as a data services company, at the end of the day, the farmer couldn't care less whether the data came from satellites, airplanes, drones, or magic as long as they had timely information.

What they needed to spend their time is first testing is whether farmers cared about the data.

Would it be cheaper to rent a camera and plane or helicopter, and fly over the farmers field, hand process the data and see if that's the information farmers would pay for?  Couldn't you do that in a day or two, for a tenth of the money you're looking for?

A minimum viable product is not always a smaller/cheaper version of your final product. Think about cheap hacks to test the goal. Great founders keep their eye on the prize.

# [Nine Webcasts to Learn From](http://www.startuplessonslearned.com/2013/11/nine-webcasts-to-learn-from.html)

#### Kent Beck

When I was younger I was convinced that programming was the most fun thing I would ever do and I'd be very happy to program increasingly large systems myself. And I think basically what happened was I kept doing that, and not having the impact I wanted to have. Because in my fantasy I could produce a massive program that's used by billions of
people and has enormous complexity and is incredibly innovative, by myself. Just, you know, with my bare hands.

> **The truth of any program is, it requires teams, and customers, and it's this complicated ecosystem...**

So the person who's considered the _founder_ or the person who created the complicated system, it doesn't matter if it's Linux or Facebook or anything, **somebody had to plant that initial seed**, and that's very satisfying.

> **In order for us to remember it and to care about the fact that they are the founder of that thing, they had to do an incredible amount of management of people to get them to grow that seed into something that is significant**.

And what's frustrating to me - it was then and it still is - is that as soon as I became a manager and a team leader and an architect and really thinking out how to do that stuff, **I was doing human systems engineering and I was no longer making things with my bare hands**. And so I've also had that frustration. Now, that's frustrating but also very satisfying, in that I'm very proud of the things that teams that I've worked with have built. But for me anyway, that transition from being a team leader to whatever it is that I do now, to try to cultivate this community and try to share these ideas on a wider scale - that was actually a much easier transition than going from an individual contributor to a team leader. Because to me, it's like, as soon as I was not making things myself, with my bare hands, it's all about, ok, then what activities will give me the greatest influence to have the impact I want to see in the world?

> **It's a fundamental waste of human energy and talent to have people working on things that nobody wants and that have no impact. That's actually morally wrong to have a system that does that.**

# [How to Build a Subscription Service on Rails: A Noob's Guide](http://www.joelhooks.com/blog/2013/10/30/how-to-build-a-subscription-service-on-rails-a-noobs-guide)

> It is all about finding pain and providing value to people in exchange for money.

It is also astonishingly easy to deploy Rails apps to Heroku. For production, Heroku quickly becomes not free as you add in background workers, SSL, and other essential pieces, but you can go a long way with free.

Stripe through [Koudoku gem](https://github.com/andrewculver/koudoku)

# [How to Stub External Services in Tests](http://robots.thoughtbot.com/how-to-stub-external-services-in-tests)

When integrating with external services we want to make sure our test suite isn't hitting any 3rd party services. Our tests should run in isolation.

```
require 'webmock/rspec'
WebMock.disable_net_connect!(allow_localhost: true)
# ...
stub_request(:get, /api.github.com/).with(headers: {}).to_return(status: 200, body: "", headers: {})
```

Use WebMock to route all requests to our Sinatra app:

```
stub_request(:any, /api.github.com/).to_rack(FakeGitHub)

require 'sinatra/base'

class FakeGitHub < Sinatra::Base
  get '/repos/:organization/:project/contributors' do
    json_response 200, 'contributors.json'
  end

  private

  def json_response(response_code, file_name)
  end
end
```

# [Constant Resolution in Ruby](http://valve.github.io/blog/2013/10/26/constant-resolution-in-ruby)

Ruby constant is anything that starts with a capital.

`Module.nesting` returns an array of searcheable lexical scopes, starting from current.

When Ruby has finished searching the constants up the nesting and ancestors chain and didn't find it, it gives the calling code the last chance by calling the `const_missing` method.

When Ruby finds a constant with a given name, it stops looking further.

# [LEGOs, Play-Doh, and Programming](http://weblog.jamisbuck.org/2008/11/9/legos-play-doh-and-programming)

In order to master LEGO brick building, you have to know all of the pieces available to you, and have a good intuitive feel for how and when they should be used. That's a lot of information to keep tabs on.

Where LEGO models require significant work to alter or extend, Play-Doh models are dead-simple. If you want to add something to the base of your model, just graft more Play-Doh onto it. Want to change the shape of the keystone of your arch? Just pinch and mold in place.

Ruby's philosophy is like that of Play-Doh's: provide a basic set of tools and make it relatively simple to build something complex with them. The very Ruby language itself is designed for this: closures, super-simple introspection of objects, runtime modification of existing objects, and the use of modules for extending classes and objects all tend to result in an environment that is simple, malleable, and extensible.

> **Always, always, always build just what you need, and only when you need it.**
You're in Ruby, the Play-Doh of programming languages, and the cost of adding features later is really, really low.

Expose only what you need. The rest can be there, available, but not formally exposed. Only when (and if) you discover a need to expose more, should you expose more.

* A smaller API is easier to describe, document, and support.
* A smaller API is easier for people to learn.
* A smaller API is easier for you to test.
* Extending a small API is much less onerous on your users than changing or restricting a larger API.

Now, separation of concerns and modularity are good things, when used in moderation. But like any design pattern, it becomes evil when taken to extremes. Too much modularity and you wind up with component soup (and I hope you're hungry, because you're going to have lots of it).

> With lots of tiny components, the interactions between those components can become
difficult to test.

> **When I added dependency injection to the mix, it became very, very difficult to follow the flow of the program, and to understand the dependencies.**

No component was too small! No object too insignificant! I was on the dependency injection horse, and riding it for all it was worth.

**Loose coupling** and **high cohesion** are terms you'll hear bandied about in defense of dependency injection, and those traits are certainly desirable. **But strike a balance with pragmatism.** There will be some who call me heretic for saying this, but don't be afraid to introduce tighter coupling when it makes sense. Loose coupling everywhere is what I had with `Net::SSH 1.x`, and the result was nearly unmaintainable.

> **Be wise. You're competent. Trust your instincts.**

**Just in time. Not just in case.** Don't play _what if_ games when you're coding. Practice discipline, and implement only what you need, when you need it. You'll wind up with tighter, more testable code that is easier to maintain in the long run.

# [Break Apart Rails Monoliths Using This 1 Weird Trick](https://www.bluebox.net/insight/blog-article/break-apart-rails-monoliths-using-this-1-weird-trick)

No business logic in models, views or controllers, put them in POROs.

Giant classes - difficult to understand, feels like luck that anything works.

Let the business object orchestrate the message passing between model objects.

# [Introducing Mutations: Putting SOA on Rails for security and maintainability](https://developer.uservoice.com/blog/2013/02/27/introducing-mutations-putting-soa-on-rails-for-security-and-maintainability/) {#mutations}

[mutations](https://github.com/cypriss/mutations) - compose your business logic into commands that sanitize and validate input.

Problems:

* different user interfaces with different controllers serving them

* invalid input polluting exception logs

* extremely fat models with callbacks that feed into each other

#### Service Layer Patter

Each service object is essentially a module of code that has one primary method and one responsibility, eg `TwitterPoster.post(tweet, user)`

We've called these service objects **Mutations**, as quite often they represent a change of state in our database from state A to state B.

Removed model callbacks and put them into service objects. Each mutation operates in a single context, there's little conditional logic.

Mutations let you validate and specify inputs on your service objects.

By building an internal API (service layer) that your controllers can use, you'll have less code duplication if you have multiple controllers that operate on the same types of data.

If you specify that a parameter must be a string, then inside the execute method, it is guaranteed to be a string. If an ignorant or malicious user passes in an array, then an error will be raised. If the parameter can be safely coerced to a string (eg an integer or boolean is passed; `3` becomes "3" and `false` becomes "false"), then the coercion is done and you have a string. This makes it much easier to write correct code, and correct code is much more likely to be secure.

With a Mutation, you have a single business rule that you're coding against. You're likely to have a short, linear method.

# [7 Patterns to Refactor Fat ActiveRecord Models](http://blog.codeclimate.com/blog/2012/10/17/7-ways-to-decompose-fat-activerecord-models/)

ActiveRecord classes handle persistence, associations and not much else.

Refactor to break them down and spread out the logic evenly. Repeat this process and you'll end up with a set of simple objects with well defined interfaces working together in a veritable symphony.

> **Any application with an `app/concerns` directory is concerning.**

**Prefer composition to inheritance.** Using mixins like this is akin to _cleaning_ a messy room by dumping the clutter into six separate junk drawers and slamming them shut. Sure, it looks cleaner at the surface, but the junk drawers actually make it harder to identify and implement the decompositions and extractions necessary to clarify the domain model.

**Value Objects** (think Domain-Driven Design) are simple objects whose equality is dependent on their value rather than an identity. They are usually immutable. Date, URI, and Pathname are examples from Ruby's standard library.

Some actions in a system warrant a **Service Object** to encapsulate their operation. I reach for Service Objects when an action meets one or more of these criteria:

* it is complex (eg closing the books at the end of an accounting period)

* it reaches across multiple models (eg an e-commerce purchase using Order, CreditCard and Customer objects)

* it interacts with an external service (eg posting to social networks)

* it is not a core concern of the underlying model (eg sweeping up outdated data after a certain time period)

* there are multiple ways of performing the action (eg authenticating with an access token or password). This is the Gang of Four Strategy pattern.

When multiple ActiveRecord models might be updated by a single form submission, a Form Object can encapsulate the aggregation. Virtus to the rescue, Form Objects quacking like an ActiveRecord.

Since validation logic is often contextual, it can be defined in the place exactly where it matters instead of needing to guard validations in the ActiveRecord itself.

For complex SQL queries littering the definition of your ActiveRecord subclass (either as scopes or class methods), consider extracting query objects.

If logic is needed purely for display purposes, it does not belong in the model.

Sometimes complex read operations might deserve their own objects. In these cases I reach for a Policy Object. This allows you to keep tangential logic, like which users are considered active for analytics purposes, out of your core domain objects.

I use the term **Service Object** for write operations and **Policy Object** for reads. They are also similar to Query Objects, but **Query Objects** focus on executing SQL to return a result set, whereas Policy Objects operate on domain models already loaded into memory.

Decorators let you layer on functionality to existing operations, and therefore serve a similar purpose to callbacks.

One sign you've added too many responsibilities in callbacks is slow and brittle tests or an urge to stub out side effects in wholly unrelated test cases.

Even in a Rails application, there are many tools to manage complexity in the model layer. None of them require you to throw out Rails. ActiveRecord is a fantastic library, but any pattern breaks down if you depend on it exclusively.

# [Crazy, Heretical, and Awesome: The Way I Write Rails Apps](http://jamesgolick.com/2010/3/14/crazy-heretical-and-awesome-the-way-i-write-rails-apps.html)

As applications grew, test suites would get slow - like minutes slow. When you're depending on your persistence objects to do all of the work, your unit tests absolutely must hit the database, and hitting the database is slow. It's a given in the rails world: `big_app == slow_tests`.

But slow tests are bad. Developers are less likely to run them. And when they do, it takes forever, which often turns in to checking twitter, reading reddit, or a coffee break, harming productivity.

Also, coupling all of your business logic to your persistence objects can have weird side-effects. In our application, when something is created, an after_create callback generates an entry in the logs, which are used to produce the activity feed. What if I want to create an object without logging - say, in the console? I can't. Saving and logging are married forever and for all eternity.

When we deploy new features to production, we roll them out selectively. To achieve this, both versions of the code have to co-exist in the application. At some level, there's a conditional that sends the user down one code path or the other. Since both versions of the code typically use the same tables in the database, the persistence objects have to be flexible enough to work in either situation.

To decouple the logging from the creation of the database record, we're going to use something called a service object. A service object is typically used to coordinate two or more objects; usually, the service object doesn't have any logic of its own (simplified definition). We're also going to use Dependency Injection so that we can mock everything out and make our tests awesomely fast (seconds not minutes).

Have you ever been in a situation where a model wouldn't save because a callback was mistakenly returning nil? Debugging (necessarily) opaque callback mechanisms is hard.

You might even write a few percent more lines of actual code. But you'll wind up with more maintainability for it (not to mention faster tests, code that's easier to understand, etc).

The truth is that in a simple application, obese persistence objects might never hurt. It's when things get a little more complicated than CRUD operations that these things start to pile up and become pain points. That's why so many rails plugins seem to get you 80% of the way there, like immediately, but then wind up taking forever to get that extra 20%.

Ever wondered why it seems impossible to write a really good state machine plugin - or why file uploads always seem to hurt eventually, even with something like paperclip? It's because these things don't belong coupled to persistence. The kinds of functionality that are typically jammed in to active record callbacks simply do not belong there.

# [Where's Your Business Logic?](http://collectiveidea.com/blog/archives/2012/06/28/wheres-your-business-logic/)

The fundamental problem with almost every Rails project (and I'm sure in other frameworks as well), is that there is no direct codifying of the business rules and use cases of the application. There is no single location you can point to and say _here, these objects implement our use cases_.

First, we rarely spend enough time up front thinking about the problem space and designing a solution, and second, we haven't been listening and reacting to test pain.

#### How do you mock behaviour provided by other "Interactor" classes?

I'm taking the stance of never mocking objects I own. I know this goes against the "isolate and test" idea, but I've been bit pretty hard by code with too many mocks, and mocking done wrong, so I don't mock them. As this project progresses I'll see if this idea is manageable or if interface mocking is really needed for keeping tests clean.

#### Aren't you simply describing the Command pattern?

This pattern is focused on being an implementation of a single application use-case, which is IMO higher level than a set of Commands that act on your domain models. The other distinction I see is that where Commands are supposed to be given everything they need to act, Interactors are supposed to be the top level access to your application.

There is a very decent implementation of the [Repository pattern](https://github.com/playlouder/persistence). Tim Cowlishaw and Tom Stuart (among others) have been working on it in 2011 (part of PlayLouder it seems).

Robert Martin has called Interactions **Transactions** in his books, where they're indeed an implementation of Command pattern. What distinguish your Interactors from Commands (or Transactions) is lack of consistent interface: method `run` takes different number of arguments. In comparison, Martin provides all required information for Transactions in constructors and setter methods. I see no reason not to provide consistent interface.

It looks to me like this is already something Eric Evans has described in Domain Driven Design with Domain Service objects (which have to be distinguished from Application Service objects.)

> **Services in DDD conflict with Martin's Transactions**
>
> **DDD tells**: Services should not have their state
> **Martin tells**: Transactions are just Command objects (thus they have state).

_Feels like low-ceremony DCI_

_Nice ideas, though your interactor class is really just a function forced into an object oriented perspective_

#### [guru_watch](https://github.com/qertoip/guru_watch)

App using Rails for the front-end and ActiveRecord for the back-end that demonstrates the Use Case Driven Approach aka Entity-Controller-Boundary.

> **Personal observations**
>
> The app is too simple, it has a single model.
>
> Almost all "use-cases" are actually persistence-related and do nothing more than spread the model and the controller concerns into command objects.
>
> The ActiveRecord back-end feels like its exposing the internals of the library which are abstracted for a reason.
>
> ActiveMemory doesn't look like a real thing.
>
> Associations? Callbacks? Guards/filters?

[See the mutations gem](#mutations) (it combines the command object with sanitizing &amp; validations).

# [Monolithic Node.js](http://www.richardrodger.com/monolithic-nodejs)

The defining attribute of most large-scale, mainstream traditional systems is that they are monolithic. That is, a single large codebase, with many files, thousands of classes, and innumerable configuration files (in XML, if you’re lucky). 

> **Monolith: a system that cannot survive the loss of any of its parts.** You pull one part out, and the whole thing fails. Each part is connected to the others, and interdependent on them.

The term monolith means single stone, and is derived from the ancient greek. The ancient city of Petra in modern-day Jordan is one of the best examples of monolithic architecture. Its buildings are constructed in one piece, hewn directly from the cliff face of a rocky mountain. It also provides a perfect example of the failure mode of monolithic systems. In 363AD an earthquake damaged many of the buildings, and the complex system of aqueducts. As these were carved directly into the mountain, they were impossible to repair, and the city fell into terminal decline.

The other effect of monolithic software is more pernicious. It generates software development processes and methodologies. Because the system has so many dependencies, you must be very careful how you let developers change it. A lot of effort must be expended to prevent breakage. Our current approaches, from waterfall to agile, serve simply to enable monolithic systems. They enable us to build bigger and add more complexity. Even unit testing is an enabler. You thought unit testing was the good guy? It’s not. If you do it properly, it just lets you build bigger, not smarter.

> **Modular system: each part stands alone, and the system is still useful when parts are missing.**

Modular software should therefore be composed of components, each, by definition, reusable in many contexts. The idea of reusable software components is one of the Holy Grails of software development.

> **The greatest modular system humanity has created to date is the intermodal shipping container.**

There are countless systems suffering from the Big Ball of Mud anti-pattern, where a few enormous classes contain most of the tangled logic. There are just too many different kinds of thing that can go into an object.

> What journeyman programmer has not launched headlong into a grand architecture, obsessed by the aesthetic of their newly imagined design?

**Our programming languages should let us think at the right level, the level of the problems we are trying to solve.**

Our abstractions are at too low a level, or end up being inappropriate. Our languages do not enable us to easily compose their low level elements into things at the right level. The complexity in doing so trips us up, and we end up with broken, leaky abstractions.

> **Most of the time, when we build things with software, we are trying to model use cases. We are trying to model things that happen in the world. The underlying entities are less important.**

There is an important data point in the observation that beginning programmers write naïve procedural code, and only later learn to create appropriate data structures. This is telling us something about the human mind. We are able to get things done by using our intelligence to accommodate differences in the entities that make up our world.

A bean-bag chair is still a chair. Every human knows how to sit in one. It has no back, and no legs, but you can still perform the use-case: sitting. If you’ve modeled a chair as a object with well-defined properties, such as assuming it has legs, you fail in cases like these.

We know that the code to send an email should not be tightly coupled to the API of the particular email sending service we are using. And yet if you create an abstract email sending API layer, it inevitably breaks when you change the implementation because you can’t anticipate all the variants needed. It’s much better to be able to say, “send this email, here’s everything I’ve got, you figure it out!”

We can use biological cells as an inspiration for building robust scalable systems. Biological cells have a number of interesting properties. They are small and single-purpose. There are many of them. They communicate using messages. Death is expected and natural.

Let’s apply this to our software systems. Instead of building a monolithic 100,000 line codebase, build 100 small services, each 100 lines long. Fred George, (the inventor of programmer anarchy) one of the biggest proponents of this approach, calls these small programs micro-services.

The micro-services approach is a radically different way of building systems. The services each perform a very limited task. This has the nice effect that they are easy to verify. You can eye-ball them.

Micro-services communicate with each other by sending messages. You can send these messages directly over internal HTTP, or use a message queue for more scale. From the perspective of the service, it just deals with whatever messages come it’s way. When you’re building services in Node.js, JSON is the most natural formatting choice. It works well for other languages too.

> They are easy to scale. They offer a much finer grained level of scaling then simply adding more servers running a single system. You just scale the parts you need.

Death becomes relatively trivial. You’re going to have more than one instance of important services running, and restarts are quick. If something strange happens, just die and restart. In fact, you can make your system incredibly robust if you build preprogrammed death into the services, so that they die and restart randomly over time. This prevents the build up of all sorts of corruption.

Teams also scale. It’s much easier to break up the work into services, and know that there will be few dependencies and blockages between team members.

Finally, micro-services let you map your use-cases to independent units of software. They allow you to think in terms of what should happen. This let’s you get beyond the conceptual changes that objects impose.

The greatest benefit that we have seen is the ability to compose and customise services. Software components are only reusable to the extent that they can be reused. Pattern matching lets you do this in a very decoupled way. Since all you care about is transforming the message in some way, you won’t break lower services so long as your transformations are additive.

A good example here is user registration. You might have a basic registration service that saves the user to a database. But then you’ll want to do things like send out a welcome email, configure their settings, verify their credit card, or any number of project-specific pieces of business logic. You don’t extend user registration by inheriting from a base class. You extend by watching out for user registration messages. There is very little scope for breakage.

# [The Principles of OOD](http://butunclebob.com/ArticleS.UncleBob.PrinciplesOfOod)

Whenever we bring up on our screens a nasty batch of tangled legacy code, we are experiencing the results of poor dependency management. Poor dependency managment leads to code that is hard to change, fragile, and non-reusable.

So it goes with monolithic software. Technical debt, the complexity built into the system over time, makes the system impossible to repair or extend at reasonable cost as the environment changes. You end up with things like month-long code freezes in December so that the crucial Christmas shopping season is not affected by unknowable side-effects.

Abbreviation | Name | Description
- | -| -
SRP | The Single Responsibility Principle | A class should have one, and only one, reason to change
OCP | The Open Closed Principle | You should be able to extend a classes behavior, without modifying it
LSP | The Liskov Substitution Principle | Derived classes must be substitutable for their base classes
ISP | The Interface Segregation Principle | Make fine grained interfaces that are client specific
DIP | The Dependency Inversion Principle | Depend on abstractions, not on concretions


#### Package principles

Abbreviation | Name | Description
- | -| -
REP | The Release Reuse Equivalency Principle | The granule of reuse is the granule of release
CCP | The Common Closure Principle | Classes that change together are packaged together
CRP | The Common Reuse Principle | Classes that are used together are packaged together
ADP | The Acyclic Dependencies Principle | The dependency graph of packages must have no cycles
SDP | The Stable Dependencies Principle | Depend in the direction of stability
SAP | The Stable Abstractions Principle | Abstractness increases with stability


# [Learning by Teaching](http://tooky.co.uk/learning-by-teaching/)

Bowman suggests that trainers should try to connect learners to past experiences, use shorter presented segments, and focus on giving learners lots of oppurtunity for concrete practice through exercises and activities.

Another major theme in the book is the idea that trainers should "step aside" and let the learners teach and learn from each other.

As a trainer we aren't there to teach, or to talk at learners. We are trying to facilitate learning. We want to create an environment where the learners are able to discover ideas and try them out for themselves, offering guidance and help when they are stuck. Not only does this make the training more interesting and relevant for the learners, but it makes it more satisfying to teach. You get fast feedback about how the group are doing, and what topics you might need to spend more time on.

#### Single Responsibility Principle = Reason to Change

Class encapsulation, more robust, high cohesion & loose coupling.

#### Open/Closed Principle

Open for extension, closed for modification. `.draw` yourself, I don't know (or even care) how **you** do it.

#### Liskov Substitution Principle

`Formula1Car.new.start` & `LandRover.new.start` behave the same, even if they are of different types.

#### Interface Segregation Principle

Good | Bad
-|-
`Eat.call` | `Person.eat`
`Work.call` | `Person.work`

#### Dependency Inversion Principle

+++ Reduces coupling, increases re-usability (think plugins).
--- Stuck with fixed interface

# [Why I wrote gocraft/web](https://developer.uservoice.com/blog/2013/12/12/why-i-wrote-gocraft-web/)

The `UserRequired` middleware would validate your session and find the appropriate user. If there was no session, it would redirect you to a login secreen. Each time I wanted to have middleware interact with my structs in this way, I'd have to write more reflection.

Traffic is a lightweight framework (akin to Sinatra), but it also requires type assertions.

Both Revel and Traffic are good web frameworks, and I'd recommend them to anyone looking for a framework. But as I get more experience, I'm finding I don't want a framework.

Don't tell me how to do app config. I don't want to use your INI config file format. I want to use TOML, or I want to use environment variables ala 12-factor apps, or I want to use etcd.

Another fallacy of many web frameworks is they assume you only want one application per executable. They assume this by using global state. This prevents you from binding different servers to different interfaces/ports to give you a public web experience and a protected health/monitoring API. Global state sucks.

**They're not controllers; they're contexts**. They don't control anything. They're simply a struct, a piece of data, that is created for you on each request, that lets you set request-specific fields on as it gets passed down your middleware.


# [Vundle, the plug-in manager for Vim](https://github.com/gmarik/vundle)

# [Anti-Pattern: Iteratively Building a Collection](http://robots.thoughtbot.com/iteration-as-an-anti-pattern)

```
def mutually_signed?
  signers.map(&:signed_by?).inject(:&)
end

def mutually_signed_concise?
  signers.all?(&:signed_by?)
end
```

# [Visualizing Garbage Collection in Ruby and Python](http://patshaughnessy.net/2013/10/24/visualizing-garbage-collection-in-ruby-and-python)

The standard version of Ruby, also known as "Matz's Ruby Interpreter" (MRI), uses a GC algorithm similar to the one used by McCarthy’s implementation of Lisp in 1960. For better or worse, Ruby uses a 53 year old algorithm for garbage collection. Just as Lisp did, Ruby creates objects ahead of time and hands them to your code when you allocate new objects or values.

Ruby doesn't immediately clean up old objects my code is no longer using! Working as a Ruby developer is like living in a messy house, with clothes lying on the floor or dirty dishes in the kitchen sink. As a Ruby developer you have to work with unused, garbage objects surrounding you. Ruby uses Mark and Sweep algorithm invented by John McCarthy (worked on the original implementation of Lisp) to clean up this messy house every now and then.

At first glance, Python's GC algorithm seems far superior to Ruby’s: why live in a messy house when you can live in a tidy one? Why does Ruby force your application to stop running periodically each time it cleans up, instead of using Python's algorithm?

Reference counting is difficult to implement. Python has to leave room inside of each object to hold the reference count. There's a minor space penalty for this. But worse, a simple operation such a changing a variable or reference becomes a more complex operation since Python needs to increment one counter, decrement another, and possibly free the object.

It can be slower. Although Python performs GC work smoothly as your application runs (cleaning dirty dishes as soon as you put them in the sink), this isn't necessarily faster. Python is constantly updating the reference count values. And when you stop using a large data structure, such as a list containing many elements, Python might have to free many objects all at once. Decrementing reference counts can be a complex, recursive process.

Reference counting can't handle cyclic data structures – data structures that contain circular references.

# [How does Bundler do its magic!?](http://kerdany.wordpress.com/2013/11/23/how-does-bundler-do-its-magic)

In Ruby1.9+ RubyGems is included in RubyCore, and is enabled by default, and there's no need to `require 'rubygems'`, it works out of the box.

<pre>
ri Kernel#require

When you call require 'x', this is what happens:
* If the file can be loaded from the existing Ruby loadpath, it is.
* Otherwise, installed gems are searched for a file that matches. If it's
  found in gem 'y', that gem is activated (added to the loadpath).

The normal require functionality of returning false if that file has already
been loaded is preserved.
</pre>

After Bundler resolves the exact gem versions that we want to be activated/enabled, `Bundler.setup` (or `require 'bundler/setup'`) simply collects the paths to these gem versions' lib/ directories, and prepends them all to the `$LOAD_PATH` global array, where any subsequent call to a `require '<file>'` would first start looking into the lib/ directories enabled by Bundler.

Any subsequent call to `require 'file'`, will thus simply find the needed file in the `$LOAD_PATH`, since it's now ready with the correct gem versions. There will be no need to search for new directories to add to the load path, and if the &lt;file&gt; wasn’t found in the `$LOAD_PATH` prepared by Bundler, a LoadError will be raised.

In order to prevent `Kernel#require` from looking further in the other installed gems, Bundler also disables the patched `Kernel#require` by reverting to the original implementation.

`Bundler.require` on the other hand, is just a convenience method that simply auto-requires all the code in your bundler, without you having to make lots of calls to `require`. `Bundler.require` also implicitly calls `Bundler.setup`, so you don't have to call `Bundler.setup` before calling `Bundler.require`.

`bundle exec` adds `-rbundler/setup` ruby command-line option (equivalent to `require 'bundler/setup'` and `Bundler.setup`) to the `$RUBYOPT` variable, so that when it executes any ruby &lt;command&gt; in a sub shell/process, Bundler will do its magic (as explained above). It invokes command through `Kernel#exec`.

# [How to Optimize Unicorn Workers in a Ruby on Rails App](https://www.digitalocean.com/community/articles/how-to-optimize-unicorn-workers-in-a-ruby-on-rails-app)

Unicorn uses forked processes to achieve concurrency. Since forked processes are essentially copies of each other, this means that the Rails application need not be thread safe.

Unicorn gives our Rails apps concurrency even when they are not thread safe. However, this comes at a cost. Rails apps running on Unicorn tend to consume much more memory.

When a child process is forked, it is the exact same copy as the parent process. However, the actual physical memory copied need not be made. Since they are exact copies, both child and parent processes can share the same physical memory. Only when a write is made - then we copy the child process into physical memory.

When the garbage collector of Ruby 1.9 kicks in, a write would have been made, thus rendering CoW useless.

`preload_app` should be set to true so that the start up time of the Unicorn worker processes is reduced. This uses CoW to preload the application before forking other worker processes. However, there is a big gotcha. We must take special care that any sockets (such as database connections) are properly reopened:

```
after_fork do |server, worker|
  if defined?(ActiveRecord::Base)
    ActiveRecord::Base.establish_connection
  end

  if defined?(Resque)
    Resque.redis.client.reconnect
  end
end
```

# [Screen Scraping With A Saw: A Nokogiri Tutorial With Examples](http://ruby.elevatedintel.com/blog/screen-scraping-with-a-saw-a-nokogiri-tutorial-with-examples)

# [Powering Actions with ElasticSearch Percolate](http://blog.feedbin.me/2013/11/10/powering-actions-with-elasticsearch-percolate)

Percolate allows you to register queries ahead of time, then whenever you send a new document to be indexed, ElasticSearch can tell you if this document matches any saved queries.

A model called Entry is used for storing RSS articles. Whenever a new entry is added it also gets sent to elasticsearch for indexing.

# [Announcing Suro: Backbone of Netflix's Data Pipeline](http://techblog.netflix.com/2013/12/announcing-suro-backbone-of-netflixs.html)

1. Applications emits events to Suro. The events include log lines.

2. Suro dispatches all the events to S3 by default. Hadoop jobs will process these events.

3. Based on a dynamically configurable routing rule, Suro also dispatches these log events to a designated Kafka cluster under a mapped topic.

4. Druid cluster indexes the log lines on the fly, making them immediately available for querying. For example, our service automatically detects error surges for each application within a 10 minute window and sends out alerts to application owners

5. A customized ElasticSearch cluster also ingests the same sets of log lines, de-duplicates them, and makes them immediately available for querying. Users are able to jump from an aggregated view on the Druid UI to individual records on ElasticSearch's Kibana UI to see exactly what went wrong.

Suro evolved over the past few years alongside many other powerful data pipeline solutions such as Apache Flume and Facebook Scribe.

# [What is Druid?](http://druid.io/druid.html)

Druid is open source infrastructure for real-time exploratory analytics that supports fast ad-hoc queries on large-scale data sets.

Druid's real-time nodes employ lock-free ingestion of append-only data sets to allow for simultaneous ingestion and querying of 10,000+ events per second. Simply put, the latency between when an event happens and when it is visible is limited only by how quickly the event can be delivered to Druid.

Druid is purpose built infrastructure that provides for exploration of very large quantities of data as it is ingested into the system. It is currently used for dashboarding of ad impression streams and operational monitoring of systems. If you have a dataset that is too large for your current infrastructure, your data has a timestamp associated with every event and you want to arbitrarily filter into the data with your
queries, then Druid can probably provide value for whatever your use case is as well.

# [jq](http://stedolan.github.io/jq/)

**jq** is like sed for JSON data - you can use it to slice and filter and map and transform structured data with the same ease that **sed**, **awk**, **grep** and friends let you play with text.

# [The Log: What every software engineer should know about real-time data's unifying abstraction](http://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying)

We were just beginning to run up against the limits of our monolithic, centralized database and needed to start the transition to a portfolio of specialized distributed systems. This has been an interesting experience: we built, deployed, and run to this day a distributed graph database, a distributed search backend, a Hadoop installation, and a first and second generation key-value store.

One of the most useful things I learned in all this was that many of the things we were building had a very simple concept at their heart: the log. Sometimes called write-ahead logs or commit logs or transaction logs, logs have been around almost as long as computers and are at the heart of many distributed data systems and real-time application architectures.

> A log is perhaps the simplest possible storage abstraction. It is an append-only, totally-ordered sequence of records ordered by time.

The ordering of records defines a notion of "time" since entries to the left are defined to be older then entries to the right. The log entry number can be thought of as the "timestamp" of the entry. Describing this ordering as a notion of time seems a bit odd at first, but it has the convenient property that it is decoupled from any particular physical clock. This property will turn out to be essential as we get to distributed systems.

> Logs have a specific purpose: they record what happened and when. For distributed data systems this is, in many ways, the very heart of the problem.

The log is the record of what happened, and each table or index is a projection of this history into some useful data structure or index. Since the log is immediately persisted it is used as the authoritative source in restoring all other persistent structures in the event of a crash.

Over-time the usage of the log grew from an implementation detail of ACID to a method for replicating data between databases. It turns out that the sequence of changes that happened on the database is exactly what is needed to keep a remote replica database in sync.

> The use of logs as a mechanism for data subscription seems to have arisen almost by chance. **But this very abstraction is ideal for supporting all kinds of messaging, data flow, and real-time data processing.**

The two problems a log solves—ordering changes and distributing data—are even more important in distributed data systems. Agreeing upon an ordering for updates (or agreeing to disagree and coping with the side-effects) are among the core design problems for these systems.

> **STATE MACHINE PRINCIPLE**
>
> **If two identical, deterministic processes begin in the same state and get the same inputs in the same order, they will produce the same output and end in the same state.**

Deterministic means that the processing isn't timing dependent.

You can describe each replica by a single number, the timestamp for the maximum log entry it has processed. This timestamp combined with the log uniquely captures the entire state of the replica.

> **Tables support data at rest and logs capture change**

The magic of the log is that if it is a complete log of changes, it holds not only the contents of the final version of the table, but also allows recreating all other versions that might have existed. It is, effectively, a sort of backup of every previous state of the table.

This might remind you of source code version control. There is a close relationship between source control and databases. Version control solves a very similar problem to what distributed data systems have to solve—managing distributed, concurrent changes in state. A version control system usually models the sequence of patches, which is in effect a log. You interact directly with a checked out "snapshot" of the current code which is analogous to the table. You will note that in version control systems, as in other distributed stateful systems, replication happens via the log: when you update, you pull down just the patches and apply them to your current snapshot.

> Data integration is making all the data an organization has available in all its services and systems.
>
> Take all the organization's data and put it into a central log for real-time subscription.

You can think of the log as acting as a kind of messaging system with durability guarantees and strong ordering semantics. In distributed systems, this model of communication sometimes goes by the (somewhat terrible) name of atomic broadcast.

> As much as possible, we needed to isolate each consumer from the source of the data. They should ideally integrate with just a single data repository that would give them access to everything.

For a long time, Kafka was a little unique (some would say odd) as an infrastructure product—neither a database nor a log file collection system nor a traditional messaging system. But recently Amazon has offered a service that is very very similar to Kafka called Kinesis. The similarity goes right down to the way partitioning is handled, data is retained, and the fairly odd split in the Kafka API between high- and low-level consumers. I was pretty happy about this. A sign you've created a good infrastructure abstraction is that AWS offers it as a service! Their vision for this seems to be exactly similar to what I am describing: it is the piping that connects all their distributed systems—DynamoDB, RedShift, S3, etc.—as well as the basis for distributed stream processing using EC2.

The job display page now just shows a job and records the fact that a job was shown along with the relevant attributes of the job, the viewer, and any other useful facts about the display of the job. Each of the other interested systems—the recommendation system, the security system, the job poster analytics system, and the data warehouse—all just subscribe to the feed and do their processing. The display code need not be aware of these other systems, and needn't be changed if a new data consumer is added.

At LinkedIn we are currently running over 60 billion unique message writes through Kafka per day (several hundred billion if you count the writes from mirroring between datacenters).

> In order to allow horizontal scaling we chop up our log into partitions.

The assignment of the messages to a particular partition is controllable by the writer, with most users choosing to partition by some kind of key (e.g. user id). Partitioning allows log appends to occur without co-ordination between shards and allows the throughput of the system to scale linearly with the Kafka cluster size.

Each partition is replicated across a configurable number of replicas, each of which has an identical copy of the partition's log. At any time, a single one of them will act as the leader; if the leader fails, one of the replicas will take over as leader.

> **Stream processing**: infrastructure for continuous data processing

Stream processing allows us to also include feeds computed off other feeds. These derived feeds look no different to consumers then the feeds of primary data from which they are computed.

A stream processing job, for our purposes, will be anything that reads from logs and writes output to logs or other systems.

When the process fails, it restores its index from the changelog. The log is the transformation of the local state into a sort of incremental record at a time backup.

Instead of simply throwing away the old log, we remove obsolete records—i.e. records whose primary key has a more recent update. By doing this, we still guarantee that the log contains a complete backup of the source system, but now we can no longer recreate all previous states of the source system, only the more recent ones. We call this feature log compaction.

If the implementation time for a distributed system goes from years to weeks because reliable, flexible building blocks emerge, then the pressure to coalesce into a single monolithic system disappears.

A system that assumes an external log is present allows the individual systems to relinquish a lot of their own complexity and rely on the shared log. Here are the things I think a log can do:

* handle data consistency (whether eventual or immediate) by sequencing concurrent updates to nodes
 
* provide data replication between nodes

* provide "commit" semantics to the writer (i.e. acknowledging only when your write guaranteed not to be lost)

* provide the external data subscription feed from the system

* provide the capability to restore failed replicas that lost their data or bootstrap new replicas

* handle rebalancing of data between nodes

The system is divided into two logical pieces: the log and the serving layer. The log captures the state changes in sequential order. The serving nodes store whatever index is required to serve queries.

The serving nodes subscribe to the log and apply writes as quickly as possible to its local index in the order the log has stored them.

The client can get read-your-write semantics from any node by providing the timestamp of a write as part of its query—a serving node receiving such a query will compare the desired timestamp to its own index point and if necessary delay the request until it has indexed up to at least that time to avoid serving stale data.

The serving nodes may or may not need to have any notion of "mastership" or "leader election". For many simple use cases, the serving nodes can be completely without leaders, since the log is the source of truth.

The idea of having a separate copy of data in the log (especially if it is a complete copy) strikes many people as wasteful. In reality, though there are a few factors that make this less of an issue. First, the log can be a particularly efficient storage mechanism. We store up to 5TB on our production Kafka servers. Meanwhile many serving systems require much more memory to serve data efficiently (text search, for example, is often all in memory). The serving system may also use optimized hardware. For example, most our live data systems either serve out of memory or else use SSDs. In contrast, the log system does only linear reads and writes, so it is quite happy using large multi-TB hard drives.

This is exactly the pattern that LinkedIn has used to build out many of its own real-time query systems. These systems feed off a database (using Databus as a log abstraction or off a dedicated log from Kafka) and provide a particular partitioning, indexing, and query capability on top of that data stream. This is the way we have implemented our search, social graph, and OLAP query systems. In fact, it is quite common to have a single data feed (whether a live feed or a derived feed coming from Hadoop) replicated into multiple serving systems for live serving. This has proven to be an enormous simplifying assumption. None of these systems need to have an externally accessible write api at all, Kafka and databases are used as the system of record and changes flow to the appropriate query systems through that log. Writes are handled locally by the nodes hosting a particular partition. These nodes blindly transcribe the feed provided by the log to their own store. A failed node can be restored by replaying the upstream log.

# [jgaskins/perpetuity](https://github.com/jgaskins/perpetuity)

Persistence gem for Ruby objects using the Data Mapper pattern.

In the Data Mapper pattern, the objects you work with don't understand how to persist themselves. They interact with other objects just as in any other object-oriented application, leaving all persistence logic to mapper objects. This decouples them from the database and allows you to write your code without it in mind.

# [agoragames/leaderboard](https://github.com/agoragames/leaderboard)

Leaderboards via Ruby gem, backed by Redis.

# [Cucumber and Full Stack Testing](http://tooky.co.uk/cucumber-and-full-stack-testing/)

Cucumber tests are a medium through which we can engage with the business people on our team and to help us understand how the system should behave. They give us an opportunity to check our understanding of what the system should do — to check the business's understanding of what the system should do. We automate these tests to give the business confidence that the system behaves as expected.

Full-stack, end-to-end, integration tests are there to give us confidence that the system fits together correctly, that we have all the different pieces in place, and they are able to talk to each other.

I have worked on many systems where the business facing acceptance tests were also the end-to-end integration tests. The test runs end up being slow, and the tests are cumbersome to work with.

> **The key thing is that your business acceptance tests do not all have to drive the whole system end-to-end**

We only a need a few scenarios to go end-to-end to give us the confidence the system as a whole is working. We can also write system tests, that aren't part of the acceptance suite, to test specific integrations.

> Try to write acceptance tests that directly drive the domain objects. Use these to accurately describe your application's behaviour. Focus them on the behaviour by not having them integrate the UI and the database.

BDD enables communication. Using stories and examples helps to create a shared language which we can then use to explore the problem space and begin to discover the things we don't know!

# [Building Stripe's API](http://amberonrails.com/building-stripes-api/)

This sounds like a no-brainer, but the best way to get people to try out your API is to make it really easy to get started.

For example, one of the first things you'll see on our front page is a curl snippet you can paste into a terminal to simulate charging a credit card. Regardless of whether you have a Stripe account or not (if logged in, we embed your test API key, else, it's a sample account's API key), you can see the Stripe API in action.

All of our documentation code snippets are similarly easy to copy and paste—we try to embed as much information as possible (API keys, actual object IDs from your account) so you don't have to.

Ultimately, I think there's a certain degree of trust that users put in official libraries, which makes it easy for them to get started (as opposed to trying to audit different third-party libraries). It also makes it really easy for us to have language-specific documentation this way.

One thing that we found critically important was to keep the API focused. It's tempting to add new features that are nice, but not necessary.

We allow our users (and third party applications) to hook into Stripe in a couple of ways.

Webhooks are a way of Stripe letting you (our user) know when some interesting event has happened on the Stripe server. Examples include `charge.succeeded`, `charge.refunded`, `invoice.paid`, and so on.

Stripe Connect is an OAuth2 API that allows a Stripe user to authorize access to their Stripe account to a third-party application. This application might be a marketplace, whose users want to accept payments, or an analytics dashboard, who wants to be able to have full access to Stripe data.

One of the most important things you need with an API is a test environment. This is particularly important for a payments API.

If there's certain behavior that your user's application potentially depends on, make sure they can test it easily.

We know that a large percentage of our users' time is probably spent debugging. We also (unfortunately) know that sometimes you spend a lot of time debugging something that eventually turns out to be really obvious or stupid.

# [stripe/poncho](https://github.com/stripe/poncho)

A DSL to build REST interfaces, it will validate input and output, coerce values and is easily extendable with custom data types.

It's compatible with any rack-based framework, such as Rails or Sinatra.

# [Keybits - Own and control your online home](http://keybits.net/)

Set up a personal server using our Ansible scripts and Docker images.

You want a place where you can host all your best work - things like a blog, email, your best photos and perhaps code you've written.

You want this to be simple, secure and easy to keep up to date.

We're choosing software that doesn't lock your data in.

# [The OAuth Bible](https://github.com/Mashape/mashape-oauth/blob/master/FLOWS.md)

Describes both OAuth 1.0a & 2 flows.

# [Running a software team at Google](http://matt-welsh.blogspot.co.uk/2013/04/running-software-team-at-google.html)

Tech Lead Manager - TLM

There are four main aspects to my job:

1. Defining the technical agenda for the team and making sure we're successful

2. Writing code of my own

3. Acting as the main liaison between our team and other groups at Google, and

4. Doing the "people management" for the team in terms of hiring, performance reviews, promotion, and so forth.

Most people on my team are much better software engineers than I am, and I lean on them heavily to do the really hard work of building solid, reliable software.

My job is to shield the engineers on my team from distractions, and support them so they can be successful.

I do spend about 50% of my time writing code. I really need to have a few solid hours each day hacking in order to stay sane. Since I don't have as many coding cycles (and service more interrupts) than other people on my team, I tend to take on the more mundane tasks such as writing MapReduce code to analyze service logs and generate reports on performance. I actually like this kind of work as it means dealing with a huge amount of data and slicing and dicing it in various interesting ways. I also don't need to show off my heroic coding skills in order to get promoted at this point, so I let the folks who are better hackers implement the sexy new features.

I do exert a lot of influence over the direction that our team's software takes, in terms of overall design and architecture. Largely this is because I have more experience thinking about systems design than some of the folks on my team, although it does mean that I need to defer to the people writing the actual code when there are hairy details with which I am unfamiliar. A big part of my job is setting priorities and making the call when we are forced to choose between several unappealing options to solve a particular problem. (It also means I am the one who takes the heat if I make the wrong decision.)

# [How to Make Your Open Source Project Really Awesome](http://blog.clojurewerkz.org/blog/2013/04/20/how-to-make-your-open-source-project-really-awesome/)

* Clear dependency/installation instructions

* At least one brief documentation guide

* A change log and tags in the repo

* Some information about supported language/runtime/tool versions and project maturity

* A mailing list where users can ask questions and help each other

How do I install this?

Have you been using it in production for months?

Is it something you still believe is incomplete?

Do you expect the API change drastically in the next version?

Is this project mature and safe to use even for the most demanding and conservative projects?

State What License You Use

Make It Easy to Upgrade

What has changed in this release?

1. Every time you fix a bug, you write a brief entry to the change log.

2. Every time you add a feature, you briefly mentoin it in the change log with a few code examples.

3. Every time you make a breaking API change, you clearly state that in bold in the change log.

Tag Releases, announce (ANN) releases, use **pre** & **rc** releases for development.

# [tmm1/test-queue](https://github.com/tmm1/test-queue)

Yet another parallel test runner, built using a centralized queue to ensure optimal distribution of tests between workers.

Specifically optimized for CI environments: build statistics from each run are stored locally and used to sort the queue at the beginning of the next run.

# [oauth-io/oauthd](https://github.com/oauth-io/oauthd)

The OAuth Daemon is the open source version of the OAuth.io core.

# [adams-heroku-values](https://gist.github.com/adamwiggins/5687294)

#### Make it real

Ideas are cheap. Make a prototype, sketch a CLI session, draw a wireframe. Discuss around concrete examples, not hand-waving abstractions. Don't say you did something, provide a URL that proves it.

#### Ship it

Nothing is real until it's being used by a real user. This doesn't mean you make a prototype in the morning and blog about it in the evening. It means you find one person you believe your product will help and try to get them to use it.

#### Do it with style

#### Intuition-driven first, data-driven when many users

#### Divide and conquer

Big, hard problems become easy if you cut them into small pieces.

> If it's hard, cut scope.

#### Timing matters

If you're building something and just can't seem to get it right, maybe now isn't the right time.

#### Throw things away

> It's not the code that is valuable, it's the understanding you've gained from building it.

Never be afraid to throw something away and do it again, it will almost always be faster to build and much better the second (or third, or Nth) time around.

#### Machete design

> Create a single, general-purpose tool which is simple to understand but can be applied to many problems.

The value of a product is the number of problems it can solve divided by the amount of complexity the user needs to keep in their head to use it

#### Small sharp tools

> **Composability**

Simple tools which do one thing well and can be composed with other tools to create a nearly infinite number of results.

Heroku examples include the add-ons API, logging/logplex, and procfile/the process model.

> **Small is beautiful**

This isn't just tools, it's also teams. Several small, autonomous, focused teams working in concert almost always beat a single monolithic team.

#### Put it in the cloud

> Services, not software

Given a choice between a great app that runs locally and a mediocre app that runs in the cloud, i'll always take the latter.

#### Results, not politics

You "get ahead" in your heroku career by delivering real value to customers and to the company, not by impressing your boss or with big talk.

#### Decision-making via ownership, not consensus or authority

Every product, feature, software component, web page, business deal, blog post, and so on should have a single owner. Many people may collaborate on it, but the owner is "the buck stops here" and makes the final call on what happens with the owned thing.

If something doesn't have an owner, no one should be working on it or trying to make decisions about it. Before those things can happen, it has to be owned.

> **Ownership can't be given, only taken. Ownership can't be declared, only demonstrated.**

Ownership begins with whoever creates the thing first. Later the owner may hand it off to someone else. If an item gets dropped for some reason (for example, the current owner switching teams or leaving the company), it's fair game for anyone else to pick up.

Apple has DRI (Directly Responsible Individual).

#### Everything is an experiment

Anything we do -- a product, a feature, a standing meeting, an email campaign -- is always subject to change. That includes discontinuing or shutting down whatever the thing is. Ending an experiment isn't a failure, since we often learn the most from experiments that don't produce the results we wanted.

#### Own up to failure

Own it. Admit your mistake, say you're sorry (when applicable), and feel the failure to make sure you learned from it. Then, get back to work.

#### Gradual rollouts

Ease into everything. Use feature flags to activate people slowly into changes, then let it bake for a bit. Test out the message for a public launch by first sending it around internally, and later writing the private beta announcement. Collect feedback and adjust. By the time you're ready to take it public to a wide audience, you'll be fairly certain to have worked out all the kinks.

#### Design everything

#### Do less

Do we really need that feature? Can we delete that code? Do we really need that command? Can we outsource to or partner with another company so that we don't have the build and maintain something?

#### Question everything

#### Interfaces matter

The two critical components of a good interface are that it be narrow and well-defined.

#### Names matter

Pick exactly one name for each concept the user needs to track, and use it consistently.

#### Maniacal focus on simplicity

There is no user manual.

#### CLI for life

#### Ignore the competition

#### Write well

#### Strong opinions, weakly held

Have a strong opinion and argue passionately for it. But when you encounter new information, be willing to change your mind.

#### Candor

Be blunt, honest, and truthful. Constructive criticism is the best kind. Avoid keeping quiet with your criticism about someone or something for the sake of politeness. Don't say something about someone to a third party that you wouldn't say to their face.

# [Learn Lua in 15 Minutes](http://tylerneylon.com/a/learn-lua/)

# [Yeoman](http://yeoman.io/)

Yeoman 1.0 is more than just a tool. It's a workflow; a collection of tools and best practices working in harmony to make developing for the web even better.

Our workflow is comprised of three tools for improving your productivity and satisfaction when building a web app: yo (the scaffolding tool), grunt (the build tool) and bower (for package management).

1. Yo scaffolds out a new application, writing your Grunt configuration and pulling in relevant Grunt tasks that you might need for your build.

2. Grunt is used to build, preview and test your project, thanks to help from tasks curated by the Yeoman team and grunt-contrib.

3. Bower is used for dependency management, so that you no longer have to manually download and manage your scripts.
All three of these tools are developed and maintained separately, but work well together as part of our prescribed workflow for keeping you effective.

# [GNU parallel, 2010 slides](http://www.slideshare.net/fscons/gnu-parallel-ole-tange)